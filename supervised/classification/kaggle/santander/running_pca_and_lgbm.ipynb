{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Santandar Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as panda\n",
    "from matplotlib import pyplot as plot\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import pandas as panda\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler,RobustScaler,label_binarize\n",
    "\n",
    "from sklearn.model_selection import train_test_split,KFold,StratifiedKFold,GridSearchCV,RepeatedStratifiedKFold,learning_curve\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, roc_curve, auc, precision_recall_curve, \\\n",
    "        classification_report,confusion_matrix,average_precision_score\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression,RidgeClassifier,SGDClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,VotingClassifier,AdaBoostClassifier,GradientBoostingClassifier,BaggingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot as plot\n",
    "from itertools import cycle\n",
    "import numpy as np \n",
    "from scipy import interp\n",
    "import seaborn as sns\n",
    "import itertools, time, datetime\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif,f_classif\n",
    "from numpy import bincount, linspace, mean, std, arange, squeeze\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(143)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = panda.read_csv('data/train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    179902\n",
       " 1     20098\n",
       " Name: target, dtype: int64, (200000, 202))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.target.value_counts(), train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['target'] = train_data.target.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot.figure(figsize=(10,4))\n",
    "train_data.target.value_counts().plot(kind='bar')\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ID_code</th>\n",
       "      <td>200000</td>\n",
       "      <td>200000</td>\n",
       "      <td>train_182780</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_0</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.6799</td>\n",
       "      <td>3.04005</td>\n",
       "      <td>0.4084</td>\n",
       "      <td>8.45385</td>\n",
       "      <td>10.5248</td>\n",
       "      <td>12.7582</td>\n",
       "      <td>20.315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_1</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.62762</td>\n",
       "      <td>4.05004</td>\n",
       "      <td>-15.0434</td>\n",
       "      <td>-4.74003</td>\n",
       "      <td>-1.60805</td>\n",
       "      <td>1.35862</td>\n",
       "      <td>10.3768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_2</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.7152</td>\n",
       "      <td>2.64089</td>\n",
       "      <td>2.1171</td>\n",
       "      <td>8.72247</td>\n",
       "      <td>10.58</td>\n",
       "      <td>12.5167</td>\n",
       "      <td>19.353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_3</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.79653</td>\n",
       "      <td>2.04332</td>\n",
       "      <td>-0.0402</td>\n",
       "      <td>5.25408</td>\n",
       "      <td>6.825</td>\n",
       "      <td>8.3241</td>\n",
       "      <td>13.1883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_4</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0783</td>\n",
       "      <td>1.62315</td>\n",
       "      <td>5.0748</td>\n",
       "      <td>9.88317</td>\n",
       "      <td>11.1082</td>\n",
       "      <td>12.2611</td>\n",
       "      <td>16.6714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_5</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5.06532</td>\n",
       "      <td>7.86327</td>\n",
       "      <td>-32.5626</td>\n",
       "      <td>-11.2004</td>\n",
       "      <td>-4.83315</td>\n",
       "      <td>0.9248</td>\n",
       "      <td>17.2516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_6</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.40895</td>\n",
       "      <td>0.866607</td>\n",
       "      <td>2.3473</td>\n",
       "      <td>4.7677</td>\n",
       "      <td>5.3851</td>\n",
       "      <td>6.003</td>\n",
       "      <td>8.4477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_7</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.5458</td>\n",
       "      <td>3.41808</td>\n",
       "      <td>5.3497</td>\n",
       "      <td>13.9438</td>\n",
       "      <td>16.4568</td>\n",
       "      <td>19.1029</td>\n",
       "      <td>27.6918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_8</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.284162</td>\n",
       "      <td>3.33263</td>\n",
       "      <td>-10.5055</td>\n",
       "      <td>-2.3178</td>\n",
       "      <td>0.3937</td>\n",
       "      <td>2.9379</td>\n",
       "      <td>10.1513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_9</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.56724</td>\n",
       "      <td>1.23507</td>\n",
       "      <td>3.9705</td>\n",
       "      <td>6.6188</td>\n",
       "      <td>7.6296</td>\n",
       "      <td>8.58442</td>\n",
       "      <td>11.1506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_10</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.39434</td>\n",
       "      <td>5.50079</td>\n",
       "      <td>-20.7313</td>\n",
       "      <td>-3.59495</td>\n",
       "      <td>0.4873</td>\n",
       "      <td>4.38293</td>\n",
       "      <td>18.6702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_11</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.2456</td>\n",
       "      <td>5.97025</td>\n",
       "      <td>-26.095</td>\n",
       "      <td>-7.5106</td>\n",
       "      <td>-3.28695</td>\n",
       "      <td>0.852825</td>\n",
       "      <td>17.1887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_12</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.024</td>\n",
       "      <td>0.190059</td>\n",
       "      <td>13.4346</td>\n",
       "      <td>13.894</td>\n",
       "      <td>14.0255</td>\n",
       "      <td>14.1642</td>\n",
       "      <td>14.6545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_13</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.53023</td>\n",
       "      <td>4.63954</td>\n",
       "      <td>-6.0111</td>\n",
       "      <td>5.0728</td>\n",
       "      <td>8.60425</td>\n",
       "      <td>12.2748</td>\n",
       "      <td>22.3315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_14</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.53761</td>\n",
       "      <td>2.24791</td>\n",
       "      <td>1.0133</td>\n",
       "      <td>5.78188</td>\n",
       "      <td>7.5203</td>\n",
       "      <td>9.27042</td>\n",
       "      <td>14.9377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_15</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.5731</td>\n",
       "      <td>0.411711</td>\n",
       "      <td>13.0769</td>\n",
       "      <td>14.2628</td>\n",
       "      <td>14.5741</td>\n",
       "      <td>14.8745</td>\n",
       "      <td>15.8633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_16</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.33326</td>\n",
       "      <td>2.55742</td>\n",
       "      <td>0.6351</td>\n",
       "      <td>7.45228</td>\n",
       "      <td>9.23205</td>\n",
       "      <td>11.0559</td>\n",
       "      <td>17.9506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_17</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5.69673</td>\n",
       "      <td>6.71261</td>\n",
       "      <td>-33.3802</td>\n",
       "      <td>-10.4762</td>\n",
       "      <td>-5.66635</td>\n",
       "      <td>-0.810775</td>\n",
       "      <td>19.0259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_18</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.244</td>\n",
       "      <td>7.85137</td>\n",
       "      <td>-10.6642</td>\n",
       "      <td>9.17795</td>\n",
       "      <td>15.1962</td>\n",
       "      <td>21.0133</td>\n",
       "      <td>41.748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_19</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.4386</td>\n",
       "      <td>7.99669</td>\n",
       "      <td>-12.4025</td>\n",
       "      <td>6.27648</td>\n",
       "      <td>12.4539</td>\n",
       "      <td>18.4333</td>\n",
       "      <td>35.183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_20</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.2909</td>\n",
       "      <td>5.87625</td>\n",
       "      <td>-5.4322</td>\n",
       "      <td>8.6278</td>\n",
       "      <td>13.1968</td>\n",
       "      <td>17.8794</td>\n",
       "      <td>31.2859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_21</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.2579</td>\n",
       "      <td>8.19656</td>\n",
       "      <td>-10.089</td>\n",
       "      <td>11.551</td>\n",
       "      <td>17.2343</td>\n",
       "      <td>23.0891</td>\n",
       "      <td>49.0443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_22</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.30543</td>\n",
       "      <td>2.84796</td>\n",
       "      <td>-5.3225</td>\n",
       "      <td>2.1824</td>\n",
       "      <td>4.27515</td>\n",
       "      <td>6.2932</td>\n",
       "      <td>14.5945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_23</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.01954</td>\n",
       "      <td>0.526893</td>\n",
       "      <td>1.2098</td>\n",
       "      <td>2.6341</td>\n",
       "      <td>3.00865</td>\n",
       "      <td>3.4038</td>\n",
       "      <td>4.8752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_24</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.5844</td>\n",
       "      <td>3.77725</td>\n",
       "      <td>-0.6784</td>\n",
       "      <td>7.613</td>\n",
       "      <td>10.3803</td>\n",
       "      <td>13.4796</td>\n",
       "      <td>25.446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_25</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.6675</td>\n",
       "      <td>0.285535</td>\n",
       "      <td>12.72</td>\n",
       "      <td>13.4564</td>\n",
       "      <td>13.6625</td>\n",
       "      <td>13.8637</td>\n",
       "      <td>14.6546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_26</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4.05513</td>\n",
       "      <td>5.92221</td>\n",
       "      <td>-24.2431</td>\n",
       "      <td>-8.32173</td>\n",
       "      <td>-4.1969</td>\n",
       "      <td>-0.0902</td>\n",
       "      <td>15.6751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_27</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.13791</td>\n",
       "      <td>1.52371</td>\n",
       "      <td>-6.1668</td>\n",
       "      <td>-2.3079</td>\n",
       "      <td>-1.1321</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>3.2431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_28</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.53298</td>\n",
       "      <td>0.783367</td>\n",
       "      <td>2.0896</td>\n",
       "      <td>4.9921</td>\n",
       "      <td>5.53485</td>\n",
       "      <td>6.0937</td>\n",
       "      <td>8.7874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_170</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.00496206</td>\n",
       "      <td>4.42462</td>\n",
       "      <td>-14.506</td>\n",
       "      <td>-3.2585</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>3.0964</td>\n",
       "      <td>16.7319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_171</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.831777</td>\n",
       "      <td>5.37801</td>\n",
       "      <td>-22.4793</td>\n",
       "      <td>-4.72035</td>\n",
       "      <td>-0.80735</td>\n",
       "      <td>2.9568</td>\n",
       "      <td>17.9173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_172</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.8171</td>\n",
       "      <td>8.67417</td>\n",
       "      <td>-11.4533</td>\n",
       "      <td>13.7318</td>\n",
       "      <td>19.748</td>\n",
       "      <td>25.9077</td>\n",
       "      <td>53.5919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_173</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.677967</td>\n",
       "      <td>5.96667</td>\n",
       "      <td>-22.7487</td>\n",
       "      <td>-5.00953</td>\n",
       "      <td>-0.56975</td>\n",
       "      <td>3.6199</td>\n",
       "      <td>18.8554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_174</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.2107</td>\n",
       "      <td>7.13643</td>\n",
       "      <td>-2.9953</td>\n",
       "      <td>15.0646</td>\n",
       "      <td>20.2061</td>\n",
       "      <td>25.6412</td>\n",
       "      <td>43.5468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_175</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.6406</td>\n",
       "      <td>2.89217</td>\n",
       "      <td>3.2415</td>\n",
       "      <td>9.3716</td>\n",
       "      <td>11.6798</td>\n",
       "      <td>13.7455</td>\n",
       "      <td>20.8548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_176</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.79958</td>\n",
       "      <td>7.51394</td>\n",
       "      <td>-29.1165</td>\n",
       "      <td>-8.3865</td>\n",
       "      <td>-2.53845</td>\n",
       "      <td>2.7044</td>\n",
       "      <td>20.2452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_177</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.8829</td>\n",
       "      <td>2.62889</td>\n",
       "      <td>4.9521</td>\n",
       "      <td>9.80867</td>\n",
       "      <td>11.7372</td>\n",
       "      <td>13.9313</td>\n",
       "      <td>20.5965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_178</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.01406</td>\n",
       "      <td>8.57981</td>\n",
       "      <td>-29.2734</td>\n",
       "      <td>-7.3957</td>\n",
       "      <td>-0.94205</td>\n",
       "      <td>5.33875</td>\n",
       "      <td>29.8413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_179</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.59144</td>\n",
       "      <td>2.79896</td>\n",
       "      <td>-7.8561</td>\n",
       "      <td>0.625575</td>\n",
       "      <td>2.5123</td>\n",
       "      <td>4.39112</td>\n",
       "      <td>13.4487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_180</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.74167</td>\n",
       "      <td>5.26124</td>\n",
       "      <td>-22.0374</td>\n",
       "      <td>-6.6739</td>\n",
       "      <td>-2.6888</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>12.7505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_181</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0855</td>\n",
       "      <td>1.37186</td>\n",
       "      <td>5.4165</td>\n",
       "      <td>9.0847</td>\n",
       "      <td>10.036</td>\n",
       "      <td>11.0113</td>\n",
       "      <td>14.3939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_182</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.719109</td>\n",
       "      <td>8.96343</td>\n",
       "      <td>-26.0011</td>\n",
       "      <td>-6.06442</td>\n",
       "      <td>0.7202</td>\n",
       "      <td>7.49918</td>\n",
       "      <td>29.2487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_183</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.76909</td>\n",
       "      <td>4.47492</td>\n",
       "      <td>-4.8082</td>\n",
       "      <td>5.4231</td>\n",
       "      <td>8.6</td>\n",
       "      <td>12.1274</td>\n",
       "      <td>23.7049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_184</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.7567</td>\n",
       "      <td>9.31828</td>\n",
       "      <td>-18.4897</td>\n",
       "      <td>5.6633</td>\n",
       "      <td>12.521</td>\n",
       "      <td>19.4562</td>\n",
       "      <td>44.3634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_185</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.98326</td>\n",
       "      <td>4.72517</td>\n",
       "      <td>-22.5833</td>\n",
       "      <td>-7.36</td>\n",
       "      <td>-3.94695</td>\n",
       "      <td>-0.59065</td>\n",
       "      <td>12.9975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_186</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.97027</td>\n",
       "      <td>3.18976</td>\n",
       "      <td>-3.0223</td>\n",
       "      <td>6.7152</td>\n",
       "      <td>8.90215</td>\n",
       "      <td>11.1938</td>\n",
       "      <td>21.7392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_187</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-10.335</td>\n",
       "      <td>11.5747</td>\n",
       "      <td>-47.7536</td>\n",
       "      <td>-19.2051</td>\n",
       "      <td>-10.2097</td>\n",
       "      <td>-1.466</td>\n",
       "      <td>22.7861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_188</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.3772</td>\n",
       "      <td>3.9446</td>\n",
       "      <td>4.4123</td>\n",
       "      <td>12.5016</td>\n",
       "      <td>15.2394</td>\n",
       "      <td>18.3452</td>\n",
       "      <td>29.3303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_189</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.746072</td>\n",
       "      <td>0.976348</td>\n",
       "      <td>-2.5543</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.7426</td>\n",
       "      <td>1.4829</td>\n",
       "      <td>4.0341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_190</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.23444</td>\n",
       "      <td>4.55992</td>\n",
       "      <td>-14.0933</td>\n",
       "      <td>-0.058825</td>\n",
       "      <td>3.2036</td>\n",
       "      <td>6.4062</td>\n",
       "      <td>18.4409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_191</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.43841</td>\n",
       "      <td>3.02327</td>\n",
       "      <td>-2.6917</td>\n",
       "      <td>5.1574</td>\n",
       "      <td>7.34775</td>\n",
       "      <td>9.51253</td>\n",
       "      <td>16.7165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_192</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.92784</td>\n",
       "      <td>1.47842</td>\n",
       "      <td>-3.8145</td>\n",
       "      <td>0.889775</td>\n",
       "      <td>1.9013</td>\n",
       "      <td>2.9495</td>\n",
       "      <td>8.4024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_193</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.33177</td>\n",
       "      <td>3.99203</td>\n",
       "      <td>-11.7834</td>\n",
       "      <td>0.5846</td>\n",
       "      <td>3.39635</td>\n",
       "      <td>6.2058</td>\n",
       "      <td>18.2818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_194</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.9938</td>\n",
       "      <td>3.13516</td>\n",
       "      <td>8.6944</td>\n",
       "      <td>15.6298</td>\n",
       "      <td>17.9579</td>\n",
       "      <td>20.3965</td>\n",
       "      <td>27.9288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_195</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.142088</td>\n",
       "      <td>1.42937</td>\n",
       "      <td>-5.261</td>\n",
       "      <td>-1.1707</td>\n",
       "      <td>-0.1727</td>\n",
       "      <td>0.8296</td>\n",
       "      <td>4.2729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_196</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.30334</td>\n",
       "      <td>5.45437</td>\n",
       "      <td>-14.2096</td>\n",
       "      <td>-1.94693</td>\n",
       "      <td>2.4089</td>\n",
       "      <td>6.55673</td>\n",
       "      <td>18.3215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_197</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.90816</td>\n",
       "      <td>0.921625</td>\n",
       "      <td>5.9606</td>\n",
       "      <td>8.2528</td>\n",
       "      <td>8.8882</td>\n",
       "      <td>9.5933</td>\n",
       "      <td>12.0004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_198</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.8707</td>\n",
       "      <td>3.01095</td>\n",
       "      <td>6.2993</td>\n",
       "      <td>13.8297</td>\n",
       "      <td>15.9341</td>\n",
       "      <td>18.0647</td>\n",
       "      <td>26.0791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_199</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.32654</td>\n",
       "      <td>10.438</td>\n",
       "      <td>-38.8528</td>\n",
       "      <td>-11.2085</td>\n",
       "      <td>-2.81955</td>\n",
       "      <td>4.8368</td>\n",
       "      <td>28.5007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>201 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          count  unique           top freq        mean       std      min  \\\n",
       "ID_code  200000  200000  train_182780    1         NaN       NaN      NaN   \n",
       "var_0    200000     NaN           NaN  NaN     10.6799   3.04005   0.4084   \n",
       "var_1    200000     NaN           NaN  NaN    -1.62762   4.05004 -15.0434   \n",
       "var_2    200000     NaN           NaN  NaN     10.7152   2.64089   2.1171   \n",
       "var_3    200000     NaN           NaN  NaN     6.79653   2.04332  -0.0402   \n",
       "var_4    200000     NaN           NaN  NaN     11.0783   1.62315   5.0748   \n",
       "var_5    200000     NaN           NaN  NaN    -5.06532   7.86327 -32.5626   \n",
       "var_6    200000     NaN           NaN  NaN     5.40895  0.866607   2.3473   \n",
       "var_7    200000     NaN           NaN  NaN     16.5458   3.41808   5.3497   \n",
       "var_8    200000     NaN           NaN  NaN    0.284162   3.33263 -10.5055   \n",
       "var_9    200000     NaN           NaN  NaN     7.56724   1.23507   3.9705   \n",
       "var_10   200000     NaN           NaN  NaN     0.39434   5.50079 -20.7313   \n",
       "var_11   200000     NaN           NaN  NaN     -3.2456   5.97025  -26.095   \n",
       "var_12   200000     NaN           NaN  NaN      14.024  0.190059  13.4346   \n",
       "var_13   200000     NaN           NaN  NaN     8.53023   4.63954  -6.0111   \n",
       "var_14   200000     NaN           NaN  NaN     7.53761   2.24791   1.0133   \n",
       "var_15   200000     NaN           NaN  NaN     14.5731  0.411711  13.0769   \n",
       "var_16   200000     NaN           NaN  NaN     9.33326   2.55742   0.6351   \n",
       "var_17   200000     NaN           NaN  NaN    -5.69673   6.71261 -33.3802   \n",
       "var_18   200000     NaN           NaN  NaN      15.244   7.85137 -10.6642   \n",
       "var_19   200000     NaN           NaN  NaN     12.4386   7.99669 -12.4025   \n",
       "var_20   200000     NaN           NaN  NaN     13.2909   5.87625  -5.4322   \n",
       "var_21   200000     NaN           NaN  NaN     17.2579   8.19656  -10.089   \n",
       "var_22   200000     NaN           NaN  NaN     4.30543   2.84796  -5.3225   \n",
       "var_23   200000     NaN           NaN  NaN     3.01954  0.526893   1.2098   \n",
       "var_24   200000     NaN           NaN  NaN     10.5844   3.77725  -0.6784   \n",
       "var_25   200000     NaN           NaN  NaN     13.6675  0.285535    12.72   \n",
       "var_26   200000     NaN           NaN  NaN    -4.05513   5.92221 -24.2431   \n",
       "var_27   200000     NaN           NaN  NaN    -1.13791   1.52371  -6.1668   \n",
       "var_28   200000     NaN           NaN  NaN     5.53298  0.783367   2.0896   \n",
       "...         ...     ...           ...  ...         ...       ...      ...   \n",
       "var_170  200000     NaN           NaN  NaN -0.00496206   4.42462  -14.506   \n",
       "var_171  200000     NaN           NaN  NaN   -0.831777   5.37801 -22.4793   \n",
       "var_172  200000     NaN           NaN  NaN     19.8171   8.67417 -11.4533   \n",
       "var_173  200000     NaN           NaN  NaN   -0.677967   5.96667 -22.7487   \n",
       "var_174  200000     NaN           NaN  NaN     20.2107   7.13643  -2.9953   \n",
       "var_175  200000     NaN           NaN  NaN     11.6406   2.89217   3.2415   \n",
       "var_176  200000     NaN           NaN  NaN    -2.79958   7.51394 -29.1165   \n",
       "var_177  200000     NaN           NaN  NaN     11.8829   2.62889   4.9521   \n",
       "var_178  200000     NaN           NaN  NaN    -1.01406   8.57981 -29.2734   \n",
       "var_179  200000     NaN           NaN  NaN     2.59144   2.79896  -7.8561   \n",
       "var_180  200000     NaN           NaN  NaN    -2.74167   5.26124 -22.0374   \n",
       "var_181  200000     NaN           NaN  NaN     10.0855   1.37186   5.4165   \n",
       "var_182  200000     NaN           NaN  NaN    0.719109   8.96343 -26.0011   \n",
       "var_183  200000     NaN           NaN  NaN     8.76909   4.47492  -4.8082   \n",
       "var_184  200000     NaN           NaN  NaN     12.7567   9.31828 -18.4897   \n",
       "var_185  200000     NaN           NaN  NaN    -3.98326   4.72517 -22.5833   \n",
       "var_186  200000     NaN           NaN  NaN     8.97027   3.18976  -3.0223   \n",
       "var_187  200000     NaN           NaN  NaN     -10.335   11.5747 -47.7536   \n",
       "var_188  200000     NaN           NaN  NaN     15.3772    3.9446   4.4123   \n",
       "var_189  200000     NaN           NaN  NaN    0.746072  0.976348  -2.5543   \n",
       "var_190  200000     NaN           NaN  NaN     3.23444   4.55992 -14.0933   \n",
       "var_191  200000     NaN           NaN  NaN     7.43841   3.02327  -2.6917   \n",
       "var_192  200000     NaN           NaN  NaN     1.92784   1.47842  -3.8145   \n",
       "var_193  200000     NaN           NaN  NaN     3.33177   3.99203 -11.7834   \n",
       "var_194  200000     NaN           NaN  NaN     17.9938   3.13516   8.6944   \n",
       "var_195  200000     NaN           NaN  NaN   -0.142088   1.42937   -5.261   \n",
       "var_196  200000     NaN           NaN  NaN     2.30334   5.45437 -14.2096   \n",
       "var_197  200000     NaN           NaN  NaN     8.90816  0.921625   5.9606   \n",
       "var_198  200000     NaN           NaN  NaN     15.8707   3.01095   6.2993   \n",
       "var_199  200000     NaN           NaN  NaN    -3.32654    10.438 -38.8528   \n",
       "\n",
       "              25%      50%       75%      max  \n",
       "ID_code       NaN      NaN       NaN      NaN  \n",
       "var_0     8.45385  10.5248   12.7582   20.315  \n",
       "var_1    -4.74003 -1.60805   1.35862  10.3768  \n",
       "var_2     8.72247    10.58   12.5167   19.353  \n",
       "var_3     5.25408    6.825    8.3241  13.1883  \n",
       "var_4     9.88317  11.1082   12.2611  16.6714  \n",
       "var_5    -11.2004 -4.83315    0.9248  17.2516  \n",
       "var_6      4.7677   5.3851     6.003   8.4477  \n",
       "var_7     13.9438  16.4568   19.1029  27.6918  \n",
       "var_8     -2.3178   0.3937    2.9379  10.1513  \n",
       "var_9      6.6188   7.6296   8.58442  11.1506  \n",
       "var_10   -3.59495   0.4873   4.38293  18.6702  \n",
       "var_11    -7.5106 -3.28695  0.852825  17.1887  \n",
       "var_12     13.894  14.0255   14.1642  14.6545  \n",
       "var_13     5.0728  8.60425   12.2748  22.3315  \n",
       "var_14    5.78188   7.5203   9.27042  14.9377  \n",
       "var_15    14.2628  14.5741   14.8745  15.8633  \n",
       "var_16    7.45228  9.23205   11.0559  17.9506  \n",
       "var_17   -10.4762 -5.66635 -0.810775  19.0259  \n",
       "var_18    9.17795  15.1962   21.0133   41.748  \n",
       "var_19    6.27648  12.4539   18.4333   35.183  \n",
       "var_20     8.6278  13.1968   17.8794  31.2859  \n",
       "var_21     11.551  17.2343   23.0891  49.0443  \n",
       "var_22     2.1824  4.27515    6.2932  14.5945  \n",
       "var_23     2.6341  3.00865    3.4038   4.8752  \n",
       "var_24      7.613  10.3803   13.4796   25.446  \n",
       "var_25    13.4564  13.6625   13.8637  14.6546  \n",
       "var_26   -8.32173  -4.1969   -0.0902  15.6751  \n",
       "var_27    -2.3079  -1.1321  0.015625   3.2431  \n",
       "var_28     4.9921  5.53485    6.0937   8.7874  \n",
       "...           ...      ...       ...      ...  \n",
       "var_170   -3.2585   0.0028    3.0964  16.7319  \n",
       "var_171  -4.72035 -0.80735    2.9568  17.9173  \n",
       "var_172   13.7318   19.748   25.9077  53.5919  \n",
       "var_173  -5.00953 -0.56975    3.6199  18.8554  \n",
       "var_174   15.0646  20.2061   25.6412  43.5468  \n",
       "var_175    9.3716  11.6798   13.7455  20.8548  \n",
       "var_176   -8.3865 -2.53845    2.7044  20.2452  \n",
       "var_177   9.80867  11.7372   13.9313  20.5965  \n",
       "var_178   -7.3957 -0.94205   5.33875  29.8413  \n",
       "var_179  0.625575   2.5123   4.39112  13.4487  \n",
       "var_180   -6.6739  -2.6888    0.9962  12.7505  \n",
       "var_181    9.0847   10.036   11.0113  14.3939  \n",
       "var_182  -6.06442   0.7202   7.49918  29.2487  \n",
       "var_183    5.4231      8.6   12.1274  23.7049  \n",
       "var_184    5.6633   12.521   19.4562  44.3634  \n",
       "var_185     -7.36 -3.94695  -0.59065  12.9975  \n",
       "var_186    6.7152  8.90215   11.1938  21.7392  \n",
       "var_187  -19.2051 -10.2097    -1.466  22.7861  \n",
       "var_188   12.5016  15.2394   18.3452  29.3303  \n",
       "var_189    0.0149   0.7426    1.4829   4.0341  \n",
       "var_190 -0.058825   3.2036    6.4062  18.4409  \n",
       "var_191    5.1574  7.34775   9.51253  16.7165  \n",
       "var_192  0.889775   1.9013    2.9495   8.4024  \n",
       "var_193    0.5846  3.39635    6.2058  18.2818  \n",
       "var_194   15.6298  17.9579   20.3965  27.9288  \n",
       "var_195   -1.1707  -0.1727    0.8296   4.2729  \n",
       "var_196  -1.94693   2.4089   6.55673  18.3215  \n",
       "var_197    8.2528   8.8882    9.5933  12.0004  \n",
       "var_198   13.8297  15.9341   18.0647  26.0791  \n",
       "var_199  -11.2085 -2.81955    4.8368  28.5007  \n",
       "\n",
       "[201 rows x 11 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[[i for i in train_data.columns.tolist() if i not in ['target','id']]].describe(include='all').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_name</th>\n",
       "      <th>col_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_code</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  col_name col_type\n",
       "0  ID_code   object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_type = train_data.dtypes.to_frame().reset_index()\n",
    "data_type.columns  = ['col_name','col_type']\n",
    "data_type[data_type.col_type==np.object].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isnull().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [i for i in train_data.columns if i not in ['target','ID_code']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_variance_scree_plot(variance, cumulative_variance = None, title = 'Principal Component vs Explained Ratio', n_components = 14):\n",
    "    with plot.style.context('seaborn-whitegrid'):\n",
    "        plot.figure(figsize=(10, 5))\n",
    "\n",
    "        plot.bar(range(1,n_components), variance, alpha=0.8, align='center',\n",
    "                label='individual explained variance', color = 'teal') ## since there are 13 dimenison\n",
    "        plot.step(range(1,n_components), cumulative_variance, where='mid',\n",
    "                 label='cumulative explained variance', color ='red') if cumulative_variance is not None else None\n",
    "        plot.plot(variance, color = 'black', label='scree')\n",
    "        plot.ylabel('Explained variance ratio')\n",
    "        plot.xlabel('Principal components')\n",
    "        plot.legend(loc='best')\n",
    "        plot.tight_layout()\n",
    "        plot.title(title)\n",
    "        plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data[col_names]\n",
    "y = train_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20,), (20, 200), (200000, 20))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=20)\n",
    "x_std = StandardScaler().fit_transform(x)\n",
    "x_pca = pca.fit_transform(x_std)\n",
    "explained_variance = pca.explained_variance_\n",
    "explained_variance.shape, pca.components_.shape,x_pca.shape\n",
    "# plot_variance_scree_plot(explained_variance)\n",
    "# x_test_pca1 = pca.transform(x_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20.380157357102245, 20, (20,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot = sum(explained_variance)\n",
    "var_exp = [(i / tot)*100 for i in sorted(explained_variance, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "tot, len(var_exp), cum_var_exp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_variance_scree_plot(var_exp,cum_var_exp, n_components = pca.components_.shape[0] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: Run handful of usual suspect classifiers using selected 50 features giving highest coeff scores\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeTimer:\n",
    "    \n",
    "    \"\"\"\n",
    "        Utility custom contextual class for calculating the time \n",
    "        taken for a certain code block to execute\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, name=None):\n",
    "        self.name = \" '\"  + name + \"'\" if name else ''\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = time.clock()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.took = (time.clock() - self.start) * 1000.0\n",
    "        time_taken = datetime.timedelta(milliseconds = self.took)\n",
    "        print('Code block' + self.name + ' took(HH:MM:SS): ' + str(time_taken))\n",
    "        \n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plot.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plot.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plot.title(title)\n",
    "    plot.colorbar()\n",
    "    tick_marks = arange(len(classes))\n",
    "    plot.xticks(tick_marks, classes, rotation=45)\n",
    "    plot.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plot.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plot.ylabel('True label')\n",
    "    plot.xlabel('Predicted label')\n",
    "#     plot.tight_layout()\n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plotLearningCurve(_x_train, _y_train, learning_model_pipeline,  model_name, k_fold = 10, training_sample_sizes = linspace(0.1,1.0,10), jobsInParallel = 1):\n",
    "    \n",
    "    print('num of classes : ',np.bincount(_y_train))\n",
    "    training_size, training_score, testing_score = learning_curve(estimator = learning_model_pipeline, \\\n",
    "                                                                X = _x_train, \\\n",
    "                                                                y = _y_train, \\\n",
    "                                                                train_sizes = training_sample_sizes, \\\n",
    "                                                                cv = k_fold, \\\n",
    "                                                                n_jobs = jobsInParallel) \n",
    "\n",
    "\n",
    "    training_mean = mean(training_score, axis = 1)\n",
    "    training_std_deviation = std(training_score, axis = 1)\n",
    "    testing_std_deviation = std(testing_score, axis = 1)\n",
    "    testing_mean = mean(testing_score, axis = 1 )\n",
    "\n",
    "    ## we have got the estimator in this case the perceptron running in 10 fold validation with \n",
    "    ## equal division of sizes betwwen .1 and 1. After execution, we get the number of training sizes used, \n",
    "    ## the training scores for those sizes and the test scores for those sizes. we will plot a scatter plot \n",
    "    ## to see the accuracy results and check for bias vs variance\n",
    "\n",
    "    # training_size : essentially 10 sets of say a1, a2, a3,,...a10 sizes (this comes from train_size parameter, here we have given linespace for equal distribution betwwen 0.1 and 1 for 10 such values)\n",
    "    # training_score : training score for the a1 samples, a2 samples...a10 samples, each samples run 10 times since cv value is 10\n",
    "    # testing_score : testing score for the a1 samples, a2 samples...a10 samples, each samples run 10 times since cv value is 10\n",
    "    ## the mean and std deviation for each are calculated simply to show ranges in the graph\n",
    "\n",
    "    plot.plot(training_size, training_mean, label= \"Training Data\", marker= '+', color = 'blue', markersize = 8)\n",
    "    plot.fill_between(training_size, training_mean+ training_std_deviation, training_mean-training_std_deviation, color='blue', alpha =0.12 )\n",
    "\n",
    "    plot.plot(training_size, testing_mean, label= \"Testing/Validation Data\", marker= '*', color = 'green', markersize = 8)\n",
    "    plot.fill_between(training_size, testing_mean+ training_std_deviation, testing_mean-training_std_deviation, color='green', alpha =0.14 )\n",
    "\n",
    "    plot.title(\"Scoring of our training and testing data vs sample sizes for model:\"+model_name)\n",
    "    plot.xlabel(\"Number of Samples\")\n",
    "    plot.ylabel(\"Accuracy\")\n",
    "    plot.legend(loc= 'best')\n",
    "    plot.show()\n",
    "    \n",
    "def plot_roc_auc_curve(false_positive_rate, true_positive_rate, model_name):\n",
    "        \n",
    "    plot.figure(figsize=(10,3))\n",
    "    plot.plot(list(false_positive_rate), list(true_positive_rate),  label = \"ROC Curve for model: \"+model_name)     \n",
    "    plot.plot([0, 1], [0, 1], 'k--', label = 'Random Guessing')\n",
    "    plot.plot([0, 0, 1], [0,1, 1], ':', label = 'Perfect Score')\n",
    "    auc_score = auc(false_positive_rate, true_positive_rate)\n",
    "    plot.title('ROC Curve for model: %s with AUC %.2f'%(model_name, auc_score))\n",
    "    plot.xlabel('False Positive Rate')\n",
    "    plot.ylabel('True Positive Rate')\n",
    "    plot.legend(loc='best')\n",
    "    plot.show()\n",
    "    \n",
    "    \n",
    "def plot_precision_recall_curve(precision, recall, model_name):\n",
    "    \n",
    "    plot.figure(figsize=(10,3))\n",
    "    plot.plot(list(recall), list(precision),  label = \"Precision/Recall Curve for model: \"+model_name)     \n",
    "#     plot.plot([0, 1], [0, 1], 'k--', label = 'Random Guessing') #\n",
    "    plot.title('Precision Recall Curve for model: %s'%model_name)\n",
    "    plot.xlabel('Recall')\n",
    "    plot.ylabel('Precision')\n",
    "    plot.legend(loc='best')\n",
    "    plot.show()\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runGridSearchAndPredict(pipeline,model_name, x_train, y_train, x_test, y_test, param_grid, n_jobs = 1, cv = 10, score = 'accuracy'):\n",
    "#     pass\n",
    "\n",
    "    response =  {}\n",
    "    training_timer       = CodeTimer('training')\n",
    "    testing_timer        = CodeTimer('testing')\n",
    "    learning_curve_timer = CodeTimer('learning_curve')\n",
    "    predict_proba_timer  = CodeTimer('predict_proba')\n",
    "    \n",
    "    with training_timer:\n",
    "        \n",
    "        gridsearch = GridSearchCV(estimator = pipeline, param_grid = param_grid, cv = cv, n_jobs = n_jobs, scoring = score)\n",
    "\n",
    "        search = gridsearch.fit(x_train,y_train)\n",
    "\n",
    "        print(\"Grid Search Best parameters \", search.best_params_)\n",
    "        print(\"Grid Search Best score \", search.best_score_)\n",
    "\n",
    "    with testing_timer:\n",
    "        y_prediction = gridsearch.predict(x_test)\n",
    "            \n",
    "    print(\"F1 score %s\" %f1_score(y_test,y_prediction, average ='weighted'))\n",
    "    print(\"Classification report  \\n %s\" %(classification_report(y_test, y_prediction)))\n",
    "    \n",
    "    with learning_curve_timer:\n",
    "        plotLearningCurve(x_train, y_train, search.best_estimator_, model_name, k_fold=cv)\n",
    "#         _matrix = confusion_matrix(y_true = _y_test ,y_pred = y_prediction, labels = list(range(_y_test.shape[1])))\n",
    "        _matrix = confusion_matrix(y_true = y_test ,y_pred = y_prediction, labels = list(set(y_test)))\n",
    "        classes = list(set(y_test))\n",
    "        plot_confusion_matrix(_matrix, classes, title = \"Confusion matrix for model:\"+model_name)\n",
    "        \n",
    "    with predict_proba_timer:\n",
    "\n",
    "        if hasattr(gridsearch.best_estimator_, 'predict_proba'):\n",
    "            \n",
    "            print('inside decision function')\n",
    "            y_probability = gridsearch.predict_proba(x_test)\n",
    "            number_of_classes = len(np.unique(y_train))\n",
    "            false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_probability[:, 1])\n",
    "            response['roc_auc_score'] = roc_auc_score(y_test, y_probability[:,1])\n",
    "            response['roc_curve'] = (false_positive_rate, true_positive_rate)  \n",
    "            response['roc_curve_false_positive_rate'] = false_positive_rate\n",
    "            response['roc_curve_true_positive_rate'] = true_positive_rate\n",
    "            precision, recall, _ = precision_recall_curve(y_test, y_probability[:,1])\n",
    "            plot_roc_auc_curve(false_positive_rate, true_positive_rate, model_name)\n",
    "            plot_precision_recall_curve(precision, recall, model_name)\n",
    "            \n",
    "        else: ## eg SVM, Perceptron doesnt have predict_proba method\n",
    "            \n",
    "            response['roc_auc_score'] = 0\n",
    "            response['roc_curve'] = 0\n",
    "            response['roc_curve_false_positive_rate'] = 0\n",
    "            response['roc_curve_true_positive_rate'] = 0\n",
    "    \n",
    "    response['learning_curve_time'] = learning_curve_timer.took\n",
    "    response['testing_time'] = testing_timer.took\n",
    "    response['_y_prediction'] = y_prediction\n",
    "#     response['accuracy_score'] = accuracy_score(y_test,y_prediction)\n",
    "    response['training_time'] = training_timer.took\n",
    "    response['f1_score']  = f1_score(y_test, y_prediction, average ='weighted')\n",
    "    response['f1_score_micro']  = f1_score(y_test, y_prediction, average ='micro')\n",
    "    response['f1_score_macro']  = f1_score(y_test, y_prediction, average ='macro')\n",
    "    response['best_estimator'] = search.best_estimator_\n",
    "    response['confusion_matrix'] = _matrix\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "def plotROCCurveAcrossModels(positive_rates_sequence, model_name):\n",
    "    \n",
    "    plot.figure(figsize=(10,5))\n",
    "    for plot_values, label_name in zip(positive_rates_sequence, model_name):\n",
    "        \n",
    "        plot.plot(list(plot_values[0]), list(plot_values[1]),  label = \"ROC Curve for model: \"+label_name)\n",
    "        \n",
    "    plot.plot([0, 1], [0, 1], 'k--', label = 'Random Guessing') #\n",
    "    plot.title('ROC Curve across models')\n",
    "    plot.xlabel('False Positive Rate')\n",
    "    plot.ylabel('True Positive Rate')\n",
    "    plot.legend(loc='best')\n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def execute( _x_train,\n",
    "             _y_train,\n",
    "             _x_test,\n",
    "             _y_test, \n",
    "            classifiers, \n",
    "            classifier_names, \n",
    "            classifier_param_grid,\n",
    "            cv  = 10 , \n",
    "            score = 'accuracy',\n",
    "            scaler = StandardScaler()\n",
    "           ):\n",
    "    \n",
    "    '''\n",
    "    This method will run your data sets against the model specified \n",
    "    Models will be fed through a pipeline where the first step would be to\n",
    "    execute a scaling operation.\n",
    "    \n",
    "    Method will also call additional lower level methods in order to plot\n",
    "    precision curve, roc curve, learning curve and will also prepare a confusion matrix\n",
    "    \n",
    "    :returns: dict containing execution metrics such as time taken, accuracy scores\n",
    "    :returntype: dict\n",
    "    \n",
    "    '''\n",
    "\n",
    "    timer = CodeTimer(name='overalltime')\n",
    "    model_metrics = {}\n",
    "\n",
    "    with timer:\n",
    "        for model, model_name, model_param_grid in zip(classifiers, classifier_names, classifier_param_grid):\n",
    "\n",
    "            pipeline_steps = [('scaler', scaler),(model_name, model)] if scaler is not None else [(model_name, model)]\n",
    "            pipeline = Pipeline(pipeline_steps)\n",
    "\n",
    "            result = runGridSearchAndPredict(pipeline, \n",
    "                                             model_name,\n",
    "                                             _x_train,\n",
    "                                             _y_train,\n",
    "                                             _x_test,\n",
    "                                             _y_test, \n",
    "                                             model_param_grid ,\n",
    "                                             cv = cv,\n",
    "                                             score = score)\n",
    "\n",
    "            _y_prediction = result['_y_prediction']\n",
    "\n",
    "            model_metrics[model_name] = {}\n",
    "            model_metrics[model_name]['confusion_matrix'] = result.get('confusion_matrix')\n",
    "            model_metrics[model_name]['training_time'] = result.get('training_time')\n",
    "            model_metrics[model_name]['testing_time'] = result.get('testing_time')\n",
    "            model_metrics[model_name]['learning_curve_time'] = result.get('learning_curve_time')\n",
    "            model_metrics[model_name]['f1_score'] = result.get('f1_score')\n",
    "            model_metrics[model_name]['f1_score_macro'] = result.get('f1_score_macro')\n",
    "            model_metrics[model_name]['f1_score_micro'] = result.get('f1_score_micro')\n",
    "            model_metrics[model_name]['roc_auc_score'] = result.get('roc_auc_score')\n",
    "            model_metrics[model_name]['roc_curve_true_positive_rate'] = result.get('roc_curve_true_positive_rate')\n",
    "            model_metrics[model_name]['roc_curve_false_positive_rate'] = result.get('roc_curve_false_positive_rate')\n",
    "\n",
    "            model_metrics[model_name]['best_estimator'] = result.get('best_estimator')\n",
    "\n",
    "\n",
    "    print(timer.took)\n",
    "    \n",
    "    return model_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifiers = [\n",
    "    Perceptron(random_state = 1),\n",
    "    LogisticRegression(random_state = 1),\n",
    "#     LogisticRegression(random_state = 1, solver='liblinear'),\n",
    "#     LogisticRegression(random_state = 1, solver='newton-cg'),\n",
    "#     LogisticRegression(random_state = 1, solver='sag'),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(random_state = 1),\n",
    "    KNeighborsClassifier(metric = 'minkowski'),\n",
    "    RidgeClassifier(random_state = 123), \n",
    "#     SVC(kernel=\"linear\"),\n",
    "#     SVC(),\n",
    "#     ExtraTreeClassifier(random_state = 123),\n",
    "#     GaussianProcessClassifier(random_state = 123),\n",
    "#     BernoulliNB(),\n",
    "#     BaggingClassifier(base_estimator = LogisticRegression(random_state = 1)),\n",
    "#     BaggingClassifier(base_estimator = BernoulliNB()),\n",
    "#     GradientBoostingClassifier(random_state= 123),\n",
    "#     LGBMClassifier(objective = 'binary'),\n",
    "#     XGBClassifier(objective = 'binary:logistic')\n",
    "]\n",
    "\n",
    "\n",
    "classifier_names = [\n",
    "            'perceptron',\n",
    "            'logisticregression',\n",
    "#             'logisticregression_liblinear_l2',\n",
    "#             'logisticregression_newton_cg',\n",
    "#             'logisticregression_sag',\n",
    "            'decisiontreeclassifier',\n",
    "            'randomforestclassifier',\n",
    "            'kneighborsclassifier',\n",
    "            'ridge',\n",
    "#             'linear_svc',\n",
    "#             'gamma_svc',\n",
    "#             'extra_trees',\n",
    "#             'gaussian_process',\n",
    "#             'bernoulli',\n",
    "#             'bagging_logistic',\n",
    "#             'bagging_bernoulli',\n",
    "#             'gradient_boosting_classifier',\n",
    "#             'lgbm_classifier',\n",
    "#             'xgb'\n",
    "]\n",
    "\n",
    "classifier_param_grid = [\n",
    "            \n",
    "            {'perceptron__max_iter': [5,10,30], 'perceptron__eta0': [.1]},\n",
    "            {\n",
    "             'logisticregression__C':[1.2,0.02,2.2,4, 0.01, 0.05], \n",
    "             'logisticregression__penalty':['l1','l2'],\n",
    "             'logisticregression__solver':['saga','liblinear']\n",
    "            },\n",
    "#             {\n",
    "#              'logisticregression_liblinear_l2__C':[1.2,0.02,2.2,4, 0.01, 0.05], \n",
    "#              'logisticregression_liblinear_l2__penalty':['l2'],\n",
    "#              'logisticregression_liblinear_l2__dual':[True]\n",
    "#             },\n",
    "#             {\n",
    "#              'logisticregression_newton_cg__C':[1.2,0.02,2.2,4, 0.01, 0.05], \n",
    "#              'logisticregression_newton_cg__penalty':['l2'],\n",
    "#             },\n",
    "#             {\n",
    "#              'logisticregression_sag__C':[1.2,0.02,2.2,4, 0.01, 0.05], \n",
    "#              'logisticregression_sag__penalty':['l2'],\n",
    "#             },\n",
    "    \n",
    "            {'decisiontreeclassifier__max_depth':[6,8,10],\n",
    "             'decisiontreeclassifier__criterion':['gini','entropy'],\n",
    "             'decisiontreeclassifier__max_features':['auto','sqrt','log2'],\n",
    "            },\n",
    "            {'randomforestclassifier__n_estimators':[6,8,12],'randomforestclassifier__criterion': ['gini','entropy']} ,\n",
    "            {'kneighborsclassifier__n_neighbors':[4,6,10]},\n",
    "            {'ridge__alpha':[1,1.2,0.9],'ridge__max_iter':[100,300,500]},\n",
    "#             {'linear_svc__C':[0.025]},\n",
    "#             {'gamma_svc__gamma':[2,4],'gamma_svc__C':[1,5]},\n",
    "#             {'extra_trees__max_depth':[6,8,12],'extra_trees__criterion': ['gini','entropy']} ,\n",
    "#             {'gaussian_process__max_iter_predict':[200,400]} ,\n",
    "#             {'bernoulli__alpha':[0.2,0.6,1.2]} ,\n",
    "#             {'bagging_logistic__base_estimator__C':[1.2,0.02,2.2,4], \n",
    "#              'bagging_logistic__base_estimator__penalty':['l1','l2'],\n",
    "#              'bagging_logistic__n_estimators': [5,8,10]\n",
    "#             },\n",
    "#             {'bagging_bernoulli__base_estimator__alpha':[1.2,0.02,2.2,4], \n",
    "#              'bagging_bernoulli__n_estimators': [5,8,10]\n",
    "#             },\n",
    "#             {\n",
    "#                 'gradient_boosting_classifier__loss':['deviance','exponential'],\n",
    "#                 'gradient_boosting_classifier__learning_rate':[0.5,1.2],\n",
    "#                 'gradient_boosting_classifier__n_estimators':[100,500,1000],\n",
    "#                 'gradient_boosting_classifier__criterion':['friedman_mse','mse','mae'],\n",
    "#                 'gradient_boosting_classifier__max_depth':[6,8,16,20],\n",
    "#             },\n",
    "#             {\n",
    "#                  'lgbm_classifier__num_leaves':[25,], \\\n",
    "# #                  'lgbm_classifier__min_data_in_leaf':[20],\\\n",
    "#                  'lgbm_classifier__max_depth':[20,], \\\n",
    "#                  'lgbm_classifier__learning_rate' : [0.01,],\\\n",
    "#                  'lgbm_classifier__min_child_samples' :[2,], \\\n",
    "#                  'lgbm_classifier__n_estimators' : [5000,], \\\n",
    "#                  'lgbm_classifier__num_boost_round' : [100], \\\n",
    "#                  'lgbm_classifier__feature_fraction' : [0.9,], \\\n",
    "#                  'lgbm_classifier__bagging_freq' : [1,], \\\n",
    "#                  'lgbm_classifier__bagging_seed' : [123], \\\n",
    "#             },\n",
    "#              {\n",
    "#                 'xgb__max_depth':[6,8,10],\n",
    "#                  'xgb__learning_rate':[0.1,0.5,1,2],\n",
    "#                  'xgb__n_estimators':[100,400,1000],             \n",
    "#                  'xgb__booster':['gbtree','dart'],\n",
    "#                  'xgb__subsample':[0.5, 0.2,0.8]\n",
    "#             },\n",
    "    \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "tl = TomekLinks(return_indices=True, ratio='majority')\n",
    "_x_train_tomek, _y_train_tomek, id_tl = tl.fit_sample(x_train, y_train)\n",
    "\n",
    "smt = SMOTETomek(ratio='auto')\n",
    "_x_train_smt, _y_train_smt = smt.fit_sample(x_train, y_train)\n",
    "\n",
    "smote = SMOTE(ratio='minority')\n",
    "x_train_smote, y_train_smote = smote.fit_sample(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((251862, 1), (251862,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_smote.shape, y_train_smote.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = panda.read_csv('data/test.csv')\n",
    "\n",
    "\n",
    "test_x= pca.transform(test_data[[i for i in test_data.columns.tolist() if i not in ['ID_code']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 20)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits = 5, shuffle= True, random_state =123)\n",
    "score= 'roc_auc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "        'num_leaves': 24,\n",
    "        'max_bin': 63,\n",
    "        'min_data_in_leaf': 45,\n",
    "        'learning_rate': 0.01,\n",
    "        'min_sum_hessian_in_leaf': 0.000446,\n",
    "        'bagging_fraction': 0.55, \n",
    "        'bagging_freq': 5, \n",
    "        'max_depth': 14,\n",
    "        'save_binary': True,\n",
    "        'seed': 31452,\n",
    "        'feature_fraction_seed': 31415,\n",
    "        'feature_fraction': 0.51,\n",
    "        'bagging_seed': 31415,\n",
    "        'drop_seed': 31415,\n",
    "        'data_random_seed': 31415,\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbose': 1,\n",
    "        'metric': 'auc',\n",
    "        'is_unbalance': True,\n",
    "        'boost_from_average': False,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 200)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Training until validation scores don't improve for 250 rounds.\n",
      "[1000]\ttraining's auc: 0.915412\tvalid_1's auc: 0.876724\n",
      "[2000]\ttraining's auc: 0.941492\tvalid_1's auc: 0.891565\n",
      "[3000]\ttraining's auc: 0.955046\tvalid_1's auc: 0.895873\n",
      "[4000]\ttraining's auc: 0.965157\tvalid_1's auc: 0.897144\n",
      "Early stopping, best iteration is:\n",
      "[4673]\ttraining's auc: 0.971066\tvalid_1's auc: 0.897354\n",
      "[0.00587479 0.04822963 0.05947965 ... 0.1459012  0.00459855 0.06272266]\n",
      "[0.         0.         0.         ... 0.31361328 0.         0.        ]\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 250 rounds.\n",
      "[1000]\ttraining's auc: 0.915083\tvalid_1's auc: 0.877601\n",
      "[2000]\ttraining's auc: 0.941113\tvalid_1's auc: 0.892309\n",
      "[3000]\ttraining's auc: 0.954828\tvalid_1's auc: 0.896224\n",
      "[4000]\ttraining's auc: 0.96505\tvalid_1's auc: 0.897427\n",
      "Early stopping, best iteration is:\n",
      "[4710]\ttraining's auc: 0.971254\tvalid_1's auc: 0.897705\n",
      "[0.00711451 0.09877276 0.06817989 ... 0.00472537 0.19054811 0.0061103 ]\n",
      "[0.         0.         0.03557257 ... 0.31361328 0.         0.0305515 ]\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 250 rounds.\n",
      "[1000]\ttraining's auc: 0.915305\tvalid_1's auc: 0.877728\n",
      "[2000]\ttraining's auc: 0.941192\tvalid_1's auc: 0.892029\n",
      "[3000]\ttraining's auc: 0.954794\tvalid_1's auc: 0.896118\n",
      "[4000]\ttraining's auc: 0.964936\tvalid_1's auc: 0.897059\n",
      "Early stopping, best iteration is:\n",
      "[4574]\ttraining's auc: 0.970058\tvalid_1's auc: 0.897341\n",
      "[0.14481609 0.02387405 0.05206804 ... 0.01028385 0.11694206 0.03125562]\n",
      "[0.         0.72408043 0.03557257 ... 0.31361328 0.         0.0305515 ]\n",
      "Fold 3\n",
      "Training until validation scores don't improve for 250 rounds.\n",
      "[1000]\ttraining's auc: 0.916885\tvalid_1's auc: 0.872656\n",
      "[2000]\ttraining's auc: 0.942292\tvalid_1's auc: 0.887101\n",
      "[3000]\ttraining's auc: 0.955394\tvalid_1's auc: 0.891628\n",
      "[4000]\ttraining's auc: 0.965486\tvalid_1's auc: 0.893186\n",
      "Early stopping, best iteration is:\n",
      "[4322]\ttraining's auc: 0.968367\tvalid_1's auc: 0.893389\n",
      "[0.07269405 0.01313227 0.15711372 ... 0.01152124 0.0102726  0.05736275]\n",
      "[0.         0.72408043 0.03557257 ... 0.31361328 0.         0.0305515 ]\n",
      "Fold 4\n",
      "Training until validation scores don't improve for 250 rounds.\n",
      "[1000]\ttraining's auc: 0.915715\tvalid_1's auc: 0.875346\n",
      "[2000]\ttraining's auc: 0.941297\tvalid_1's auc: 0.890312\n",
      "[3000]\ttraining's auc: 0.954841\tvalid_1's auc: 0.894653\n",
      "[4000]\ttraining's auc: 0.965047\tvalid_1's auc: 0.896086\n",
      "Early stopping, best iteration is:\n",
      "[4163]\ttraining's auc: 0.966494\tvalid_1's auc: 0.896138\n",
      "[0.01463343 0.14754044 0.08064052 ... 0.01181093 0.07489736 0.07279507]\n",
      "[0.07316714 0.72408043 0.03557257 ... 0.31361328 0.36397533 0.0305515 ]\n"
     ]
    }
   ],
   "source": [
    "oof = np.zeros(x.shape[0])\n",
    "predictions = np.zeros(x.shape[0])\n",
    "\n",
    "feature_importance_df = panda.DataFrame()\n",
    "target = y.values\n",
    "for fold_, (trn_idx, val_idx) in enumerate(cv.split(x, y)):\n",
    "    \n",
    "    print(\"Fold {}\".format(fold_))\n",
    "    \n",
    "    trn_data = lgb.Dataset(x.iloc[trn_idx], label=y.iloc[trn_idx])\n",
    "    val_data = lgb.Dataset(x.iloc[val_idx], label=y.iloc[val_idx])\n",
    "\n",
    "    num_round = 10000\n",
    "\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 250)\n",
    "    \n",
    "    oof[val_idx] = clf.predict(x.iloc[val_idx], num_iteration=clf.best_iteration)\n",
    "    print(clf.predict(x.iloc[val_idx], num_iteration=clf.best_iteration)/cv.n_splits)\n",
    "    print(oof)    \n",
    "#     fold_importance_df = pd.DataFrame()\n",
    "    \n",
    "#     fold_importance_df[\"Feature\"] = features\n",
    "#     fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "#     fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    \n",
    "#     feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "#     predictions += clf.predict(test_x, num_iteration=clf.best_iteration) / cv.n_splits\n",
    "#     print(predictions)\n",
    "\n",
    "# print(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "\n",
    "1. We got highest scores of ROC/AUC 0.92 in logistic regression with l2 parameter\n",
    "\n",
    "2. From the learning curve, we also see that overfitting tendency is less\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_x = lda.transform(test_data[[i for i in test_data.columns.tolist() if i not in ['ID_code']]])\n",
    "test_x=test_data[[i for i in test_data.columns.tolist() if i not in ['ID_code']]]\n",
    "test_x.shape\n",
    "test_x=lda.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target = best_estimator.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = panda.read_csv('data/sample_submission.csv')\n",
    "submission['target'] = test_target\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('sample_submission_1.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
