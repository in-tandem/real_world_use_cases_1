{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Santandar Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as panda\n",
    "from matplotlib import pyplot as plot\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as panda\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis,QuadraticDiscriminantAnalysis\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler,RobustScaler,label_binarize\n",
    "\n",
    "from sklearn.model_selection import train_test_split,KFold,StratifiedKFold,GridSearchCV,RepeatedStratifiedKFold,learning_curve\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, roc_curve, auc, precision_recall_curve, \\\n",
    "        classification_report,confusion_matrix,average_precision_score\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression,RidgeClassifier,SGDClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,VotingClassifier,AdaBoostClassifier,GradientBoostingClassifier,BaggingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot as plot\n",
    "from itertools import cycle\n",
    "import numpy as np \n",
    "from scipy import interp\n",
    "import seaborn as sns\n",
    "import itertools, time, datetime\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif,f_classif\n",
    "from numpy import bincount, linspace, mean, std, arange, squeeze\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(143)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = panda.read_csv('data/train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    179902\n",
       " 1     20098\n",
       " Name: target, dtype: int64, (200000, 202))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.target.value_counts(), train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['target'] = train_data.target.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot.figure(figsize=(10,4))\n",
    "train_data.target.value_counts().plot(kind='bar')\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ID_code</th>\n",
       "      <td>200000</td>\n",
       "      <td>200000</td>\n",
       "      <td>train_131342</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_0</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.6799</td>\n",
       "      <td>3.04005</td>\n",
       "      <td>0.4084</td>\n",
       "      <td>8.45385</td>\n",
       "      <td>10.5248</td>\n",
       "      <td>12.7582</td>\n",
       "      <td>20.315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_1</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.62762</td>\n",
       "      <td>4.05004</td>\n",
       "      <td>-15.0434</td>\n",
       "      <td>-4.74003</td>\n",
       "      <td>-1.60805</td>\n",
       "      <td>1.35862</td>\n",
       "      <td>10.3768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_2</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.7152</td>\n",
       "      <td>2.64089</td>\n",
       "      <td>2.1171</td>\n",
       "      <td>8.72247</td>\n",
       "      <td>10.58</td>\n",
       "      <td>12.5167</td>\n",
       "      <td>19.353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_3</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.79653</td>\n",
       "      <td>2.04332</td>\n",
       "      <td>-0.0402</td>\n",
       "      <td>5.25408</td>\n",
       "      <td>6.825</td>\n",
       "      <td>8.3241</td>\n",
       "      <td>13.1883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_4</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0783</td>\n",
       "      <td>1.62315</td>\n",
       "      <td>5.0748</td>\n",
       "      <td>9.88317</td>\n",
       "      <td>11.1082</td>\n",
       "      <td>12.2611</td>\n",
       "      <td>16.6714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_5</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5.06532</td>\n",
       "      <td>7.86327</td>\n",
       "      <td>-32.5626</td>\n",
       "      <td>-11.2004</td>\n",
       "      <td>-4.83315</td>\n",
       "      <td>0.9248</td>\n",
       "      <td>17.2516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_6</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.40895</td>\n",
       "      <td>0.866607</td>\n",
       "      <td>2.3473</td>\n",
       "      <td>4.7677</td>\n",
       "      <td>5.3851</td>\n",
       "      <td>6.003</td>\n",
       "      <td>8.4477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_7</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.5458</td>\n",
       "      <td>3.41808</td>\n",
       "      <td>5.3497</td>\n",
       "      <td>13.9438</td>\n",
       "      <td>16.4568</td>\n",
       "      <td>19.1029</td>\n",
       "      <td>27.6918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_8</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.284162</td>\n",
       "      <td>3.33263</td>\n",
       "      <td>-10.5055</td>\n",
       "      <td>-2.3178</td>\n",
       "      <td>0.3937</td>\n",
       "      <td>2.9379</td>\n",
       "      <td>10.1513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_9</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.56724</td>\n",
       "      <td>1.23507</td>\n",
       "      <td>3.9705</td>\n",
       "      <td>6.6188</td>\n",
       "      <td>7.6296</td>\n",
       "      <td>8.58442</td>\n",
       "      <td>11.1506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_10</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.39434</td>\n",
       "      <td>5.50079</td>\n",
       "      <td>-20.7313</td>\n",
       "      <td>-3.59495</td>\n",
       "      <td>0.4873</td>\n",
       "      <td>4.38293</td>\n",
       "      <td>18.6702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_11</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.2456</td>\n",
       "      <td>5.97025</td>\n",
       "      <td>-26.095</td>\n",
       "      <td>-7.5106</td>\n",
       "      <td>-3.28695</td>\n",
       "      <td>0.852825</td>\n",
       "      <td>17.1887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_12</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.024</td>\n",
       "      <td>0.190059</td>\n",
       "      <td>13.4346</td>\n",
       "      <td>13.894</td>\n",
       "      <td>14.0255</td>\n",
       "      <td>14.1642</td>\n",
       "      <td>14.6545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_13</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.53023</td>\n",
       "      <td>4.63954</td>\n",
       "      <td>-6.0111</td>\n",
       "      <td>5.0728</td>\n",
       "      <td>8.60425</td>\n",
       "      <td>12.2748</td>\n",
       "      <td>22.3315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_14</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.53761</td>\n",
       "      <td>2.24791</td>\n",
       "      <td>1.0133</td>\n",
       "      <td>5.78188</td>\n",
       "      <td>7.5203</td>\n",
       "      <td>9.27042</td>\n",
       "      <td>14.9377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_15</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.5731</td>\n",
       "      <td>0.411711</td>\n",
       "      <td>13.0769</td>\n",
       "      <td>14.2628</td>\n",
       "      <td>14.5741</td>\n",
       "      <td>14.8745</td>\n",
       "      <td>15.8633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_16</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.33326</td>\n",
       "      <td>2.55742</td>\n",
       "      <td>0.6351</td>\n",
       "      <td>7.45228</td>\n",
       "      <td>9.23205</td>\n",
       "      <td>11.0559</td>\n",
       "      <td>17.9506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_17</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5.69673</td>\n",
       "      <td>6.71261</td>\n",
       "      <td>-33.3802</td>\n",
       "      <td>-10.4762</td>\n",
       "      <td>-5.66635</td>\n",
       "      <td>-0.810775</td>\n",
       "      <td>19.0259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_18</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.244</td>\n",
       "      <td>7.85137</td>\n",
       "      <td>-10.6642</td>\n",
       "      <td>9.17795</td>\n",
       "      <td>15.1962</td>\n",
       "      <td>21.0133</td>\n",
       "      <td>41.748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_19</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.4386</td>\n",
       "      <td>7.99669</td>\n",
       "      <td>-12.4025</td>\n",
       "      <td>6.27648</td>\n",
       "      <td>12.4539</td>\n",
       "      <td>18.4333</td>\n",
       "      <td>35.183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_20</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.2909</td>\n",
       "      <td>5.87625</td>\n",
       "      <td>-5.4322</td>\n",
       "      <td>8.6278</td>\n",
       "      <td>13.1968</td>\n",
       "      <td>17.8794</td>\n",
       "      <td>31.2859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_21</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.2579</td>\n",
       "      <td>8.19656</td>\n",
       "      <td>-10.089</td>\n",
       "      <td>11.551</td>\n",
       "      <td>17.2343</td>\n",
       "      <td>23.0891</td>\n",
       "      <td>49.0443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_22</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.30543</td>\n",
       "      <td>2.84796</td>\n",
       "      <td>-5.3225</td>\n",
       "      <td>2.1824</td>\n",
       "      <td>4.27515</td>\n",
       "      <td>6.2932</td>\n",
       "      <td>14.5945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_23</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.01954</td>\n",
       "      <td>0.526893</td>\n",
       "      <td>1.2098</td>\n",
       "      <td>2.6341</td>\n",
       "      <td>3.00865</td>\n",
       "      <td>3.4038</td>\n",
       "      <td>4.8752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_24</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.5844</td>\n",
       "      <td>3.77725</td>\n",
       "      <td>-0.6784</td>\n",
       "      <td>7.613</td>\n",
       "      <td>10.3803</td>\n",
       "      <td>13.4796</td>\n",
       "      <td>25.446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_25</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.6675</td>\n",
       "      <td>0.285535</td>\n",
       "      <td>12.72</td>\n",
       "      <td>13.4564</td>\n",
       "      <td>13.6625</td>\n",
       "      <td>13.8637</td>\n",
       "      <td>14.6546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_26</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4.05513</td>\n",
       "      <td>5.92221</td>\n",
       "      <td>-24.2431</td>\n",
       "      <td>-8.32173</td>\n",
       "      <td>-4.1969</td>\n",
       "      <td>-0.0902</td>\n",
       "      <td>15.6751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_27</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.13791</td>\n",
       "      <td>1.52371</td>\n",
       "      <td>-6.1668</td>\n",
       "      <td>-2.3079</td>\n",
       "      <td>-1.1321</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>3.2431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_28</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.53298</td>\n",
       "      <td>0.783367</td>\n",
       "      <td>2.0896</td>\n",
       "      <td>4.9921</td>\n",
       "      <td>5.53485</td>\n",
       "      <td>6.0937</td>\n",
       "      <td>8.7874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_170</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.00496206</td>\n",
       "      <td>4.42462</td>\n",
       "      <td>-14.506</td>\n",
       "      <td>-3.2585</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>3.0964</td>\n",
       "      <td>16.7319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_171</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.831777</td>\n",
       "      <td>5.37801</td>\n",
       "      <td>-22.4793</td>\n",
       "      <td>-4.72035</td>\n",
       "      <td>-0.80735</td>\n",
       "      <td>2.9568</td>\n",
       "      <td>17.9173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_172</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.8171</td>\n",
       "      <td>8.67417</td>\n",
       "      <td>-11.4533</td>\n",
       "      <td>13.7318</td>\n",
       "      <td>19.748</td>\n",
       "      <td>25.9077</td>\n",
       "      <td>53.5919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_173</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.677967</td>\n",
       "      <td>5.96667</td>\n",
       "      <td>-22.7487</td>\n",
       "      <td>-5.00953</td>\n",
       "      <td>-0.56975</td>\n",
       "      <td>3.6199</td>\n",
       "      <td>18.8554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_174</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.2107</td>\n",
       "      <td>7.13643</td>\n",
       "      <td>-2.9953</td>\n",
       "      <td>15.0646</td>\n",
       "      <td>20.2061</td>\n",
       "      <td>25.6412</td>\n",
       "      <td>43.5468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_175</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.6406</td>\n",
       "      <td>2.89217</td>\n",
       "      <td>3.2415</td>\n",
       "      <td>9.3716</td>\n",
       "      <td>11.6798</td>\n",
       "      <td>13.7455</td>\n",
       "      <td>20.8548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_176</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.79958</td>\n",
       "      <td>7.51394</td>\n",
       "      <td>-29.1165</td>\n",
       "      <td>-8.3865</td>\n",
       "      <td>-2.53845</td>\n",
       "      <td>2.7044</td>\n",
       "      <td>20.2452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_177</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.8829</td>\n",
       "      <td>2.62889</td>\n",
       "      <td>4.9521</td>\n",
       "      <td>9.80867</td>\n",
       "      <td>11.7372</td>\n",
       "      <td>13.9313</td>\n",
       "      <td>20.5965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_178</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.01406</td>\n",
       "      <td>8.57981</td>\n",
       "      <td>-29.2734</td>\n",
       "      <td>-7.3957</td>\n",
       "      <td>-0.94205</td>\n",
       "      <td>5.33875</td>\n",
       "      <td>29.8413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_179</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.59144</td>\n",
       "      <td>2.79896</td>\n",
       "      <td>-7.8561</td>\n",
       "      <td>0.625575</td>\n",
       "      <td>2.5123</td>\n",
       "      <td>4.39112</td>\n",
       "      <td>13.4487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_180</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.74167</td>\n",
       "      <td>5.26124</td>\n",
       "      <td>-22.0374</td>\n",
       "      <td>-6.6739</td>\n",
       "      <td>-2.6888</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>12.7505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_181</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0855</td>\n",
       "      <td>1.37186</td>\n",
       "      <td>5.4165</td>\n",
       "      <td>9.0847</td>\n",
       "      <td>10.036</td>\n",
       "      <td>11.0113</td>\n",
       "      <td>14.3939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_182</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.719109</td>\n",
       "      <td>8.96343</td>\n",
       "      <td>-26.0011</td>\n",
       "      <td>-6.06442</td>\n",
       "      <td>0.7202</td>\n",
       "      <td>7.49918</td>\n",
       "      <td>29.2487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_183</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.76909</td>\n",
       "      <td>4.47492</td>\n",
       "      <td>-4.8082</td>\n",
       "      <td>5.4231</td>\n",
       "      <td>8.6</td>\n",
       "      <td>12.1274</td>\n",
       "      <td>23.7049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_184</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.7567</td>\n",
       "      <td>9.31828</td>\n",
       "      <td>-18.4897</td>\n",
       "      <td>5.6633</td>\n",
       "      <td>12.521</td>\n",
       "      <td>19.4562</td>\n",
       "      <td>44.3634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_185</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.98326</td>\n",
       "      <td>4.72517</td>\n",
       "      <td>-22.5833</td>\n",
       "      <td>-7.36</td>\n",
       "      <td>-3.94695</td>\n",
       "      <td>-0.59065</td>\n",
       "      <td>12.9975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_186</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.97027</td>\n",
       "      <td>3.18976</td>\n",
       "      <td>-3.0223</td>\n",
       "      <td>6.7152</td>\n",
       "      <td>8.90215</td>\n",
       "      <td>11.1938</td>\n",
       "      <td>21.7392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_187</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-10.335</td>\n",
       "      <td>11.5747</td>\n",
       "      <td>-47.7536</td>\n",
       "      <td>-19.2051</td>\n",
       "      <td>-10.2097</td>\n",
       "      <td>-1.466</td>\n",
       "      <td>22.7861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_188</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.3772</td>\n",
       "      <td>3.9446</td>\n",
       "      <td>4.4123</td>\n",
       "      <td>12.5016</td>\n",
       "      <td>15.2394</td>\n",
       "      <td>18.3452</td>\n",
       "      <td>29.3303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_189</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.746072</td>\n",
       "      <td>0.976348</td>\n",
       "      <td>-2.5543</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.7426</td>\n",
       "      <td>1.4829</td>\n",
       "      <td>4.0341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_190</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.23444</td>\n",
       "      <td>4.55992</td>\n",
       "      <td>-14.0933</td>\n",
       "      <td>-0.058825</td>\n",
       "      <td>3.2036</td>\n",
       "      <td>6.4062</td>\n",
       "      <td>18.4409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_191</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.43841</td>\n",
       "      <td>3.02327</td>\n",
       "      <td>-2.6917</td>\n",
       "      <td>5.1574</td>\n",
       "      <td>7.34775</td>\n",
       "      <td>9.51253</td>\n",
       "      <td>16.7165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_192</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.92784</td>\n",
       "      <td>1.47842</td>\n",
       "      <td>-3.8145</td>\n",
       "      <td>0.889775</td>\n",
       "      <td>1.9013</td>\n",
       "      <td>2.9495</td>\n",
       "      <td>8.4024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_193</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.33177</td>\n",
       "      <td>3.99203</td>\n",
       "      <td>-11.7834</td>\n",
       "      <td>0.5846</td>\n",
       "      <td>3.39635</td>\n",
       "      <td>6.2058</td>\n",
       "      <td>18.2818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_194</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.9938</td>\n",
       "      <td>3.13516</td>\n",
       "      <td>8.6944</td>\n",
       "      <td>15.6298</td>\n",
       "      <td>17.9579</td>\n",
       "      <td>20.3965</td>\n",
       "      <td>27.9288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_195</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.142088</td>\n",
       "      <td>1.42937</td>\n",
       "      <td>-5.261</td>\n",
       "      <td>-1.1707</td>\n",
       "      <td>-0.1727</td>\n",
       "      <td>0.8296</td>\n",
       "      <td>4.2729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_196</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.30334</td>\n",
       "      <td>5.45437</td>\n",
       "      <td>-14.2096</td>\n",
       "      <td>-1.94693</td>\n",
       "      <td>2.4089</td>\n",
       "      <td>6.55673</td>\n",
       "      <td>18.3215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_197</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.90816</td>\n",
       "      <td>0.921625</td>\n",
       "      <td>5.9606</td>\n",
       "      <td>8.2528</td>\n",
       "      <td>8.8882</td>\n",
       "      <td>9.5933</td>\n",
       "      <td>12.0004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_198</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.8707</td>\n",
       "      <td>3.01095</td>\n",
       "      <td>6.2993</td>\n",
       "      <td>13.8297</td>\n",
       "      <td>15.9341</td>\n",
       "      <td>18.0647</td>\n",
       "      <td>26.0791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_199</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.32654</td>\n",
       "      <td>10.438</td>\n",
       "      <td>-38.8528</td>\n",
       "      <td>-11.2085</td>\n",
       "      <td>-2.81955</td>\n",
       "      <td>4.8368</td>\n",
       "      <td>28.5007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>201 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          count  unique           top freq        mean       std      min  \\\n",
       "ID_code  200000  200000  train_131342    1         NaN       NaN      NaN   \n",
       "var_0    200000     NaN           NaN  NaN     10.6799   3.04005   0.4084   \n",
       "var_1    200000     NaN           NaN  NaN    -1.62762   4.05004 -15.0434   \n",
       "var_2    200000     NaN           NaN  NaN     10.7152   2.64089   2.1171   \n",
       "var_3    200000     NaN           NaN  NaN     6.79653   2.04332  -0.0402   \n",
       "var_4    200000     NaN           NaN  NaN     11.0783   1.62315   5.0748   \n",
       "var_5    200000     NaN           NaN  NaN    -5.06532   7.86327 -32.5626   \n",
       "var_6    200000     NaN           NaN  NaN     5.40895  0.866607   2.3473   \n",
       "var_7    200000     NaN           NaN  NaN     16.5458   3.41808   5.3497   \n",
       "var_8    200000     NaN           NaN  NaN    0.284162   3.33263 -10.5055   \n",
       "var_9    200000     NaN           NaN  NaN     7.56724   1.23507   3.9705   \n",
       "var_10   200000     NaN           NaN  NaN     0.39434   5.50079 -20.7313   \n",
       "var_11   200000     NaN           NaN  NaN     -3.2456   5.97025  -26.095   \n",
       "var_12   200000     NaN           NaN  NaN      14.024  0.190059  13.4346   \n",
       "var_13   200000     NaN           NaN  NaN     8.53023   4.63954  -6.0111   \n",
       "var_14   200000     NaN           NaN  NaN     7.53761   2.24791   1.0133   \n",
       "var_15   200000     NaN           NaN  NaN     14.5731  0.411711  13.0769   \n",
       "var_16   200000     NaN           NaN  NaN     9.33326   2.55742   0.6351   \n",
       "var_17   200000     NaN           NaN  NaN    -5.69673   6.71261 -33.3802   \n",
       "var_18   200000     NaN           NaN  NaN      15.244   7.85137 -10.6642   \n",
       "var_19   200000     NaN           NaN  NaN     12.4386   7.99669 -12.4025   \n",
       "var_20   200000     NaN           NaN  NaN     13.2909   5.87625  -5.4322   \n",
       "var_21   200000     NaN           NaN  NaN     17.2579   8.19656  -10.089   \n",
       "var_22   200000     NaN           NaN  NaN     4.30543   2.84796  -5.3225   \n",
       "var_23   200000     NaN           NaN  NaN     3.01954  0.526893   1.2098   \n",
       "var_24   200000     NaN           NaN  NaN     10.5844   3.77725  -0.6784   \n",
       "var_25   200000     NaN           NaN  NaN     13.6675  0.285535    12.72   \n",
       "var_26   200000     NaN           NaN  NaN    -4.05513   5.92221 -24.2431   \n",
       "var_27   200000     NaN           NaN  NaN    -1.13791   1.52371  -6.1668   \n",
       "var_28   200000     NaN           NaN  NaN     5.53298  0.783367   2.0896   \n",
       "...         ...     ...           ...  ...         ...       ...      ...   \n",
       "var_170  200000     NaN           NaN  NaN -0.00496206   4.42462  -14.506   \n",
       "var_171  200000     NaN           NaN  NaN   -0.831777   5.37801 -22.4793   \n",
       "var_172  200000     NaN           NaN  NaN     19.8171   8.67417 -11.4533   \n",
       "var_173  200000     NaN           NaN  NaN   -0.677967   5.96667 -22.7487   \n",
       "var_174  200000     NaN           NaN  NaN     20.2107   7.13643  -2.9953   \n",
       "var_175  200000     NaN           NaN  NaN     11.6406   2.89217   3.2415   \n",
       "var_176  200000     NaN           NaN  NaN    -2.79958   7.51394 -29.1165   \n",
       "var_177  200000     NaN           NaN  NaN     11.8829   2.62889   4.9521   \n",
       "var_178  200000     NaN           NaN  NaN    -1.01406   8.57981 -29.2734   \n",
       "var_179  200000     NaN           NaN  NaN     2.59144   2.79896  -7.8561   \n",
       "var_180  200000     NaN           NaN  NaN    -2.74167   5.26124 -22.0374   \n",
       "var_181  200000     NaN           NaN  NaN     10.0855   1.37186   5.4165   \n",
       "var_182  200000     NaN           NaN  NaN    0.719109   8.96343 -26.0011   \n",
       "var_183  200000     NaN           NaN  NaN     8.76909   4.47492  -4.8082   \n",
       "var_184  200000     NaN           NaN  NaN     12.7567   9.31828 -18.4897   \n",
       "var_185  200000     NaN           NaN  NaN    -3.98326   4.72517 -22.5833   \n",
       "var_186  200000     NaN           NaN  NaN     8.97027   3.18976  -3.0223   \n",
       "var_187  200000     NaN           NaN  NaN     -10.335   11.5747 -47.7536   \n",
       "var_188  200000     NaN           NaN  NaN     15.3772    3.9446   4.4123   \n",
       "var_189  200000     NaN           NaN  NaN    0.746072  0.976348  -2.5543   \n",
       "var_190  200000     NaN           NaN  NaN     3.23444   4.55992 -14.0933   \n",
       "var_191  200000     NaN           NaN  NaN     7.43841   3.02327  -2.6917   \n",
       "var_192  200000     NaN           NaN  NaN     1.92784   1.47842  -3.8145   \n",
       "var_193  200000     NaN           NaN  NaN     3.33177   3.99203 -11.7834   \n",
       "var_194  200000     NaN           NaN  NaN     17.9938   3.13516   8.6944   \n",
       "var_195  200000     NaN           NaN  NaN   -0.142088   1.42937   -5.261   \n",
       "var_196  200000     NaN           NaN  NaN     2.30334   5.45437 -14.2096   \n",
       "var_197  200000     NaN           NaN  NaN     8.90816  0.921625   5.9606   \n",
       "var_198  200000     NaN           NaN  NaN     15.8707   3.01095   6.2993   \n",
       "var_199  200000     NaN           NaN  NaN    -3.32654    10.438 -38.8528   \n",
       "\n",
       "              25%      50%       75%      max  \n",
       "ID_code       NaN      NaN       NaN      NaN  \n",
       "var_0     8.45385  10.5248   12.7582   20.315  \n",
       "var_1    -4.74003 -1.60805   1.35862  10.3768  \n",
       "var_2     8.72247    10.58   12.5167   19.353  \n",
       "var_3     5.25408    6.825    8.3241  13.1883  \n",
       "var_4     9.88317  11.1082   12.2611  16.6714  \n",
       "var_5    -11.2004 -4.83315    0.9248  17.2516  \n",
       "var_6      4.7677   5.3851     6.003   8.4477  \n",
       "var_7     13.9438  16.4568   19.1029  27.6918  \n",
       "var_8     -2.3178   0.3937    2.9379  10.1513  \n",
       "var_9      6.6188   7.6296   8.58442  11.1506  \n",
       "var_10   -3.59495   0.4873   4.38293  18.6702  \n",
       "var_11    -7.5106 -3.28695  0.852825  17.1887  \n",
       "var_12     13.894  14.0255   14.1642  14.6545  \n",
       "var_13     5.0728  8.60425   12.2748  22.3315  \n",
       "var_14    5.78188   7.5203   9.27042  14.9377  \n",
       "var_15    14.2628  14.5741   14.8745  15.8633  \n",
       "var_16    7.45228  9.23205   11.0559  17.9506  \n",
       "var_17   -10.4762 -5.66635 -0.810775  19.0259  \n",
       "var_18    9.17795  15.1962   21.0133   41.748  \n",
       "var_19    6.27648  12.4539   18.4333   35.183  \n",
       "var_20     8.6278  13.1968   17.8794  31.2859  \n",
       "var_21     11.551  17.2343   23.0891  49.0443  \n",
       "var_22     2.1824  4.27515    6.2932  14.5945  \n",
       "var_23     2.6341  3.00865    3.4038   4.8752  \n",
       "var_24      7.613  10.3803   13.4796   25.446  \n",
       "var_25    13.4564  13.6625   13.8637  14.6546  \n",
       "var_26   -8.32173  -4.1969   -0.0902  15.6751  \n",
       "var_27    -2.3079  -1.1321  0.015625   3.2431  \n",
       "var_28     4.9921  5.53485    6.0937   8.7874  \n",
       "...           ...      ...       ...      ...  \n",
       "var_170   -3.2585   0.0028    3.0964  16.7319  \n",
       "var_171  -4.72035 -0.80735    2.9568  17.9173  \n",
       "var_172   13.7318   19.748   25.9077  53.5919  \n",
       "var_173  -5.00953 -0.56975    3.6199  18.8554  \n",
       "var_174   15.0646  20.2061   25.6412  43.5468  \n",
       "var_175    9.3716  11.6798   13.7455  20.8548  \n",
       "var_176   -8.3865 -2.53845    2.7044  20.2452  \n",
       "var_177   9.80867  11.7372   13.9313  20.5965  \n",
       "var_178   -7.3957 -0.94205   5.33875  29.8413  \n",
       "var_179  0.625575   2.5123   4.39112  13.4487  \n",
       "var_180   -6.6739  -2.6888    0.9962  12.7505  \n",
       "var_181    9.0847   10.036   11.0113  14.3939  \n",
       "var_182  -6.06442   0.7202   7.49918  29.2487  \n",
       "var_183    5.4231      8.6   12.1274  23.7049  \n",
       "var_184    5.6633   12.521   19.4562  44.3634  \n",
       "var_185     -7.36 -3.94695  -0.59065  12.9975  \n",
       "var_186    6.7152  8.90215   11.1938  21.7392  \n",
       "var_187  -19.2051 -10.2097    -1.466  22.7861  \n",
       "var_188   12.5016  15.2394   18.3452  29.3303  \n",
       "var_189    0.0149   0.7426    1.4829   4.0341  \n",
       "var_190 -0.058825   3.2036    6.4062  18.4409  \n",
       "var_191    5.1574  7.34775   9.51253  16.7165  \n",
       "var_192  0.889775   1.9013    2.9495   8.4024  \n",
       "var_193    0.5846  3.39635    6.2058  18.2818  \n",
       "var_194   15.6298  17.9579   20.3965  27.9288  \n",
       "var_195   -1.1707  -0.1727    0.8296   4.2729  \n",
       "var_196  -1.94693   2.4089   6.55673  18.3215  \n",
       "var_197    8.2528   8.8882    9.5933  12.0004  \n",
       "var_198   13.8297  15.9341   18.0647  26.0791  \n",
       "var_199  -11.2085 -2.81955    4.8368  28.5007  \n",
       "\n",
       "[201 rows x 11 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[[i for i in train_data.columns.tolist() if i not in ['target','id']]].describe(include='all').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_name</th>\n",
       "      <th>col_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_code</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  col_name col_type\n",
       "0  ID_code   object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_type = train_data.dtypes.to_frame().reset_index()\n",
    "data_type.columns  = ['col_name','col_type']\n",
    "data_type[data_type.col_type==np.object].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isnull().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [i for i in train_data.columns if i not in ['target','ID_code']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_variance_scree_plot(variance, cumulative_variance = None, title = 'Principal Component vs Explained Ratio', n_components = 14):\n",
    "    with plot.style.context('seaborn-whitegrid'):\n",
    "        plot.figure(figsize=(10, 5))\n",
    "\n",
    "        plot.bar(range(1,n_components), variance, alpha=0.8, align='center',\n",
    "                label='individual explained variance', color = 'teal') ## since there are 13 dimenison\n",
    "        plot.step(range(1,n_components), cumulative_variance, where='mid',\n",
    "                 label='cumulative explained variance', color ='red') if cumulative_variance is not None else None\n",
    "        plot.plot(variance, color = 'black', label='scree')\n",
    "        plot.ylabel('Explained variance ratio')\n",
    "        plot.xlabel('Principal components')\n",
    "        plot.legend(loc='best')\n",
    "        plot.tight_layout()\n",
    "        plot.title(title)\n",
    "        plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data[col_names]\n",
    "y = train_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10,), (10, 200), (200000, 10))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=10)\n",
    "x_std = StandardScaler().fit_transform(x)\n",
    "x_pca = pca.fit_transform(x_std)\n",
    "explained_variance = pca.explained_variance_\n",
    "explained_variance.shape, pca.components_.shape,x_pca.shape\n",
    "# plot_variance_scree_plot(explained_variance)\n",
    "# x_test_pca1 = pca.transform(x_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.366235129327034, 10, (10,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot = sum(explained_variance)\n",
    "var_exp = [(i / tot)*100 for i in sorted(explained_variance, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "tot, len(var_exp), cum_var_exp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_variance_scree_plot(var_exp,cum_var_exp, n_components = pca.components_.shape[0] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1,), (200000, 1))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LinearDiscriminantAnalysis(n_components=10, solver='eigen',shrinkage='auto')\n",
    "x_lda = lda.fit_transform(x,y)\n",
    "explained_variance = lda.explained_variance_ratio_\n",
    "explained_variance.shape, x_lda.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: Run handful of usual suspect classifiers using selected 50 features giving highest coeff scores\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeTimer:\n",
    "    \n",
    "    \"\"\"\n",
    "        Utility custom contextual class for calculating the time \n",
    "        taken for a certain code block to execute\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, name=None):\n",
    "        self.name = \" '\"  + name + \"'\" if name else ''\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = time.clock()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.took = (time.clock() - self.start) * 1000.0\n",
    "        time_taken = datetime.timedelta(milliseconds = self.took)\n",
    "        print('Code block' + self.name + ' took(HH:MM:SS): ' + str(time_taken))\n",
    "        \n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plot.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plot.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plot.title(title)\n",
    "    plot.colorbar()\n",
    "    tick_marks = arange(len(classes))\n",
    "    plot.xticks(tick_marks, classes, rotation=45)\n",
    "    plot.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plot.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plot.ylabel('True label')\n",
    "    plot.xlabel('Predicted label')\n",
    "#     plot.tight_layout()\n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plotLearningCurve(_x_train, _y_train, learning_model_pipeline,  model_name, k_fold = 10, training_sample_sizes = linspace(0.1,1.0,10), jobsInParallel = 1):\n",
    "    \n",
    "    print('num of classes : ',np.bincount(_y_train))\n",
    "    training_size, training_score, testing_score = learning_curve(estimator = learning_model_pipeline, \\\n",
    "                                                                X = _x_train, \\\n",
    "                                                                y = _y_train, \\\n",
    "                                                                train_sizes = training_sample_sizes, \\\n",
    "                                                                cv = k_fold, \\\n",
    "                                                                n_jobs = jobsInParallel) \n",
    "\n",
    "\n",
    "    training_mean = mean(training_score, axis = 1)\n",
    "    training_std_deviation = std(training_score, axis = 1)\n",
    "    testing_std_deviation = std(testing_score, axis = 1)\n",
    "    testing_mean = mean(testing_score, axis = 1 )\n",
    "\n",
    "    ## we have got the estimator in this case the perceptron running in 10 fold validation with \n",
    "    ## equal division of sizes betwwen .1 and 1. After execution, we get the number of training sizes used, \n",
    "    ## the training scores for those sizes and the test scores for those sizes. we will plot a scatter plot \n",
    "    ## to see the accuracy results and check for bias vs variance\n",
    "\n",
    "    # training_size : essentially 10 sets of say a1, a2, a3,,...a10 sizes (this comes from train_size parameter, here we have given linespace for equal distribution betwwen 0.1 and 1 for 10 such values)\n",
    "    # training_score : training score for the a1 samples, a2 samples...a10 samples, each samples run 10 times since cv value is 10\n",
    "    # testing_score : testing score for the a1 samples, a2 samples...a10 samples, each samples run 10 times since cv value is 10\n",
    "    ## the mean and std deviation for each are calculated simply to show ranges in the graph\n",
    "\n",
    "    plot.plot(training_size, training_mean, label= \"Training Data\", marker= '+', color = 'blue', markersize = 8)\n",
    "    plot.fill_between(training_size, training_mean+ training_std_deviation, training_mean-training_std_deviation, color='blue', alpha =0.12 )\n",
    "\n",
    "    plot.plot(training_size, testing_mean, label= \"Testing/Validation Data\", marker= '*', color = 'green', markersize = 8)\n",
    "    plot.fill_between(training_size, testing_mean+ training_std_deviation, testing_mean-training_std_deviation, color='green', alpha =0.14 )\n",
    "\n",
    "    plot.title(\"Scoring of our training and testing data vs sample sizes for model:\"+model_name)\n",
    "    plot.xlabel(\"Number of Samples\")\n",
    "    plot.ylabel(\"Accuracy\")\n",
    "    plot.legend(loc= 'best')\n",
    "    plot.show()\n",
    "    \n",
    "def plot_roc_auc_curve(false_positive_rate, true_positive_rate, model_name):\n",
    "        \n",
    "    plot.figure(figsize=(10,3))\n",
    "    plot.plot(list(false_positive_rate), list(true_positive_rate),  label = \"ROC Curve for model: \"+model_name)     \n",
    "    plot.plot([0, 1], [0, 1], 'k--', label = 'Random Guessing')\n",
    "    plot.plot([0, 0, 1], [0,1, 1], ':', label = 'Perfect Score')\n",
    "    auc_score = auc(false_positive_rate, true_positive_rate)\n",
    "    plot.title('ROC Curve for model: %s with AUC %.2f'%(model_name, auc_score))\n",
    "    plot.xlabel('False Positive Rate')\n",
    "    plot.ylabel('True Positive Rate')\n",
    "    plot.legend(loc='best')\n",
    "    plot.show()\n",
    "    \n",
    "    \n",
    "def plot_precision_recall_curve(precision, recall, model_name):\n",
    "    \n",
    "    plot.figure(figsize=(10,3))\n",
    "    plot.plot(list(recall), list(precision),  label = \"Precision/Recall Curve for model: \"+model_name)     \n",
    "#     plot.plot([0, 1], [0, 1], 'k--', label = 'Random Guessing') #\n",
    "    plot.title('Precision Recall Curve for model: %s'%model_name)\n",
    "    plot.xlabel('Recall')\n",
    "    plot.ylabel('Precision')\n",
    "    plot.legend(loc='best')\n",
    "    plot.show()\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runGridSearchAndPredict(pipeline,model_name, x_train, y_train, x_test, y_test, param_grid, n_jobs = 1, cv = 10, score = 'accuracy'):\n",
    "#     pass\n",
    "\n",
    "    response =  {}\n",
    "    training_timer       = CodeTimer('training')\n",
    "    testing_timer        = CodeTimer('testing')\n",
    "    learning_curve_timer = CodeTimer('learning_curve')\n",
    "    predict_proba_timer  = CodeTimer('predict_proba')\n",
    "    \n",
    "    with training_timer:\n",
    "        \n",
    "        gridsearch = GridSearchCV(estimator = pipeline, param_grid = param_grid, cv = cv, n_jobs = n_jobs, scoring = score)\n",
    "\n",
    "        search = gridsearch.fit(x_train,y_train)\n",
    "\n",
    "        print(\"Grid Search Best parameters \", search.best_params_)\n",
    "        print(\"Grid Search Best score \", search.best_score_)\n",
    "\n",
    "    with testing_timer:\n",
    "        y_prediction = gridsearch.predict(x_test)\n",
    "            \n",
    "    print(\"F1 score %s\" %f1_score(y_test,y_prediction, average ='weighted'))\n",
    "    print(\"Classification report  \\n %s\" %(classification_report(y_test, y_prediction)))\n",
    "    \n",
    "    with learning_curve_timer:\n",
    "#         plotLearningCurve(x_train, y_train, search.best_estimator_, model_name, k_fold=cv)\n",
    "#         _matrix = confusion_matrix(y_true = _y_test ,y_pred = y_prediction, labels = list(range(_y_test.shape[1])))\n",
    "        _matrix = confusion_matrix(y_true = y_test ,y_pred = y_prediction, labels = list(set(y_test)))\n",
    "        classes = list(set(y_test))\n",
    "        plot_confusion_matrix(_matrix, classes, title = \"Confusion matrix for model:\"+model_name)\n",
    "        \n",
    "    with predict_proba_timer:\n",
    "\n",
    "        if hasattr(gridsearch.best_estimator_, 'predict_proba'):\n",
    "            \n",
    "            print('inside decision function')\n",
    "            y_probability = gridsearch.predict_proba(x_test)\n",
    "            number_of_classes = len(np.unique(y_train))\n",
    "            false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_probability[:, 1])\n",
    "            response['roc_auc_score'] = roc_auc_score(y_test, y_probability[:,1])\n",
    "            response['roc_curve'] = (false_positive_rate, true_positive_rate)  \n",
    "            response['roc_curve_false_positive_rate'] = false_positive_rate\n",
    "            response['roc_curve_true_positive_rate'] = true_positive_rate\n",
    "            precision, recall, _ = precision_recall_curve(y_test, y_probability[:,1])\n",
    "            plot_roc_auc_curve(false_positive_rate, true_positive_rate, model_name)\n",
    "            plot_precision_recall_curve(precision, recall, model_name)\n",
    "            \n",
    "        else: ## eg SVM, Perceptron doesnt have predict_proba method\n",
    "            \n",
    "            response['roc_auc_score'] = 0\n",
    "            response['roc_curve'] = 0\n",
    "            response['roc_curve_false_positive_rate'] = 0\n",
    "            response['roc_curve_true_positive_rate'] = 0\n",
    "    \n",
    "    response['learning_curve_time'] = learning_curve_timer.took\n",
    "    response['testing_time'] = testing_timer.took\n",
    "    response['_y_prediction'] = y_prediction\n",
    "#     response['accuracy_score'] = accuracy_score(y_test,y_prediction)\n",
    "    response['training_time'] = training_timer.took\n",
    "    response['f1_score']  = f1_score(y_test, y_prediction, average ='weighted')\n",
    "    response['f1_score_micro']  = f1_score(y_test, y_prediction, average ='micro')\n",
    "    response['f1_score_macro']  = f1_score(y_test, y_prediction, average ='macro')\n",
    "    response['best_estimator'] = search.best_estimator_\n",
    "    response['confusion_matrix'] = _matrix\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "def plotROCCurveAcrossModels(positive_rates_sequence, model_name):\n",
    "    \n",
    "    plot.figure(figsize=(10,5))\n",
    "    for plot_values, label_name in zip(positive_rates_sequence, model_name):\n",
    "        \n",
    "        plot.plot(list(plot_values[0]), list(plot_values[1]),  label = \"ROC Curve for model: \"+label_name)\n",
    "        \n",
    "    plot.plot([0, 1], [0, 1], 'k--', label = 'Random Guessing') #\n",
    "    plot.title('ROC Curve across models')\n",
    "    plot.xlabel('False Positive Rate')\n",
    "    plot.ylabel('True Positive Rate')\n",
    "    plot.legend(loc='best')\n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def execute( _x_train,\n",
    "             _y_train,\n",
    "             _x_test,\n",
    "             _y_test, \n",
    "            classifiers, \n",
    "            classifier_names, \n",
    "            classifier_param_grid,\n",
    "            cv  = 10 , \n",
    "            score = 'accuracy',\n",
    "            scaler = StandardScaler()\n",
    "           ):\n",
    "    \n",
    "    '''\n",
    "    This method will run your data sets against the model specified \n",
    "    Models will be fed through a pipeline where the first step would be to\n",
    "    execute a scaling operation.\n",
    "    \n",
    "    Method will also call additional lower level methods in order to plot\n",
    "    precision curve, roc curve, learning curve and will also prepare a confusion matrix\n",
    "    \n",
    "    :returns: dict containing execution metrics such as time taken, accuracy scores\n",
    "    :returntype: dict\n",
    "    \n",
    "    '''\n",
    "\n",
    "    timer = CodeTimer(name='overalltime')\n",
    "    model_metrics = {}\n",
    "\n",
    "    with timer:\n",
    "        for model, model_name, model_param_grid in zip(classifiers, classifier_names, classifier_param_grid):\n",
    "\n",
    "            pipeline_steps = [('scaler', scaler),(model_name, model)] if scaler is not None else [(model_name, model)]\n",
    "            pipeline = Pipeline(pipeline_steps)\n",
    "\n",
    "            result = runGridSearchAndPredict(pipeline, \n",
    "                                             model_name,\n",
    "                                             _x_train,\n",
    "                                             _y_train,\n",
    "                                             _x_test,\n",
    "                                             _y_test, \n",
    "                                             model_param_grid ,\n",
    "                                             cv = cv,\n",
    "                                             score = score)\n",
    "\n",
    "            _y_prediction = result['_y_prediction']\n",
    "\n",
    "            model_metrics[model_name] = {}\n",
    "            model_metrics[model_name]['confusion_matrix'] = result.get('confusion_matrix')\n",
    "            model_metrics[model_name]['training_time'] = result.get('training_time')\n",
    "            model_metrics[model_name]['testing_time'] = result.get('testing_time')\n",
    "            model_metrics[model_name]['learning_curve_time'] = result.get('learning_curve_time')\n",
    "            model_metrics[model_name]['f1_score'] = result.get('f1_score')\n",
    "            model_metrics[model_name]['f1_score_macro'] = result.get('f1_score_macro')\n",
    "            model_metrics[model_name]['f1_score_micro'] = result.get('f1_score_micro')\n",
    "            model_metrics[model_name]['roc_auc_score'] = result.get('roc_auc_score')\n",
    "            model_metrics[model_name]['roc_curve_true_positive_rate'] = result.get('roc_curve_true_positive_rate')\n",
    "            model_metrics[model_name]['roc_curve_false_positive_rate'] = result.get('roc_curve_false_positive_rate')\n",
    "\n",
    "            model_metrics[model_name]['best_estimator'] = result.get('best_estimator')\n",
    "\n",
    "\n",
    "    print(timer.took)\n",
    "    \n",
    "    return model_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifiers = [\n",
    "#     Perceptron(random_state = 1),\n",
    "#     LogisticRegression(random_state = 1),\n",
    "#     LogisticRegression(random_state = 1, solver='liblinear'),\n",
    "#     LogisticRegression(random_state = 1, solver='newton-cg'),\n",
    "#     LogisticRegression(random_state = 1, solver='sag'),\n",
    "#     DecisionTreeClassifier(),\n",
    "#     RandomForestClassifier(random_state = 1),\n",
    "#     KNeighborsClassifier(metric = 'minkowski'),\n",
    "#     RidgeClassifier(random_state = 123), \n",
    "#     SVC(kernel=\"linear\"),\n",
    "#     SVC(),\n",
    "#     ExtraTreeClassifier(random_state = 123),\n",
    "#     GaussianProcessClassifier(random_state = 123),\n",
    "#     BernoulliNB(),\n",
    "#     BaggingClassifier(base_estimator = LogisticRegression(random_state = 1)),\n",
    "#     BaggingClassifier(base_estimator = BernoulliNB()),\n",
    "    GradientBoostingClassifier(random_state= 123),\n",
    "#     LGBMClassifier(objective = 'binary'),\n",
    "    XGBClassifier(objective = 'binary:logistic')\n",
    "]\n",
    "\n",
    "\n",
    "classifier_names = [\n",
    "#             'perceptron',\n",
    "#             'logisticregression',\n",
    "#             'logisticregression_liblinear_l2',\n",
    "#             'logisticregression_newton_cg',\n",
    "#             'logisticregression_sag',\n",
    "#             'decisiontreeclassifier',\n",
    "#             'randomforestclassifier',\n",
    "#             'kneighborsclassifier',\n",
    "#             'ridge',\n",
    "#             'linear_svc',\n",
    "#             'gamma_svc',\n",
    "#             'extra_trees',\n",
    "#             'gaussian_process',\n",
    "#             'bernoulli',\n",
    "#             'bagging_logistic',\n",
    "#             'bagging_bernoulli',\n",
    "            'gradient_boosting_classifier',\n",
    "#             'lgbm_classifier',\n",
    "            'xgb'\n",
    "]\n",
    "\n",
    "classifier_param_grid = [\n",
    "            \n",
    "#             {'perceptron__max_iter': [5,10,30], 'perceptron__eta0': [.1]},\n",
    "#             {\n",
    "#              'logisticregression__C':[1.2,0.02,2.2,4, 0.01, 0.05], \n",
    "#              'logisticregression__penalty':['l1','l2'],\n",
    "#              'logisticregression__solver':['saga','liblinear']\n",
    "#             },\n",
    "#             {\n",
    "#              'logisticregression_liblinear_l2__C':[1.2,0.02,2.2,4, 0.01, 0.05], \n",
    "#              'logisticregression_liblinear_l2__penalty':['l2'],\n",
    "#              'logisticregression_liblinear_l2__dual':[True]\n",
    "#             },\n",
    "#             {\n",
    "#              'logisticregression_newton_cg__C':[1.2,0.02,2.2,4, 0.01, 0.05], \n",
    "#              'logisticregression_newton_cg__penalty':['l2'],\n",
    "#             },\n",
    "#             {\n",
    "#              'logisticregression_sag__C':[1.2,0.02,2.2,4, 0.01, 0.05], \n",
    "#              'logisticregression_sag__penalty':['l2'],\n",
    "#             },\n",
    "    \n",
    "#             {'decisiontreeclassifier__max_depth':[6,8,10],\n",
    "#              'decisiontreeclassifier__criterion':['gini','entropy'],\n",
    "#              'decisiontreeclassifier__max_features':['auto','sqrt','log2'],\n",
    "#             },\n",
    "#             {'randomforestclassifier__n_estimators':[6,8,12],'randomforestclassifier__criterion': ['gini','entropy']} ,\n",
    "#             {'kneighborsclassifier__n_neighbors':[4,6,10]},\n",
    "#             {'ridge__alpha':[1,1.2,0.9],'ridge__max_iter':[100,300,500]},\n",
    "#             {'linear_svc__C':[0.025]},\n",
    "#             {'gamma_svc__gamma':[2,4],'gamma_svc__C':[1,5]},\n",
    "#             {'extra_trees__max_depth':[6,8,12],'extra_trees__criterion': ['gini','entropy']} ,\n",
    "#             {'gaussian_process__max_iter_predict':[200,400]} ,\n",
    "#             {'bernoulli__alpha':[0.2,0.6,1.2]} ,\n",
    "#             {'bagging_logistic__base_estimator__C':[1.2,0.02,2.2,4], \n",
    "#              'bagging_logistic__base_estimator__penalty':['l1','l2'],\n",
    "#              'bagging_logistic__n_estimators': [5,8,10]\n",
    "#             },\n",
    "#             {'bagging_bernoulli__base_estimator__alpha':[1.2,0.02,2.2,4], \n",
    "#              'bagging_bernoulli__n_estimators': [5,8,10]\n",
    "#             },\n",
    "            {\n",
    "                'gradient_boosting_classifier__loss':['deviance','exponential'],\n",
    "                'gradient_boosting_classifier__learning_rate':[0.5,1.2],\n",
    "                'gradient_boosting_classifier__n_estimators':[100,500,1000],\n",
    "                'gradient_boosting_classifier__criterion':['friedman_mse','mse','mae'],\n",
    "                'gradient_boosting_classifier__max_depth':[6,8,16,20],\n",
    "            },\n",
    "#             {\n",
    "#                  'lgbm_classifier__num_leaves':[25,], \\\n",
    "#                  'lgbm_classifier__min_data_in_leaf':[100],\\\n",
    "#                  'lgbm_classifier__max_depth':[20,], \\\n",
    "#                  'lgbm_classifier__learning_rate' : [0.01,],\\\n",
    "#                  'lgbm_classifier__min_child_samples' :[2,], \\\n",
    "#                  'lgbm_classifier__n_estimators' : [5000,], \\\n",
    "#                  'lgbm_classifier__num_boost_round' : [10000], \\\n",
    "#                  'lgbm_classifier__feature_fraction' : [0.5,], \\\n",
    "#                  'lgbm_classifier__bagging_freq' : [1,], \\\n",
    "#                  'lgbm_classifier__bagging_seed' : [123], \\\n",
    "#                 'lgbm_classifier__boosting_type' : ['gbdt'], \\\n",
    "#                 'lgbm_classifier__min_sum_hessian_in_leaf' : [0.000446], \\\n",
    "#             },\n",
    "             {\n",
    "                'xgb__max_depth':[6,8,10],\n",
    "                 'xgb__learning_rate':[0.1,0.5,1,2],\n",
    "                 'xgb__n_estimators':[100,400,1000],             \n",
    "                 'xgb__booster':['gbtree','dart'],\n",
    "                 'xgb__subsample':[0.5, 0.2,0.8]\n",
    "            },\n",
    "    \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = train_data[[i for i in train_data.columns.tolist() if i not in ['target','id']]]\n",
    "# x = train_data[reqd_columns[:51]]\n",
    "x = x_lda\n",
    "y = train_data['target']\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y , stratify = y, test_size = 0.3, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "tl = TomekLinks(return_indices=True, ratio='majority')\n",
    "_x_train_tomek, _y_train_tomek, id_tl = tl.fit_sample(x_train, y_train)\n",
    "\n",
    "smt = SMOTETomek(ratio='auto')\n",
    "_x_train_smt, _y_train_smt = smt.fit_sample(x_train, y_train)\n",
    "\n",
    "smote = SMOTE(ratio='minority')\n",
    "x_train_smote, y_train_smote = smote.fit_sample(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((251862, 1), (251862,), (133832, 1), (133832,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_smote.shape, y_train_smote.shape, _x_train_tomek.shape, _y_train_tomek.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([125931, 125931], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits = 5, shuffle= True, random_state =123)\n",
    "score= 'roc_auc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = execute(\n",
    "        x_train_smote,\n",
    "        y_train_smote,\n",
    "        x_test,\n",
    "        y_test,\n",
    "        classifiers,\n",
    "        classifier_names,\n",
    "        classifier_param_grid,\n",
    "        cv=cv,\n",
    "        score=score,\n",
    "        scaler= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = panda.DataFrame(response).transpose()\n",
    "results.head()\n",
    "results[['f1_score',\n",
    "         'f1_score_macro',\n",
    "         'f1_score_micro',\n",
    "         'learning_curve_time',\n",
    "         'roc_auc_score',\n",
    "         'testing_time',\n",
    "         'training_time',\n",
    "        ]]\\\n",
    ".sort_values(by=['roc_auc_score',],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator = response.get('lgbm_classifier').get('best_estimator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "roc_rates = []\n",
    "model_name = []\n",
    "for index, key in enumerate(response):\n",
    "    \n",
    "    \n",
    "    estimator = response.get(key)\n",
    "    if estimator.get('roc_auc_score')!=0:\n",
    "        roc_curve_true_positive_rate = estimator.get('roc_curve_true_positive_rate')\n",
    "        roc_curve_false_positive_rate = estimator.get('roc_curve_false_positive_rate')\n",
    "        roc_rates.append([roc_curve_false_positive_rate,roc_curve_true_positive_rate])\n",
    "        model_name.append(key)\n",
    "\n",
    "plotROCCurveAcrossModels(roc_rates,model_name) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['learning_curve_time'] = results['learning_curve_time'].astype('float64')\n",
    "results['testing_time'] = results['testing_time'].astype('float64')\n",
    "results['training_time'] = results['training_time'].astype('float64')\n",
    "results['f1_score'] = results['f1_score'].astype('float64')\n",
    "results['f1_score_micro'] = results['f1_score_micro'].astype('float64')\n",
    "results['f1_score_macro'] = results['f1_score_macro'].astype('float64')\n",
    "results['roc_auc_score'] = results['roc_auc_score'].astype('float64')\n",
    "# results['roc_auc_macro'] = results['roc_auc_macro'].astype('float64')\n",
    "\n",
    "#scaling time parameters between 0 and 1\n",
    "results['learning_curve_time'] = (results['learning_curve_time']- results['learning_curve_time'].min())/(results['learning_curve_time'].max()- results['learning_curve_time'].min())\n",
    "results['testing_time'] = (results['testing_time']- results['testing_time'].min())/(results['testing_time'].max()- results['testing_time'].min())\n",
    "results['training_time'] = (results['training_time']- results['training_time'].min())/(results['training_time'].max()- results['training_time'].min())\n",
    "\n",
    "results.plot(kind='barh',figsize=(12, 10))\n",
    "plot.title(\"Scaled Estimates across different classifiers used\")\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "\n",
    "1. We got highest scores of ROC/AUC 0.92 in logistic regression with l2 parameter\n",
    "\n",
    "2. From the learning curve, we also see that overfitting tendency is less\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = panda.read_csv('data/test.csv')\n",
    "\n",
    "\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_x = lda.transform(test_data[[i for i in test_data.columns.tolist() if i not in ['ID_code']]])\n",
    "test_x=test_data[[i for i in test_data.columns.tolist() if i not in ['ID_code']]]\n",
    "test_x.shape\n",
    "test_x=lda.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target = best_estimator.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = panda.read_csv('data/sample_submission.csv')\n",
    "submission['target'] = test_target\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('sample_submission_3.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
