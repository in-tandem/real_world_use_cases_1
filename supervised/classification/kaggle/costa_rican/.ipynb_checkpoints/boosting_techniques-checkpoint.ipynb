{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We would run a set of models to establish a base line of accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as panda\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler,RobustScaler,label_binarize\n",
    "\n",
    "from sklearn.model_selection import train_test_split,KFold,StratifiedKFold,GridSearchCV,RepeatedStratifiedKFold,learning_curve\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, roc_curve, auc, precision_recall_curve, \\\n",
    "        classification_report,confusion_matrix,average_precision_score\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression,RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot as plot\n",
    "from itertools import cycle\n",
    "import numpy as np \n",
    "from scipy import interp\n",
    "import seaborn as sns\n",
    "import itertools, time, datetime\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from numpy import bincount, linspace, mean, std, arange, squeeze\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeTimer:\n",
    "    \n",
    "    \"\"\"\n",
    "        Utility custom contextual class for calculating the time \n",
    "        taken for a certain code block to execute\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, name=None):\n",
    "        self.name = \" '\"  + name + \"'\" if name else ''\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = time.clock()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.took = (time.clock() - self.start) * 1000.0\n",
    "        time_taken = datetime.timedelta(milliseconds = self.took)\n",
    "        print('Code block' + self.name + ' took(HH:MM:SS): ' + str(time_taken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plot.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plot.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plot.title(title)\n",
    "    plot.colorbar()\n",
    "    tick_marks = arange(len(classes))\n",
    "    plot.xticks(tick_marks, classes, rotation=45)\n",
    "    plot.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plot.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plot.ylabel('True label')\n",
    "    plot.xlabel('Predicted label')\n",
    "#     plot.tight_layout()\n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## over sampling majorities first \n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "smt = SMOTETomek(ratio='auto')\n",
    "# _x_train, _y_train = smt.fit_sample(_x_train, _y_train)\n",
    "\n",
    "# smote = SMOTE(ratio='minority')\n",
    "# x_train, y_train = smote.fit_sample(_x_train,_y_train)\n",
    "\n",
    "# tl = TomekLinks(return_indices=True, ratio='majority')\n",
    "# _x_train, _y_train, id_tl = tl.fit_sample(_x_train, _y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeTimer:\n",
    "    \n",
    "    \"\"\"\n",
    "        Utility custom contextual class for calculating the time \n",
    "        taken for a certain code block to execute\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, name=None):\n",
    "        self.name = \" '\"  + name + \"'\" if name else ''\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = time.clock()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.took = (time.clock() - self.start) * 1000.0\n",
    "        time_taken = datetime.timedelta(milliseconds = self.took)\n",
    "        print('Code block' + self.name + ' took(HH:MM:SS): ' + str(time_taken))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_roc_and_prec_metrics(y, y_probabilities, number_of_classes):\n",
    "    \n",
    "    false_positive_rate_across_class    = dict()\n",
    "    true_positive_rate_across_class     = dict()\n",
    "    recall_across_class                 = dict()\n",
    "    precision_across_class              = dict()\n",
    "    roc_auc_across_class                = dict()\n",
    "    average_precision_across_class      = dict()\n",
    "\n",
    "\n",
    "    for i in range(number_of_classes):\n",
    "\n",
    "        false_positive_rate_across_class[i], \\\n",
    "                true_positive_rate_across_class[i], _ = roc_curve(y[:, i], y_probabilities[:, i])\n",
    "\n",
    "        precision_across_class[i], recall_across_class[i], _ = precision_recall_curve(y[:, i],\n",
    "                                                        y_probabilities[:, i])\n",
    "\n",
    "        average_precision_across_class[i] = average_precision_score(y[:, i], y_probabilities[:, i])\n",
    "\n",
    "        roc_auc_across_class[i] = auc(false_positive_rate_across_class[i], true_positive_rate_across_class[i])\n",
    "\n",
    "        # Compute micro-average ROC curve and ROC area\n",
    "\n",
    "        false_positive_rate_across_class[\"micro\"], \\\n",
    "                true_positive_rate_across_class[\"micro\"], _ = roc_curve(y.ravel(), y_probabilities.ravel())\n",
    "\n",
    "        roc_auc_across_class[\"micro\"] = auc(false_positive_rate_across_class[\"micro\"], \\\n",
    "                            true_positive_rate_across_class[\"micro\"])\n",
    "\n",
    "\n",
    "        precision_across_class[\"micro\"], recall_across_class[\"micro\"], _ = \\\n",
    "                    precision_recall_curve(y.ravel(), y_probabilities.ravel())\n",
    "\n",
    "        average_precision_across_class[\"micro\"] = average_precision_score(y, y_probabilities,\n",
    "                                                            average=\"micro\")\n",
    "\n",
    "    response  = dict() \n",
    "    response['false_positive_rate_across_class']    = false_positive_rate_across_class\n",
    "    response['true_positive_rate_across_class']     = true_positive_rate_across_class\n",
    "    response['recall_across_class']                 = recall_across_class\n",
    "    response['precision_across_class']              = precision_across_class\n",
    "    response['roc_auc_across_class']                = roc_auc_across_class\n",
    "    response['average_precision_across_class']      = average_precision_across_class\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def plot_roc_auc_curve(false_positive_rate_across_class, \\\n",
    "            true_positive_rate_across_class, roc_auc_across_class, n_classes):\n",
    "    \n",
    "    # First aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([false_positive_rate_across_class[i] for i in range(n_classes)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += interp(all_fpr, false_positive_rate_across_class[i], true_positive_rate_across_class[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    false_positive_rate_across_class[\"macro\"] = all_fpr\n",
    "    true_positive_rate_across_class[\"macro\"] = mean_tpr\n",
    "    roc_auc_across_class[\"macro\"] = auc(false_positive_rate_across_class[\"macro\"], true_positive_rate_across_class[\"macro\"])\n",
    "\n",
    "    # Plot all ROC curves\n",
    "    plot.figure(figsize = (15,5))\n",
    "    plot.plot(false_positive_rate_across_class[\"micro\"], true_positive_rate_across_class[\"micro\"],\n",
    "            label='micro-average ROC curve (area = {0:0.2f})'\n",
    "                ''.format(roc_auc_across_class[\"micro\"]),\n",
    "            color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "    plot.plot(false_positive_rate_across_class[\"macro\"], true_positive_rate_across_class[\"macro\"],\n",
    "            label='macro-average ROC curve (area = {0:0.2f})'\n",
    "                ''.format(roc_auc_across_class[\"macro\"]),\n",
    "            color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plot.plot(false_positive_rate_across_class[i], true_positive_rate_across_class[i], color=color, lw=4,\n",
    "                label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                ''.format(i, roc_auc_across_class[i]))\n",
    "\n",
    "    plot.plot([0, 1], [0, 1], 'k--', lw=4)\n",
    "    plot.xlim([0.0, 1.0])\n",
    "    plot.ylim([0.0, 1.05])\n",
    "    plot.xlabel('False Positive Rate')\n",
    "    plot.ylabel('True Positive Rate')\n",
    "    plot.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "    plot.legend(loc=\"lower right\")\n",
    "    plot.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_precision_recall_curve(recall_across_class, \\\n",
    "            precision_across_class, average_precision_across_class, n_classes):\n",
    "\n",
    "    line_width = 4\n",
    "    from itertools import cycle\n",
    "    # setup plot details\n",
    "    colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal'])\n",
    "\n",
    "    plot.figure(figsize=(15, 8))\n",
    "    f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "    lines = []\n",
    "    labels = []\n",
    "    for f_score in f_scores:\n",
    "        x = np.linspace(0.01, 1)\n",
    "        y = f_score * x / (2 * x - f_score)\n",
    "        l, = plot.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "        plot.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "    lines.append(l)\n",
    "    labels.append('iso-f1 curves')\n",
    "    l, = plot.plot(recall_across_class[\"micro\"], precision_across_class[\"micro\"], color='gold', lw=line_width)\n",
    "    lines.append(l)\n",
    "    labels.append('micro-average Precision-recall (area = {0:0.2f})'\n",
    "                ''.format(average_precision_across_class[\"micro\"]))\n",
    "\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        l, = plot.plot(recall_across_class[i], precision_across_class[i], color=color, lw=line_width)\n",
    "        lines.append(l)\n",
    "        labels.append('Precision-recall for class {0} (area = {1:0.2f})'\n",
    "                    ''.format(i, average_precision_across_class[i]))\n",
    "\n",
    "    fig = plot.gcf()\n",
    "    fig.subplots_adjust(bottom=0.25)\n",
    "    plot.xlim([0.0, 1.0])\n",
    "    plot.ylim([0.0, 1.05])\n",
    "    plot.xlabel('Recall')\n",
    "    plot.ylabel('Precision')\n",
    "    plot.title('Extension of Precision-Recall curve to multi-class')\n",
    "#     plot.legend(lines, labels, loc=(0, -.38), prop=dict(size=14))\n",
    "    plot.legend(lines, labels, loc='best', prop=dict(size=14))\n",
    "    plot.show()\n",
    "\n",
    "    \n",
    "\n",
    "def plotLearningCurve(_x_train, _y_train, learning_model_pipeline,  k_fold = 10, training_sample_sizes = linspace(0.1,1.0,10), jobsInParallel = 1):\n",
    "    \n",
    "    training_size, training_score, testing_score = learning_curve(estimator = learning_model_pipeline, \\\n",
    "                                                                X = _x_train, \\\n",
    "                                                                y = _y_train, \\\n",
    "                                                                train_sizes = training_sample_sizes, \\\n",
    "                                                                cv = k_fold, \\\n",
    "                                                                n_jobs = jobsInParallel) \n",
    "\n",
    "\n",
    "    training_mean = mean(training_score, axis = 1)\n",
    "    training_std_deviation = std(training_score, axis = 1)\n",
    "    testing_std_deviation = std(testing_score, axis = 1)\n",
    "    testing_mean = mean(testing_score, axis = 1 )\n",
    "\n",
    "    ## we have got the estimator in this case the perceptron running in 10 fold validation with \n",
    "    ## equal division of sizes betwwen .1 and 1. After execution, we get the number of training sizes used, \n",
    "    ## the training scores for those sizes and the test scores for those sizes. we will plot a scatter plot \n",
    "    ## to see the accuracy results and check for bias vs variance\n",
    "\n",
    "    # training_size : essentially 10 sets of say a1, a2, a3,,...a10 sizes (this comes from train_size parameter, here we have given linespace for equal distribution betwwen 0.1 and 1 for 10 such values)\n",
    "    # training_score : training score for the a1 samples, a2 samples...a10 samples, each samples run 10 times since cv value is 10\n",
    "    # testing_score : testing score for the a1 samples, a2 samples...a10 samples, each samples run 10 times since cv value is 10\n",
    "    ## the mean and std deviation for each are calculated simply to show ranges in the graph\n",
    "\n",
    "    plot.plot(training_size, training_mean, label= \"Training Data\", marker= '+', color = 'blue', markersize = 8)\n",
    "    plot.fill_between(training_size, training_mean+ training_std_deviation, training_mean-training_std_deviation, color='blue', alpha =0.12 )\n",
    "\n",
    "    plot.plot(training_size, testing_mean, label= \"Testing/Validation Data\", marker= '*', color = 'green', markersize = 8)\n",
    "    plot.fill_between(training_size, testing_mean+ training_std_deviation, testing_mean-training_std_deviation, color='green', alpha =0.14 )\n",
    "\n",
    "    plot.title(\"Scoring of our training and testing data vs sample sizes\")\n",
    "    plot.xlabel(\"Number of Samples\")\n",
    "    plot.ylabel(\"Accuracy\")\n",
    "    plot.legend(loc= 'best')\n",
    "    plot.show()\n",
    "    \n",
    "    \n",
    "def runGridSearchAndPredict(pipeline, x_train, y_train, x_test, y_test, param_grid, n_jobs = 1, cv = 10, score = 'accuracy'):\n",
    "    \n",
    "    response = {}\n",
    "    training_timer       = CodeTimer('training')\n",
    "    testing_timer        = CodeTimer('testing')\n",
    "    learning_curve_timer = CodeTimer('learning_curve')\n",
    "    predict_proba_timer  = CodeTimer('predict_proba')\n",
    "    \n",
    "    with training_timer:\n",
    "        \n",
    "        gridsearch = GridSearchCV(estimator = pipeline, param_grid = param_grid, cv = cv, n_jobs = n_jobs, scoring = score)\n",
    "\n",
    "        search = gridsearch.fit(x_train,y_train)\n",
    "\n",
    "        print(\"Grid Search Best parameters \", search.best_params_)\n",
    "        print(\"Grid Search Best score \", search.best_score_)\n",
    "\n",
    "    with testing_timer:\n",
    "        y_prediction = gridsearch.predict(x_test)\n",
    "            \n",
    "    print(\"F1 score %s\" %f1_score(y_test,y_prediction, average ='weighted'))\n",
    "    print(\"Classification report  \\n %s\" %(classification_report(y_test, y_prediction)))\n",
    "    \n",
    "    with learning_curve_timer:\n",
    "        plotLearningCurve(_x_train, _y_train, search.best_estimator_)\n",
    "#         _matrix = confusion_matrix(y_true = _y_test ,y_pred = y_prediction, labels = list(range(_y_test.shape[1])))\n",
    "        _matrix = confusion_matrix(y_true = _y_test ,y_pred = y_prediction, labels = list(set(_y_test)))\n",
    "        classes = list(set(_y_test))\n",
    "        plot_confusion_matrix(_matrix, classes)\n",
    "        \n",
    "    with predict_proba_timer:\n",
    "\n",
    "        if hasattr(gridsearch.best_estimator_, 'predict_proba'):\n",
    "            \n",
    "            print('inside decision function')\n",
    "            y_probability = gridsearch.predict_proba(_x_test)\n",
    "            number_of_classes = len(np.unique(y_train))\n",
    "            y_test_bin = label_binarize(y_test, classes = np.unique(y_train))\n",
    "            response = calculate_roc_and_prec_metrics(y_test_bin, y_probability, number_of_classes = number_of_classes)\n",
    "\n",
    "            roc_params = {\n",
    "\n",
    "                'false_positive_rate_across_class': response.get('false_positive_rate_across_class'),\n",
    "                'true_positive_rate_across_class': response.get('true_positive_rate_across_class'),\n",
    "                'roc_auc_across_class': response.get('roc_auc_across_class'),\n",
    "                'n_classes': number_of_classes\n",
    "            }\n",
    "\n",
    "            precision_recall_params  = {\n",
    "\n",
    "                'precision_across_class': response.get('precision_across_class'),\n",
    "                'average_precision_across_class': response.get('average_precision_across_class'),\n",
    "                'recall_across_class': response.get('recall_across_class'),\n",
    "                'n_classes': number_of_classes\n",
    "            }\n",
    "\n",
    "            plot_roc_auc_curve(**roc_params)\n",
    "            plot_precision_recall_curve(**precision_recall_params)\n",
    "            response['roc_auc_micro'] = response.get('roc_auc_across_class',{}).get(\"micro\")\n",
    "            response['roc_auc_macro'] = response.get('roc_auc_across_class',{}).get(\"macro\")\n",
    "            \n",
    "            \n",
    "        else: ## eg SVM, Perceptron doesnt have predict_proba method\n",
    "            \n",
    "            response['roc_auc_micro'] = 0\n",
    "            response['roc_auc_macro'] = 0\n",
    "    \n",
    "    response['learning_curve_time'] = learning_curve_timer.took\n",
    "    response['testing_time'] = testing_timer.took\n",
    "    response['_y_prediction'] = y_prediction\n",
    "#     response['accuracy_score'] = accuracy_score(y_test,y_prediction)\n",
    "    response['training_time'] = training_timer.took\n",
    "    response['f1_score']  = f1_score(y_test, y_prediction, average ='weighted')\n",
    "    response['f1_score_micro']  = f1_score(y_test, y_prediction, average ='micro')\n",
    "    response['f1_score_macro']  = f1_score(y_test, y_prediction, average ='macro')\n",
    "    response['best_estimator'] = search.best_estimator_\n",
    "    response['confusion_matrix'] = _matrix\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = label_binarize(y,classes = np.unique(y))\n",
    "\n",
    "\n",
    "\n",
    "# _y_train.shape, _y_test.shape\n",
    "\n",
    "## over sampling majorities first \n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "smt = SMOTETomek(ratio='auto')\n",
    "# _x_train, _y_train = smt.fit_sample(_x_train, _y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## we will add more classifiers in round2\n",
    "classifiers = [\n",
    "    XGBClassifier(objective = 'multi:softmax'),\n",
    "    LGBMClassifier(subsample_for_bin=0.5, colsample_bytree = 0.5, objective = 'multiclass')\n",
    "]\n",
    "\n",
    "\n",
    "classifier_names = [\n",
    "            'xgb',\n",
    "            'lgb',\n",
    "]\n",
    "\n",
    "classifier_param_grid = [\n",
    "            \n",
    "            {'xgb__max_depth':[6,8,10],\n",
    "             'xgb__learning_rate':[0.1,0.5,1,2],\n",
    "             'xgb__n_estimators':[100,400,1000],\n",
    "             'xgb__reg_alpha':[0.2,0.3,0.6,1.3],\n",
    "             'xgb__reg_lambda':[1.2,1.5],\n",
    "             'xgb__booster':['gbtree','dart'],\n",
    "             'xgb__subsample':[0.5]\n",
    "            },\n",
    "    \n",
    "            {\n",
    "             'lgb__num_leaves':[25,], \\\n",
    "             'lgb__min_data_in_leaf':[20],\\\n",
    "             'lgb__max_depth':[20,], \\\n",
    "             'lgb__learning_rate' : [0.01,],\\\n",
    "             'lgb__min_child_samples' :[100,], \\\n",
    "             'lgb__n_estimators' : [5000,], \\\n",
    "             'lgb__num_boost_round' : [100], \\\n",
    "             'lgb__feature_fraction' : [0.9,], \\\n",
    "             'lgb__bagging_freq' : [1,], \\\n",
    "             'lgb__bagging_seed' : [123], \\\n",
    "            },\n",
    "    \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def execute( _x_train,\n",
    "             _y_train,\n",
    "             _x_test,\n",
    "             _y_test, \n",
    "            classifiers, \n",
    "            classifier_names, \n",
    "            classifier_param_grid,\n",
    "            cv  = 10 , \n",
    "            score = 'accuracy',\n",
    "            scaler = StandardScaler()\n",
    "           ):\n",
    "    \n",
    "    '''\n",
    "    This method will run your data sets against the model specified \n",
    "    Models will be fed through a pipeline where the first step would be to\n",
    "    execute a scaling operation.\n",
    "    \n",
    "    Method will also call additional lower level methods in order to plot\n",
    "    precision curve, roc curve, learning curve and will also prepare a confusion matrix\n",
    "    \n",
    "    :returns: dict containing execution metrics such as time taken, accuracy scores\n",
    "    :returntype: dict\n",
    "    \n",
    "    '''\n",
    "\n",
    "    timer = CodeTimer(name='overalltime')\n",
    "    model_metrics = {}\n",
    "\n",
    "    with timer:\n",
    "        for model, model_name, model_param_grid in zip(classifiers, classifier_names, classifier_param_grid):\n",
    "\n",
    "            pipeline = Pipeline([\n",
    "                    ('scaler', scaler),\n",
    "                    (model_name, model)\n",
    "            ])\n",
    "\n",
    "            result = runGridSearchAndPredict(pipeline, \n",
    "                                             _x_train,\n",
    "                                             _y_train,\n",
    "                                             _x_test,\n",
    "                                             _y_test, \n",
    "                                             model_param_grid ,\n",
    "                                             cv = cv,\n",
    "                                             score = 'f1_weighted')\n",
    "\n",
    "            _y_prediction = result['_y_prediction']\n",
    "\n",
    "#             cf = panda.crosstab(\n",
    "#                 panda.Series(_y_test, name='Actual'),\n",
    "#                 panda.Series(_y_prediction, name = 'Prediction'),\n",
    "#                 margins = True\n",
    "#             )\n",
    "\n",
    "#             print(cf)\n",
    "\n",
    "            model_metrics[model_name] = {}\n",
    "            model_metrics[model_name]['confusion_matrix'] = result.get('confusion_matrix')\n",
    "            model_metrics[model_name]['training_time'] = result.get('training_time')\n",
    "            model_metrics[model_name]['testing_time'] = result.get('testing_time')\n",
    "            model_metrics[model_name]['learning_curve_time'] = result.get('learning_curve_time')\n",
    "            model_metrics[model_name]['f1_score'] = result.get('f1_score')\n",
    "            model_metrics[model_name]['f1_score_macro'] = result.get('f1_score_macro')\n",
    "            model_metrics[model_name]['f1_score_micro'] = result.get('f1_score_micro')\n",
    "            model_metrics[model_name]['roc_auc_macro'] = result.get('roc_auc_macro')\n",
    "            model_metrics[model_name]['roc_auc_micro'] = result.get('roc_auc_micro')\n",
    "\n",
    "            model_metrics[model_name]['best_estimator'] = result.get('best_estimator')\n",
    "#             model_metrics[model_name]['cf'] = cf\n",
    "\n",
    "\n",
    "    print(timer.took)\n",
    "    \n",
    "    return model_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = panda.read_csv('data/train_2.csv')\n",
    "test_data = panda.read_csv('data/test_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns_not_required = ['id','idhogar','target']\n",
    "x = train_data[[i for i in train_data.columns.tolist() if i not in columns_not_required]]\n",
    "y = train_data['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_x_train,_x_test,_y_train,_y_test = train_test_split(x,y, test_size = 0.3, stratify= y, random_state =12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1368"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_y_train[_y_train==4].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "f, (ax1, ax2) = plot.subplots(1, 2, sharey=True, figsize=(10,4))\n",
    "_y_train.value_counts().plot(kind='bar', label='Distribution of Training targets',ax=ax1)\n",
    "\n",
    "ax1.set_xlabel('Target classes')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Distribution of Training Targets')\n",
    "\n",
    "_y_test.value_counts().plot(kind='bar', label='Distribution of Training targets', ax=ax2)\n",
    "\n",
    "ax2.set_xlabel('Target classes')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Distribution of Test Targets')\n",
    "plot.tight_layout()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits = 5, shuffle= True, random_state =123)\n",
    "score= 'f1_weighted'\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = execute(_x_train,\n",
    "        _y_train,\n",
    "        _x_test,\n",
    "        _y_test,\n",
    "        classifiers,\n",
    "        classifier_names,\n",
    "        classifier_param_grid,\n",
    "        cv=cv,\n",
    "        score=score,\n",
    "        scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = panda.DataFrame(response).transpose()\n",
    "\n",
    "# we are sorting our results using f1_score_micro, since it gives the best indicator for sorting multi class imbalanced set\n",
    "results[['f1_score',\n",
    "         'f1_score_macro',\n",
    "         'f1_score_micro',\n",
    "         'learning_curve_time',\n",
    "         'roc_auc_macro',\n",
    "         'roc_auc_micro',\n",
    "         'testing_time',\n",
    "         'training_time']]\\\n",
    ".sort_values(by=['f1_score_micro',],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['learning_curve_time'] = results['learning_curve_time'].astype('float64')\n",
    "results['testing_time'] = results['testing_time'].astype('float64')\n",
    "results['training_time'] = results['training_time'].astype('float64')\n",
    "results['f1_score'] = results['f1_score'].astype('float64')\n",
    "results['f1_score_micro'] = results['f1_score_micro'].astype('float64')\n",
    "results['f1_score_macro'] = results['f1_score_macro'].astype('float64')\n",
    "results['roc_auc_micro'] = results['roc_auc_micro'].astype('float64')\n",
    "results['roc_auc_macro'] = results['roc_auc_macro'].astype('float64')\n",
    "\n",
    "#scaling time parameters between 0 and 1\n",
    "results['learning_curve_time'] = (results['learning_curve_time']- results['learning_curve_time'].min())/(results['learning_curve_time'].max()- results['learning_curve_time'].min())\n",
    "results['testing_time'] = (results['testing_time']- results['testing_time'].min())/(results['testing_time'].max()- results['testing_time'].min())\n",
    "results['training_time'] = (results['training_time']- results['training_time'].min())/(results['training_time'].max()- results['training_time'].min())\n",
    "\n",
    "results.plot(kind='barh',figsize=(12, 10))\n",
    "plot.title(\"Scaled Estimates across different classifiers used\")\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion on our baseline model: Logistic Regression works the best with f1_micro score of 0.66 and roc score of 0.86\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
