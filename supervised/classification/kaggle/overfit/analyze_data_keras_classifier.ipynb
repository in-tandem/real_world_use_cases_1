{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do Not Overfit Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kaggle recently launched a challenge to run classification models on datasets with a very particular goal - do not overfit!!\n",
    "\n",
    "#### Datasets had only 250 rows and features around 320. Since our number of independent features is greater than sample size, we are faced with a very pressing overfitting challenge.\n",
    "\n",
    "##### Below I enumerate the methodology to reduce overfitting :\n",
    "\n",
    "###### 1. Reduce feature size using two specific techniques: pearsons  correlation coefficient and dimensionality reduction techniques\n",
    "###### 2. Create handful of samples with goal on simplicity. eg less number of estimators,less number of iterations etc\n",
    "###### 3. Provide regularization parameters where applicable\n",
    "###### 4. Plot precision-recall curves and roc-auc curves for each model. Check learning curve as well.\n",
    "###### 5. Use StratifiedKFold cross validation to parameter tune\n",
    "###### 6. Deal with imbalance in target classes using under sampling and over sampling techniques\n",
    "\n",
    "\n",
    "\n",
    "Note: baseline model creation(using all features + regularization + tsne dimensionality reduction + over/under sampling) technique provided no real use. CV scores for roc auc did not exceed 0.6 in every scenario possible.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\somak\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as panda\n",
    "from matplotlib import pyplot as plot\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as panda\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler,RobustScaler,label_binarize\n",
    "\n",
    "from sklearn.model_selection import train_test_split,KFold,\\\n",
    "    StratifiedKFold,GridSearchCV,RepeatedStratifiedKFold,learning_curve, cross_val_score\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, roc_curve, auc, precision_recall_curve, \\\n",
    "        classification_report,confusion_matrix,average_precision_score\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression,RidgeClassifier,SGDClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,VotingClassifier,AdaBoostClassifier,GradientBoostingClassifier,BaggingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot as plot\n",
    "from itertools import cycle\n",
    "import numpy as np \n",
    "from scipy import interp\n",
    "import seaborn as sns\n",
    "import itertools, time, datetime\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif,f_classif\n",
    "from numpy import bincount, linspace, mean, std, arange, squeeze\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, GaussianNoise,Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.callbacks import EarlyStopping,ReduceLROnPlateau\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras import regularizers\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(143)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = panda.read_csv('data/train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0    160\n",
       " 0.0     90\n",
       " Name: target, dtype: int64, (250, 302))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.target.value_counts(), train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['target'] = train_data.target.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot.figure(figsize=(10,4))\n",
    "train_data.target.value_counts().plot(kind='bar')\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.023292</td>\n",
       "      <td>0.998354</td>\n",
       "      <td>-2.319</td>\n",
       "      <td>-0.64475</td>\n",
       "      <td>-0.0155</td>\n",
       "      <td>0.67700</td>\n",
       "      <td>2.567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>250.0</td>\n",
       "      <td>-0.026872</td>\n",
       "      <td>1.009314</td>\n",
       "      <td>-2.931</td>\n",
       "      <td>-0.73975</td>\n",
       "      <td>0.0570</td>\n",
       "      <td>0.62075</td>\n",
       "      <td>2.419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.167404</td>\n",
       "      <td>1.021709</td>\n",
       "      <td>-2.477</td>\n",
       "      <td>-0.42525</td>\n",
       "      <td>0.1840</td>\n",
       "      <td>0.80500</td>\n",
       "      <td>3.392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.001904</td>\n",
       "      <td>1.011751</td>\n",
       "      <td>-2.359</td>\n",
       "      <td>-0.68650</td>\n",
       "      <td>-0.0165</td>\n",
       "      <td>0.72000</td>\n",
       "      <td>2.771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.001588</td>\n",
       "      <td>1.035411</td>\n",
       "      <td>-2.566</td>\n",
       "      <td>-0.65900</td>\n",
       "      <td>-0.0230</td>\n",
       "      <td>0.73500</td>\n",
       "      <td>2.901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>250.0</td>\n",
       "      <td>-0.007304</td>\n",
       "      <td>0.955700</td>\n",
       "      <td>-2.845</td>\n",
       "      <td>-0.64375</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>0.66050</td>\n",
       "      <td>2.793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.032052</td>\n",
       "      <td>1.006657</td>\n",
       "      <td>-2.976</td>\n",
       "      <td>-0.67500</td>\n",
       "      <td>0.0605</td>\n",
       "      <td>0.78325</td>\n",
       "      <td>2.546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.078412</td>\n",
       "      <td>0.939731</td>\n",
       "      <td>-3.444</td>\n",
       "      <td>-0.55075</td>\n",
       "      <td>0.1835</td>\n",
       "      <td>0.76625</td>\n",
       "      <td>2.846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>250.0</td>\n",
       "      <td>-0.036920</td>\n",
       "      <td>0.963688</td>\n",
       "      <td>-2.768</td>\n",
       "      <td>-0.68950</td>\n",
       "      <td>-0.0125</td>\n",
       "      <td>0.63500</td>\n",
       "      <td>2.512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.035448</td>\n",
       "      <td>1.019689</td>\n",
       "      <td>-2.361</td>\n",
       "      <td>-0.64350</td>\n",
       "      <td>0.0520</td>\n",
       "      <td>0.73300</td>\n",
       "      <td>2.959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>250.0</td>\n",
       "      <td>-0.005032</td>\n",
       "      <td>1.085089</td>\n",
       "      <td>-3.302</td>\n",
       "      <td>-0.69350</td>\n",
       "      <td>0.0660</td>\n",
       "      <td>0.69425</td>\n",
       "      <td>3.271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.110248</td>\n",
       "      <td>1.036265</td>\n",
       "      <td>-2.851</td>\n",
       "      <td>-0.52400</td>\n",
       "      <td>0.1155</td>\n",
       "      <td>0.78625</td>\n",
       "      <td>2.998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.019808</td>\n",
       "      <td>1.050041</td>\n",
       "      <td>-2.681</td>\n",
       "      <td>-0.70850</td>\n",
       "      <td>0.0900</td>\n",
       "      <td>0.80525</td>\n",
       "      <td>2.729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>250.0</td>\n",
       "      <td>-0.001108</td>\n",
       "      <td>1.024305</td>\n",
       "      <td>-2.596</td>\n",
       "      <td>-0.69200</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.65400</td>\n",
       "      <td>2.651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>250.0</td>\n",
       "      <td>-0.016280</td>\n",
       "      <td>0.926789</td>\n",
       "      <td>-3.275</td>\n",
       "      <td>-0.67700</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.61100</td>\n",
       "      <td>2.913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>250.0</td>\n",
       "      <td>-0.039644</td>\n",
       "      <td>0.955915</td>\n",
       "      <td>-3.512</td>\n",
       "      <td>-0.63450</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.57800</td>\n",
       "      <td>2.508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.017260</td>\n",
       "      <td>1.025655</td>\n",
       "      <td>-2.476</td>\n",
       "      <td>-0.68350</td>\n",
       "      <td>-0.1190</td>\n",
       "      <td>0.69900</td>\n",
       "      <td>3.286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>250.0</td>\n",
       "      <td>-0.106856</td>\n",
       "      <td>1.012777</td>\n",
       "      <td>-3.619</td>\n",
       "      <td>-0.80150</td>\n",
       "      <td>-0.1645</td>\n",
       "      <td>0.49650</td>\n",
       "      <td>2.430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.036184</td>\n",
       "      <td>0.945099</td>\n",
       "      <td>-2.428</td>\n",
       "      <td>-0.57425</td>\n",
       "      <td>-0.0095</td>\n",
       "      <td>0.68600</td>\n",
       "      <td>2.557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>250.0</td>\n",
       "      <td>-0.043296</td>\n",
       "      <td>1.055935</td>\n",
       "      <td>-3.229</td>\n",
       "      <td>-0.75800</td>\n",
       "      <td>-0.0180</td>\n",
       "      <td>0.69800</td>\n",
       "      <td>2.868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>250.0</td>\n",
       "      <td>-0.110832</td>\n",
       "      <td>1.003178</td>\n",
       "      <td>-3.024</td>\n",
       "      <td>-0.87050</td>\n",
       "      <td>-0.1615</td>\n",
       "      <td>0.56050</td>\n",
       "      <td>2.703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.072680</td>\n",
       "      <td>1.039556</td>\n",
       "      <td>-2.775</td>\n",
       "      <td>-0.59600</td>\n",
       "      <td>0.0480</td>\n",
       "      <td>0.79700</td>\n",
       "      <td>2.691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.017296</td>\n",
       "      <td>0.988482</td>\n",
       "      <td>-2.962</td>\n",
       "      <td>-0.72575</td>\n",
       "      <td>0.1350</td>\n",
       "      <td>0.63150</td>\n",
       "      <td>2.604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>250.0</td>\n",
       "      <td>-0.030728</td>\n",
       "      <td>0.945902</td>\n",
       "      <td>-2.490</td>\n",
       "      <td>-0.65200</td>\n",
       "      <td>-0.0160</td>\n",
       "      <td>0.61925</td>\n",
       "      <td>2.362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>250.0</td>\n",
       "      <td>-0.128252</td>\n",
       "      <td>0.997026</td>\n",
       "      <td>-3.107</td>\n",
       "      <td>-0.77950</td>\n",
       "      <td>-0.1655</td>\n",
       "      <td>0.57925</td>\n",
       "      <td>2.927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.154736</td>\n",
       "      <td>0.997894</td>\n",
       "      <td>-2.943</td>\n",
       "      <td>-0.42425</td>\n",
       "      <td>0.1255</td>\n",
       "      <td>0.71950</td>\n",
       "      <td>2.976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.083408</td>\n",
       "      <td>1.040371</td>\n",
       "      <td>-2.933</td>\n",
       "      <td>-0.58575</td>\n",
       "      <td>0.0365</td>\n",
       "      <td>0.79825</td>\n",
       "      <td>2.581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.039552</td>\n",
       "      <td>0.922270</td>\n",
       "      <td>-2.942</td>\n",
       "      <td>-0.62500</td>\n",
       "      <td>0.0450</td>\n",
       "      <td>0.72175</td>\n",
       "      <td>2.305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>250.0</td>\n",
       "      <td>-0.091784</td>\n",
       "      <td>1.047282</td>\n",
       "      <td>-2.957</td>\n",
       "      <td>-0.75125</td>\n",
       "      <td>-0.0260</td>\n",
       "      <td>0.67850</td>\n",
       "      <td>2.489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.054636</td>\n",
       "      <td>1.041432</td>\n",
       "      <td>-2.911</td>\n",
       "      <td>-0.58250</td>\n",
       "      <td>0.0450</td>\n",
       "      <td>0.72875</td>\n",
       "      <td>2.895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>250.0</td>\n",
       "      <td>-0.066236</td>\n",
       "      <td>0.985451</td>\n",
       "      <td>-2.448</td>\n",
       "      <td>-0.78675</td>\n",
       "      <td>-0.1010</td>\n",
       "      <td>0.52575</td>\n",
       "      <td>3.753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>250.0</td>\n",
       "      <td>-0.057988</td>\n",
       "      <td>0.951879</td>\n",
       "      <td>-2.771</td>\n",
       "      <td>-0.70100</td>\n",
       "      <td>-0.1090</td>\n",
       "      <td>0.61200</td>\n",
       "      <td>2.498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.091556</td>\n",
       "      <td>1.027877</td>\n",
       "      <td>-2.903</td>\n",
       "      <td>-0.54325</td>\n",
       "      <td>0.0505</td>\n",
       "      <td>0.86400</td>\n",
       "      <td>2.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>250.0</td>\n",
       "      <td>-0.029896</td>\n",
       "      <td>0.966882</td>\n",
       "      <td>-2.522</td>\n",
       "      <td>-0.67275</td>\n",
       "      <td>-0.0815</td>\n",
       "      <td>0.63025</td>\n",
       "      <td>2.680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.115648</td>\n",
       "      <td>1.037173</td>\n",
       "      <td>-2.759</td>\n",
       "      <td>-0.62675</td>\n",
       "      <td>0.0875</td>\n",
       "      <td>0.83675</td>\n",
       "      <td>3.445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.007372</td>\n",
       "      <td>1.004543</td>\n",
       "      <td>-2.915</td>\n",
       "      <td>-0.73025</td>\n",
       "      <td>0.0350</td>\n",
       "      <td>0.71800</td>\n",
       "      <td>2.846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.033552</td>\n",
       "      <td>1.006219</td>\n",
       "      <td>-2.618</td>\n",
       "      <td>-0.64975</td>\n",
       "      <td>0.0865</td>\n",
       "      <td>0.79150</td>\n",
       "      <td>2.315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.090524</td>\n",
       "      <td>1.037119</td>\n",
       "      <td>-3.623</td>\n",
       "      <td>-0.58950</td>\n",
       "      <td>0.1265</td>\n",
       "      <td>0.72600</td>\n",
       "      <td>2.780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.001576</td>\n",
       "      <td>1.024067</td>\n",
       "      <td>-2.673</td>\n",
       "      <td>-0.72575</td>\n",
       "      <td>0.0535</td>\n",
       "      <td>0.68800</td>\n",
       "      <td>2.364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>250.0</td>\n",
       "      <td>-0.007784</td>\n",
       "      <td>1.056086</td>\n",
       "      <td>-3.229</td>\n",
       "      <td>-0.66775</td>\n",
       "      <td>0.0635</td>\n",
       "      <td>0.66575</td>\n",
       "      <td>2.908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.043184</td>\n",
       "      <td>1.012516</td>\n",
       "      <td>-2.537</td>\n",
       "      <td>-0.60500</td>\n",
       "      <td>0.0525</td>\n",
       "      <td>0.60375</td>\n",
       "      <td>2.926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.082696</td>\n",
       "      <td>1.068741</td>\n",
       "      <td>-2.748</td>\n",
       "      <td>-0.63775</td>\n",
       "      <td>0.1030</td>\n",
       "      <td>0.70550</td>\n",
       "      <td>3.441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.098476</td>\n",
       "      <td>0.934163</td>\n",
       "      <td>-2.850</td>\n",
       "      <td>-0.45825</td>\n",
       "      <td>0.1355</td>\n",
       "      <td>0.68825</td>\n",
       "      <td>2.319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.055356</td>\n",
       "      <td>0.988100</td>\n",
       "      <td>-2.577</td>\n",
       "      <td>-0.55350</td>\n",
       "      <td>0.0390</td>\n",
       "      <td>0.75900</td>\n",
       "      <td>2.842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.111708</td>\n",
       "      <td>1.043230</td>\n",
       "      <td>-2.973</td>\n",
       "      <td>-0.56675</td>\n",
       "      <td>0.0935</td>\n",
       "      <td>0.70475</td>\n",
       "      <td>3.343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>250.0</td>\n",
       "      <td>-0.015688</td>\n",
       "      <td>1.010720</td>\n",
       "      <td>-2.709</td>\n",
       "      <td>-0.77825</td>\n",
       "      <td>0.0145</td>\n",
       "      <td>0.67375</td>\n",
       "      <td>3.266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.035992</td>\n",
       "      <td>1.058982</td>\n",
       "      <td>-3.605</td>\n",
       "      <td>-0.69325</td>\n",
       "      <td>-0.0075</td>\n",
       "      <td>0.74875</td>\n",
       "      <td>3.061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.026452</td>\n",
       "      <td>0.896318</td>\n",
       "      <td>-2.357</td>\n",
       "      <td>-0.59675</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.60425</td>\n",
       "      <td>2.146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>250.0</td>\n",
       "      <td>-0.059152</td>\n",
       "      <td>1.113760</td>\n",
       "      <td>-2.904</td>\n",
       "      <td>-0.78900</td>\n",
       "      <td>-0.1225</td>\n",
       "      <td>0.65050</td>\n",
       "      <td>2.853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.077272</td>\n",
       "      <td>0.972530</td>\n",
       "      <td>-2.734</td>\n",
       "      <td>-0.67125</td>\n",
       "      <td>0.0575</td>\n",
       "      <td>0.77250</td>\n",
       "      <td>3.026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.044652</td>\n",
       "      <td>1.011416</td>\n",
       "      <td>-2.804</td>\n",
       "      <td>-0.61700</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.79725</td>\n",
       "      <td>2.865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.126344</td>\n",
       "      <td>0.972567</td>\n",
       "      <td>-2.443</td>\n",
       "      <td>-0.51050</td>\n",
       "      <td>0.0910</td>\n",
       "      <td>0.80425</td>\n",
       "      <td>2.801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.018436</td>\n",
       "      <td>0.954229</td>\n",
       "      <td>-2.757</td>\n",
       "      <td>-0.53575</td>\n",
       "      <td>0.0575</td>\n",
       "      <td>0.63150</td>\n",
       "      <td>2.736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>250.0</td>\n",
       "      <td>-0.012092</td>\n",
       "      <td>0.960630</td>\n",
       "      <td>-2.466</td>\n",
       "      <td>-0.65700</td>\n",
       "      <td>-0.0210</td>\n",
       "      <td>0.65025</td>\n",
       "      <td>2.596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>250.0</td>\n",
       "      <td>-0.065720</td>\n",
       "      <td>1.057414</td>\n",
       "      <td>-3.287</td>\n",
       "      <td>-0.81850</td>\n",
       "      <td>-0.0090</td>\n",
       "      <td>0.73950</td>\n",
       "      <td>2.226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>250.0</td>\n",
       "      <td>-0.106112</td>\n",
       "      <td>1.038389</td>\n",
       "      <td>-3.072</td>\n",
       "      <td>-0.82100</td>\n",
       "      <td>-0.0795</td>\n",
       "      <td>0.49300</td>\n",
       "      <td>3.131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.046472</td>\n",
       "      <td>0.967661</td>\n",
       "      <td>-2.634</td>\n",
       "      <td>-0.60550</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.68300</td>\n",
       "      <td>3.236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.998984</td>\n",
       "      <td>-2.776</td>\n",
       "      <td>-0.75125</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.79425</td>\n",
       "      <td>2.626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.009372</td>\n",
       "      <td>1.008099</td>\n",
       "      <td>-3.211</td>\n",
       "      <td>-0.55000</td>\n",
       "      <td>-0.0090</td>\n",
       "      <td>0.65425</td>\n",
       "      <td>3.530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>250.0</td>\n",
       "      <td>-0.128952</td>\n",
       "      <td>0.971219</td>\n",
       "      <td>-3.500</td>\n",
       "      <td>-0.75425</td>\n",
       "      <td>-0.1325</td>\n",
       "      <td>0.50325</td>\n",
       "      <td>2.771</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     count      mean       std    min      25%     50%      75%    max\n",
       "0    250.0  0.023292  0.998354 -2.319 -0.64475 -0.0155  0.67700  2.567\n",
       "1    250.0 -0.026872  1.009314 -2.931 -0.73975  0.0570  0.62075  2.419\n",
       "2    250.0  0.167404  1.021709 -2.477 -0.42525  0.1840  0.80500  3.392\n",
       "3    250.0  0.001904  1.011751 -2.359 -0.68650 -0.0165  0.72000  2.771\n",
       "4    250.0  0.001588  1.035411 -2.566 -0.65900 -0.0230  0.73500  2.901\n",
       "5    250.0 -0.007304  0.955700 -2.845 -0.64375  0.0375  0.66050  2.793\n",
       "6    250.0  0.032052  1.006657 -2.976 -0.67500  0.0605  0.78325  2.546\n",
       "7    250.0  0.078412  0.939731 -3.444 -0.55075  0.1835  0.76625  2.846\n",
       "8    250.0 -0.036920  0.963688 -2.768 -0.68950 -0.0125  0.63500  2.512\n",
       "9    250.0  0.035448  1.019689 -2.361 -0.64350  0.0520  0.73300  2.959\n",
       "10   250.0 -0.005032  1.085089 -3.302 -0.69350  0.0660  0.69425  3.271\n",
       "11   250.0  0.110248  1.036265 -2.851 -0.52400  0.1155  0.78625  2.998\n",
       "12   250.0  0.019808  1.050041 -2.681 -0.70850  0.0900  0.80525  2.729\n",
       "13   250.0 -0.001108  1.024305 -2.596 -0.69200  0.0160  0.65400  2.651\n",
       "14   250.0 -0.016280  0.926789 -3.275 -0.67700  0.0095  0.61100  2.913\n",
       "15   250.0 -0.039644  0.955915 -3.512 -0.63450  0.0100  0.57800  2.508\n",
       "16   250.0  0.017260  1.025655 -2.476 -0.68350 -0.1190  0.69900  3.286\n",
       "17   250.0 -0.106856  1.012777 -3.619 -0.80150 -0.1645  0.49650  2.430\n",
       "18   250.0  0.036184  0.945099 -2.428 -0.57425 -0.0095  0.68600  2.557\n",
       "19   250.0 -0.043296  1.055935 -3.229 -0.75800 -0.0180  0.69800  2.868\n",
       "20   250.0 -0.110832  1.003178 -3.024 -0.87050 -0.1615  0.56050  2.703\n",
       "21   250.0  0.072680  1.039556 -2.775 -0.59600  0.0480  0.79700  2.691\n",
       "22   250.0  0.017296  0.988482 -2.962 -0.72575  0.1350  0.63150  2.604\n",
       "23   250.0 -0.030728  0.945902 -2.490 -0.65200 -0.0160  0.61925  2.362\n",
       "24   250.0 -0.128252  0.997026 -3.107 -0.77950 -0.1655  0.57925  2.927\n",
       "25   250.0  0.154736  0.997894 -2.943 -0.42425  0.1255  0.71950  2.976\n",
       "26   250.0  0.083408  1.040371 -2.933 -0.58575  0.0365  0.79825  2.581\n",
       "27   250.0  0.039552  0.922270 -2.942 -0.62500  0.0450  0.72175  2.305\n",
       "28   250.0 -0.091784  1.047282 -2.957 -0.75125 -0.0260  0.67850  2.489\n",
       "29   250.0  0.054636  1.041432 -2.911 -0.58250  0.0450  0.72875  2.895\n",
       "..     ...       ...       ...    ...      ...     ...      ...    ...\n",
       "270  250.0 -0.066236  0.985451 -2.448 -0.78675 -0.1010  0.52575  3.753\n",
       "271  250.0 -0.057988  0.951879 -2.771 -0.70100 -0.1090  0.61200  2.498\n",
       "272  250.0  0.091556  1.027877 -2.903 -0.54325  0.0505  0.86400  2.725\n",
       "273  250.0 -0.029896  0.966882 -2.522 -0.67275 -0.0815  0.63025  2.680\n",
       "274  250.0  0.115648  1.037173 -2.759 -0.62675  0.0875  0.83675  3.445\n",
       "275  250.0  0.007372  1.004543 -2.915 -0.73025  0.0350  0.71800  2.846\n",
       "276  250.0  0.033552  1.006219 -2.618 -0.64975  0.0865  0.79150  2.315\n",
       "277  250.0  0.090524  1.037119 -3.623 -0.58950  0.1265  0.72600  2.780\n",
       "278  250.0  0.001576  1.024067 -2.673 -0.72575  0.0535  0.68800  2.364\n",
       "279  250.0 -0.007784  1.056086 -3.229 -0.66775  0.0635  0.66575  2.908\n",
       "280  250.0  0.043184  1.012516 -2.537 -0.60500  0.0525  0.60375  2.926\n",
       "281  250.0  0.082696  1.068741 -2.748 -0.63775  0.1030  0.70550  3.441\n",
       "282  250.0  0.098476  0.934163 -2.850 -0.45825  0.1355  0.68825  2.319\n",
       "283  250.0  0.055356  0.988100 -2.577 -0.55350  0.0390  0.75900  2.842\n",
       "284  250.0  0.111708  1.043230 -2.973 -0.56675  0.0935  0.70475  3.343\n",
       "285  250.0 -0.015688  1.010720 -2.709 -0.77825  0.0145  0.67375  3.266\n",
       "286  250.0  0.035992  1.058982 -3.605 -0.69325 -0.0075  0.74875  3.061\n",
       "287  250.0  0.026452  0.896318 -2.357 -0.59675  0.0005  0.60425  2.146\n",
       "288  250.0 -0.059152  1.113760 -2.904 -0.78900 -0.1225  0.65050  2.853\n",
       "289  250.0  0.077272  0.972530 -2.734 -0.67125  0.0575  0.77250  3.026\n",
       "290  250.0  0.044652  1.011416 -2.804 -0.61700  0.0675  0.79725  2.865\n",
       "291  250.0  0.126344  0.972567 -2.443 -0.51050  0.0910  0.80425  2.801\n",
       "292  250.0  0.018436  0.954229 -2.757 -0.53575  0.0575  0.63150  2.736\n",
       "293  250.0 -0.012092  0.960630 -2.466 -0.65700 -0.0210  0.65025  2.596\n",
       "294  250.0 -0.065720  1.057414 -3.287 -0.81850 -0.0090  0.73950  2.226\n",
       "295  250.0 -0.106112  1.038389 -3.072 -0.82100 -0.0795  0.49300  3.131\n",
       "296  250.0  0.046472  0.967661 -2.634 -0.60550  0.0095  0.68300  3.236\n",
       "297  250.0  0.006452  0.998984 -2.776 -0.75125  0.0055  0.79425  2.626\n",
       "298  250.0  0.009372  1.008099 -3.211 -0.55000 -0.0090  0.65425  3.530\n",
       "299  250.0 -0.128952  0.971219 -3.500 -0.75425 -0.1325  0.50325  2.771\n",
       "\n",
       "[300 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[[i for i in train_data.columns.tolist() if i not in ['target','id']]].describe(include='all').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_name</th>\n",
       "      <th>col_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [col_name, col_type]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_type = train_data.dtypes.to_frame().reset_index()\n",
    "data_type.columns  = ['col_name','col_type']\n",
    "data_type[data_type.col_type==np.object].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isnull().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [i for i in train_data.columns if i not in ['target','id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def calculateCorrelationCoefficientsAndpValues(x_data, y_data, xlabel):\n",
    "    \n",
    "    pearson_coef, p_value = stats.pearsonr(x_data, y_data)\n",
    "    print(\"The Pearson Correlation Coefficient for %s is %s with a P-value of P = %s\" %(xlabel,pearson_coef, p_value))\n",
    "    \n",
    "    return (pearson_coef,p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Pearson Correlation Coefficient for 0 is 0.10896613295780612 with a P-value of P = 0.0855425108978631\n",
      "The Pearson Correlation Coefficient for 1 is -0.07331927684495224 with a P-value of P = 0.2480801859987947\n",
      "The Pearson Correlation Coefficient for 2 is -0.015140943820013032 with a P-value of P = 0.8117157095966047\n",
      "The Pearson Correlation Coefficient for 3 is 0.011549011379043529 with a P-value of P = 0.8558206568804985\n",
      "The Pearson Correlation Coefficient for 4 is -0.11472596336421398 with a P-value of P = 0.07016102972091894\n",
      "The Pearson Correlation Coefficient for 5 is -0.05032883942137733 with a P-value of P = 0.42819565779211044\n",
      "The Pearson Correlation Coefficient for 6 is -0.05706281139477776 with a P-value of P = 0.36894411376906777\n",
      "The Pearson Correlation Coefficient for 7 is 0.0042391278386217585 with a P-value of P = 0.9468277872565647\n",
      "The Pearson Correlation Coefficient for 8 is -0.04844250987759829 with a P-value of P = 0.44573026468867594\n",
      "The Pearson Correlation Coefficient for 9 is -0.08594723846418578 with a P-value of P = 0.17553143505657193\n",
      "The Pearson Correlation Coefficient for 10 is -0.013612001065138336 with a P-value of P = 0.8304254460961307\n",
      "The Pearson Correlation Coefficient for 11 is -0.03146327173724238 with a P-value of P = 0.6205242541394711\n",
      "The Pearson Correlation Coefficient for 12 is 0.009102947815470285 with a P-value of P = 0.8861228341082578\n",
      "The Pearson Correlation Coefficient for 13 is 0.10782774112074847 with a P-value of P = 0.0888818139226698\n",
      "The Pearson Correlation Coefficient for 14 is 0.06362438861092952 with a P-value of P = 0.3163610323457606\n",
      "The Pearson Correlation Coefficient for 15 is -0.09413828837914727 with a P-value of P = 0.13773049807305365\n",
      "The Pearson Correlation Coefficient for 16 is -0.144266792744662 with a P-value of P = 0.022514438322226576\n",
      "The Pearson Correlation Coefficient for 17 is 0.11099815539409388 with a P-value of P = 0.07983294374650739\n",
      "The Pearson Correlation Coefficient for 18 is 0.05036504862506431 with a P-value of P = 0.4278630213113235\n",
      "The Pearson Correlation Coefficient for 19 is -0.048692959292580607 with a P-value of P = 0.44337903564942616\n",
      "The Pearson Correlation Coefficient for 20 is -0.002429491252169555 with a P-value of P = 0.9695113386247632\n",
      "The Pearson Correlation Coefficient for 21 is 0.04551275626072648 with a P-value of P = 0.4737537879070056\n",
      "The Pearson Correlation Coefficient for 22 is -0.018173291105579054 with a P-value of P = 0.774932855713625\n",
      "The Pearson Correlation Coefficient for 23 is -0.05141657625256417 with a P-value of P = 0.41826855060973167\n",
      "The Pearson Correlation Coefficient for 24 is 0.17309632914995238 with a P-value of P = 0.00607102751969165\n",
      "The Pearson Correlation Coefficient for 25 is 0.07261665861925834 with a P-value of P = 0.25265393575611567\n",
      "The Pearson Correlation Coefficient for 26 is -0.09743826211417392 with a P-value of P = 0.12439821405544421\n",
      "The Pearson Correlation Coefficient for 27 is 0.05637515717379669 with a P-value of P = 0.3747513839193273\n",
      "The Pearson Correlation Coefficient for 28 is -0.04133686657712149 with a P-value of P = 0.5153073833053428\n",
      "The Pearson Correlation Coefficient for 29 is 0.05535721050566726 with a P-value of P = 0.38345042596172885\n",
      "The Pearson Correlation Coefficient for 30 is 0.132704964391349 with a P-value of P = 0.03599201043094182\n",
      "The Pearson Correlation Coefficient for 31 is -0.018271001528041106 with a P-value of P = 0.7737554431298906\n",
      "The Pearson Correlation Coefficient for 32 is -0.031088060752615304 with a P-value of P = 0.6246999190835808\n",
      "The Pearson Correlation Coefficient for 33 is 0.3736084079793187 with a P-value of P = 1.0608082490880965e-09\n",
      "The Pearson Correlation Coefficient for 34 is -0.032471972912515364 with a P-value of P = 0.6093589721625381\n",
      "The Pearson Correlation Coefficient for 35 is -0.015632921755940575 with a P-value of P = 0.8057174667568586\n",
      "The Pearson Correlation Coefficient for 36 is 0.06083284275004163 with a P-value of P = 0.33810369731941203\n",
      "The Pearson Correlation Coefficient for 37 is -0.007289269257949458 with a P-value of P = 0.9087008340534317\n",
      "The Pearson Correlation Coefficient for 38 is -0.04054519472948481 with a P-value of P = 0.5233919848639443\n",
      "The Pearson Correlation Coefficient for 39 is -0.14291538793434105 with a P-value of P = 0.023822524993573888\n",
      "The Pearson Correlation Coefficient for 40 is -0.0037049419717683526 with a P-value of P = 0.9535201033452312\n",
      "The Pearson Correlation Coefficient for 41 is -0.007515631669586558 with a P-value of P = 0.9058785222391251\n",
      "The Pearson Correlation Coefficient for 42 is 0.03775249698772097 with a P-value of P = 0.5524186409153129\n",
      "The Pearson Correlation Coefficient for 43 is -0.12141272613674027 with a P-value of P = 0.0552148909056543\n",
      "The Pearson Correlation Coefficient for 44 is 0.00844056692951176 with a P-value of P = 0.8943589723240533\n",
      "The Pearson Correlation Coefficient for 45 is -0.09027970714682543 with a P-value of P = 0.1546777186964068\n",
      "The Pearson Correlation Coefficient for 46 is 0.07459816642585058 with a P-value of P = 0.2399036439331226\n",
      "The Pearson Correlation Coefficient for 47 is 0.04231616998935063 with a P-value of P = 0.5053965956826826\n",
      "The Pearson Correlation Coefficient for 48 is -0.0463162453957421 with a P-value of P = 0.46597387586577377\n",
      "The Pearson Correlation Coefficient for 49 is -0.014354654290941786 with a P-value of P = 0.821324935304374\n",
      "The Pearson Correlation Coefficient for 50 is 0.051054690235304775 with a P-value of P = 0.42155621651341435\n",
      "The Pearson Correlation Coefficient for 51 is -0.012838534153879616 with a P-value of P = 0.8399276253771486\n",
      "The Pearson Correlation Coefficient for 52 is -0.06877669630099441 with a P-value of P = 0.2786803996161697\n",
      "The Pearson Correlation Coefficient for 53 is 0.04843410534510945 with a P-value of P = 0.44580928902404027\n",
      "The Pearson Correlation Coefficient for 54 is 0.035848452500776985 with a P-value of P = 0.5726489442932259\n",
      "The Pearson Correlation Coefficient for 55 is -0.05238600022724242 with a P-value of P = 0.409535795591346\n",
      "The Pearson Correlation Coefficient for 56 is -0.0390058911752561 with a P-value of P = 0.5392945190182408\n",
      "The Pearson Correlation Coefficient for 57 is -0.020597565813032483 with a P-value of P = 0.7458787105907005\n",
      "The Pearson Correlation Coefficient for 58 is -0.035136836853515985 with a P-value of P = 0.5802985172722148\n",
      "The Pearson Correlation Coefficient for 59 is 0.05049782475891964 with a P-value of P = 0.42664455120482847\n",
      "The Pearson Correlation Coefficient for 60 is 0.016479976126899828 with a P-value of P = 0.7954168578779419\n",
      "The Pearson Correlation Coefficient for 61 is -0.0624175115220229 with a P-value of P = 0.3256466456628453\n",
      "The Pearson Correlation Coefficient for 62 is -0.08267154435164815 with a P-value of P = 0.1926320143585601\n",
      "The Pearson Correlation Coefficient for 63 is -0.12743956238357956 with a P-value of P = 0.04410267538889468\n",
      "The Pearson Correlation Coefficient for 64 is -0.05512032610202086 with a P-value of P = 0.38549225397777187\n",
      "The Pearson Correlation Coefficient for 65 is 0.2938463193509698 with a P-value of P = 2.2716945844200894e-06\n",
      "The Pearson Correlation Coefficient for 66 is -0.028691255437740985 with a P-value of P = 0.6516522376189233\n",
      "The Pearson Correlation Coefficient for 67 is 0.004668020933137718 with a P-value of P = 0.9414572612953114\n",
      "The Pearson Correlation Coefficient for 68 is -0.04441080419255426 with a P-value of P = 0.48453805268658334\n",
      "The Pearson Correlation Coefficient for 69 is 0.0045140281412347156 with a P-value of P = 0.9433852410063895\n",
      "The Pearson Correlation Coefficient for 70 is -0.07478376304495758 with a P-value of P = 0.238732911930959\n",
      "The Pearson Correlation Coefficient for 71 is 0.030142467114210713 with a P-value of P = 0.6352763100827648\n",
      "The Pearson Correlation Coefficient for 72 is 0.07522663270949621 with a P-value of P = 0.23595552289288318\n",
      "The Pearson Correlation Coefficient for 73 is -0.16755683600563856 with a P-value of P = 0.007934978403993069\n",
      "The Pearson Correlation Coefficient for 74 is 0.0026317994756393337 with a P-value of P = 0.9669738777155372\n",
      "The Pearson Correlation Coefficient for 75 is -0.0006936803998717512 with a P-value of P = 0.9912927911459983\n",
      "The Pearson Correlation Coefficient for 76 is -0.07015626458778323 with a P-value of P = 0.26912883834372414\n",
      "The Pearson Correlation Coefficient for 77 is -0.035457135747727124 with a P-value of P = 0.5768495490838261\n",
      "The Pearson Correlation Coefficient for 78 is 0.04056617574356557 with a P-value of P = 0.523176894031681\n",
      "The Pearson Correlation Coefficient for 79 is 0.03324955852855977 with a P-value of P = 0.6008130175720164\n",
      "The Pearson Correlation Coefficient for 80 is -0.162557800679397 with a P-value of P = 0.010037630606028513\n",
      "The Pearson Correlation Coefficient for 81 is 0.03151540350197794 with a P-value of P = 0.6199450443523261\n",
      "The Pearson Correlation Coefficient for 82 is -0.128247893943336 with a P-value of P = 0.042765941897649824\n",
      "The Pearson Correlation Coefficient for 83 is -0.02877020620854552 with a P-value of P = 0.6507569307481538\n",
      "The Pearson Correlation Coefficient for 84 is 0.04335907527720279 with a P-value of P = 0.49495289285000754\n",
      "The Pearson Correlation Coefficient for 85 is -0.05517000364738988 with a P-value of P = 0.38506351151317886\n",
      "The Pearson Correlation Coefficient for 86 is -0.04616146130261415 with a P-value of P = 0.46746709062143754\n",
      "The Pearson Correlation Coefficient for 87 is -0.01628251282319217 with a P-value of P = 0.7978150164562104\n",
      "The Pearson Correlation Coefficient for 88 is 0.014069962019947778 with a P-value of P = 0.8248108151984158\n",
      "The Pearson Correlation Coefficient for 89 is 0.09952582637060914 with a P-value of P = 0.11649546079901389\n",
      "The Pearson Correlation Coefficient for 90 is -0.1362296801225528 with a P-value of P = 0.031300308066093574\n",
      "The Pearson Correlation Coefficient for 91 is -0.19253612919360197 with a P-value of P = 0.0022306781001398515\n",
      "The Pearson Correlation Coefficient for 92 is 0.011131349091020804 with a P-value of P = 0.8609810478781248\n",
      "The Pearson Correlation Coefficient for 93 is -0.026397637219527346 with a P-value of P = 0.6778754891799935\n",
      "The Pearson Correlation Coefficient for 94 is -0.07059609862502574 with a P-value of P = 0.2661311282101072\n",
      "The Pearson Correlation Coefficient for 95 is -0.06283916867491353 with a P-value of P = 0.32238262579402754\n",
      "The Pearson Correlation Coefficient for 96 is -0.05463245793005215 with a P-value of P = 0.3897181907570928\n",
      "The Pearson Correlation Coefficient for 97 is -0.008075218306930991 with a P-value of P = 0.8989066757456521\n",
      "The Pearson Correlation Coefficient for 98 is -0.10170166319281916 with a P-value of P = 0.10868150060452884\n",
      "The Pearson Correlation Coefficient for 99 is 0.0655415083046409 with a P-value of P = 0.3019695136002106\n",
      "The Pearson Correlation Coefficient for 100 is 0.0410549393345614 with a P-value of P = 0.5181790348249693\n",
      "The Pearson Correlation Coefficient for 101 is 0.11837851404016257 with a P-value of P = 0.06163323515988077\n",
      "The Pearson Correlation Coefficient for 102 is -0.04665062936745738 with a P-value of P = 0.4627570528698074\n",
      "The Pearson Correlation Coefficient for 103 is 0.04382892946925664 with a P-value of P = 0.4902854809199967\n",
      "The Pearson Correlation Coefficient for 104 is 0.08165611079766583 with a P-value of P = 0.19817238879494672\n",
      "The Pearson Correlation Coefficient for 105 is 0.11058899292112229 with a P-value of P = 0.08095715371350645\n",
      "The Pearson Correlation Coefficient for 106 is -0.06362117710479967 with a P-value of P = 0.3163855099350282\n",
      "The Pearson Correlation Coefficient for 107 is 0.08024228795032205 with a P-value of P = 0.20607806814380508\n",
      "The Pearson Correlation Coefficient for 108 is -0.11387451945913514 with a P-value of P = 0.07228138092367574\n",
      "The Pearson Correlation Coefficient for 109 is -0.0037320658164643186 with a P-value of P = 0.9531802115205463\n",
      "The Pearson Correlation Coefficient for 110 is -0.0655764203791134 with a P-value of P = 0.3017115142411731\n",
      "The Pearson Correlation Coefficient for 111 is -0.005309648859291381 with a P-value of P = 0.9334279179738477\n",
      "The Pearson Correlation Coefficient for 112 is 0.0628949898766039 with a P-value of P = 0.32195211329800355\n",
      "The Pearson Correlation Coefficient for 113 is -0.06147047044547484 with a P-value of P = 0.333055202545007\n",
      "The Pearson Correlation Coefficient for 114 is 0.12479181895723682 with a P-value of P = 0.04872906660134762\n",
      "The Pearson Correlation Coefficient for 115 is 0.03261913741929072 with a P-value of P = 0.6077374613689721\n",
      "The Pearson Correlation Coefficient for 116 is -0.054593425946369775 with a P-value of P = 0.3900574931244022\n",
      "The Pearson Correlation Coefficient for 117 is -0.1974960622259256 with a P-value of P = 0.0017010335652401756\n",
      "The Pearson Correlation Coefficient for 118 is -0.02212824596798594 with a P-value of P = 0.727714394640391\n",
      "The Pearson Correlation Coefficient for 119 is 0.0926223876702724 with a P-value of P = 0.14421037952535476\n",
      "The Pearson Correlation Coefficient for 120 is -0.06691431873488267 with a P-value of P = 0.291934352392036\n",
      "The Pearson Correlation Coefficient for 121 is -0.01739726836304317 with a P-value of P = 0.7843020093446557\n",
      "The Pearson Correlation Coefficient for 122 is 0.01222482050957448 with a P-value of P = 0.8474838117156436\n",
      "The Pearson Correlation Coefficient for 123 is -0.026253741063693787 with a P-value of P = 0.679534095519621\n",
      "The Pearson Correlation Coefficient for 124 is -0.009550096279148754 with a P-value of P = 0.8805697756609052\n",
      "The Pearson Correlation Coefficient for 125 is -0.02287578037052994 with a P-value of P = 0.7188977680958824\n",
      "The Pearson Correlation Coefficient for 126 is 0.04650309121296987 with a P-value of P = 0.4641748719790971\n",
      "The Pearson Correlation Coefficient for 127 is -0.10155781738791524 with a P-value of P = 0.10918502519905922\n",
      "The Pearson Correlation Coefficient for 128 is -0.033039577569966574 with a P-value of P = 0.6031154790367645\n",
      "The Pearson Correlation Coefficient for 129 is -0.13943862416906938 with a P-value of P = 0.027492743358450394\n",
      "The Pearson Correlation Coefficient for 130 is 0.046614578224327796 with a P-value of P = 0.4631032770583209\n",
      "The Pearson Correlation Coefficient for 131 is 0.028945514087535217 with a P-value of P = 0.6487707192968777\n",
      "The Pearson Correlation Coefficient for 132 is -0.06261935920853856 with a P-value of P = 0.32408150009608494\n",
      "The Pearson Correlation Coefficient for 133 is -0.13909983476314886 with a P-value of P = 0.02787502602676708\n",
      "The Pearson Correlation Coefficient for 134 is -0.13373115136262884 with a P-value of P = 0.03456823380304194\n",
      "The Pearson Correlation Coefficient for 135 is -0.02713136236115589 with a P-value of P = 0.6694425243247502\n",
      "The Pearson Correlation Coefficient for 136 is -0.0027843866439153124 with a P-value of P = 0.9650602640756036\n",
      "The Pearson Correlation Coefficient for 137 is 0.07780617697565487 with a P-value of P = 0.22022967385992542\n",
      "The Pearson Correlation Coefficient for 138 is -0.05754548470622709 with a P-value of P = 0.3649013381049381\n",
      "The Pearson Correlation Coefficient for 139 is -0.03085520615018143 with a P-value of P = 0.6272973688062492\n",
      "The Pearson Correlation Coefficient for 140 is -0.023854339124909495 with a P-value of P = 0.7074125047837012\n",
      "The Pearson Correlation Coefficient for 141 is -0.061538912455174365 with a P-value of P = 0.33251619381835085\n",
      "The Pearson Correlation Coefficient for 142 is -0.0678669751125039 with a P-value of P = 0.285102893240053\n",
      "The Pearson Correlation Coefficient for 143 is -0.09476057062952684 with a P-value of P = 0.13513600956896046\n",
      "The Pearson Correlation Coefficient for 144 is -0.07767512210720533 with a P-value of P = 0.22101015601357088\n",
      "The Pearson Correlation Coefficient for 145 is 0.0721404788170908 with a P-value of P = 0.2557866735991262\n",
      "The Pearson Correlation Coefficient for 146 is -0.05049942710588996 with a P-value of P = 0.42662985895592354\n",
      "The Pearson Correlation Coefficient for 147 is -0.025572055683299094 with a P-value of P = 0.6874123341453939\n",
      "The Pearson Correlation Coefficient for 148 is -0.09160746052778665 with a P-value of P = 0.1486769268834366\n",
      "The Pearson Correlation Coefficient for 149 is -0.041600683129701006 with a P-value of P = 0.5126276612465774\n",
      "The Pearson Correlation Coefficient for 150 is -0.1304539975299056 with a P-value of P = 0.039289892767372096\n",
      "The Pearson Correlation Coefficient for 151 is -0.09840949801531473 with a P-value of P = 0.12067125222630858\n",
      "The Pearson Correlation Coefficient for 152 is 0.04423559871525506 with a P-value of P = 0.4862648045514327\n",
      "The Pearson Correlation Coefficient for 153 is -0.008602082133414011 with a P-value of P = 0.8923495907960527\n",
      "The Pearson Correlation Coefficient for 154 is 0.02960812114180609 with a P-value of P = 0.6412859828676634\n",
      "The Pearson Correlation Coefficient for 155 is -0.05422546838799454 with a P-value of P = 0.39326488829601225\n",
      "The Pearson Correlation Coefficient for 156 is -0.09756354304018186 with a P-value of P = 0.12391251459485964\n",
      "The Pearson Correlation Coefficient for 157 is 0.058889899575393215 with a P-value of P = 0.3537865249337381\n",
      "The Pearson Correlation Coefficient for 158 is -0.025581368389171896 with a P-value of P = 0.687304477214974\n",
      "The Pearson Correlation Coefficient for 159 is -0.01902927635532472 with a P-value of P = 0.7646359891988095\n",
      "The Pearson Correlation Coefficient for 160 is 0.04331168117604291 with a P-value of P = 0.4954250005267974\n",
      "The Pearson Correlation Coefficient for 161 is -0.036358833584265164 with a P-value of P = 0.5671920284799585\n",
      "The Pearson Correlation Coefficient for 162 is 0.04139386080197989 with a P-value of P = 0.5147278517359117\n",
      "The Pearson Correlation Coefficient for 163 is 0.06971193024822539 with a P-value of P = 0.2721805291902668\n",
      "The Pearson Correlation Coefficient for 164 is 0.12415134255033095 with a P-value of P = 0.049907053547646886\n",
      "The Pearson Correlation Coefficient for 165 is -0.14252568220665415 with a P-value of P = 0.02421170398851865\n",
      "The Pearson Correlation Coefficient for 166 is -0.06087832407622114 with a P-value of P = 0.3377419837107317\n",
      "The Pearson Correlation Coefficient for 167 is -0.033505333661929516 with a P-value of P = 0.5980137638656802\n",
      "The Pearson Correlation Coefficient for 168 is 0.0721140966852298 with a P-value of P = 0.2559610198289597\n",
      "The Pearson Correlation Coefficient for 169 is -0.006587675590698754 with a P-value of P = 0.9174555056135539\n",
      "The Pearson Correlation Coefficient for 170 is 0.05960847601809882 with a P-value of P = 0.34793393661761396\n",
      "The Pearson Correlation Coefficient for 171 is 0.025447390626457816 with a P-value of P = 0.6888567750529699\n",
      "The Pearson Correlation Coefficient for 172 is -0.0013750603121321899 with a P-value of P = 0.9827409778119743\n",
      "The Pearson Correlation Coefficient for 173 is -0.005157103643028765 with a P-value of P = 0.9353362922149533\n",
      "The Pearson Correlation Coefficient for 174 is 0.01778508342205003 with a P-value of P = 0.779615825121198\n",
      "The Pearson Correlation Coefficient for 175 is -0.020313038344137474 with a P-value of P = 0.7492710249165968\n",
      "The Pearson Correlation Coefficient for 176 is 0.09978963897903059 with a P-value of P = 0.11552530788434172\n",
      "The Pearson Correlation Coefficient for 177 is 0.0403910147037309 with a P-value of P = 0.5249739727760374\n",
      "The Pearson Correlation Coefficient for 178 is -0.009089344056602784 with a P-value of P = 0.8862918653691476\n",
      "The Pearson Correlation Coefficient for 179 is -0.07156339341456139 with a P-value of P = 0.25961908348526386\n",
      "The Pearson Correlation Coefficient for 180 is -0.09692598442715698 with a P-value of P = 0.1263996259342107\n",
      "The Pearson Correlation Coefficient for 181 is -0.025465025677434708 with a P-value of P = 0.6886523765991144\n",
      "The Pearson Correlation Coefficient for 182 is -0.07850032263761068 with a P-value of P = 0.2161285346668555\n",
      "The Pearson Correlation Coefficient for 183 is 0.16414552685546507 with a P-value of P = 0.009321804972261058\n",
      "The Pearson Correlation Coefficient for 184 is 0.009849180176368509 with a P-value of P = 0.8768587430883273\n",
      "The Pearson Correlation Coefficient for 185 is 0.014293007670748202 with a P-value of P = 0.8220794649471901\n",
      "The Pearson Correlation Coefficient for 186 is 0.05567168836616811 with a P-value of P = 0.3807499724270851\n",
      "The Pearson Correlation Coefficient for 187 is 0.05625880147032597 with a P-value of P = 0.375739539175192\n",
      "The Pearson Correlation Coefficient for 188 is 0.015292342194607062 with a P-value of P = 0.8098686581043137\n",
      "The Pearson Correlation Coefficient for 189 is -0.15595585665134087 with a P-value of P = 0.013562589911404044\n",
      "The Pearson Correlation Coefficient for 190 is 0.02429722132322806 with a P-value of P = 0.7022359228224195\n",
      "The Pearson Correlation Coefficient for 191 is -0.003449036233808001 with a P-value of P = 0.956727290891607\n",
      "The Pearson Correlation Coefficient for 192 is -0.05099393231166822 with a P-value of P = 0.42210966395335725\n",
      "The Pearson Correlation Coefficient for 193 is 0.08630180751825221 with a P-value of P = 0.17375017840242218\n",
      "The Pearson Correlation Coefficient for 194 is -0.15038406370575386 with a P-value of P = 0.017340298609720767\n",
      "The Pearson Correlation Coefficient for 195 is -0.0009094000472159425 with a P-value of P = 0.9885851987781539\n",
      "The Pearson Correlation Coefficient for 196 is -0.03660579646528904 with a P-value of P = 0.5645604594249902\n",
      "The Pearson Correlation Coefficient for 197 is -0.016515474774802096 with a P-value of P = 0.7949859357137847\n",
      "The Pearson Correlation Coefficient for 198 is -0.0811236988296908 with a P-value of P = 0.2011231921078372\n",
      "The Pearson Correlation Coefficient for 199 is 0.15944237498737707 with a P-value of P = 0.011584920711393528\n",
      "The Pearson Correlation Coefficient for 200 is 0.016541863286158958 with a P-value of P = 0.7946656428162713\n",
      "The Pearson Correlation Coefficient for 201 is 0.14223818126708662 with a P-value of P = 0.024502316536160595\n",
      "The Pearson Correlation Coefficient for 202 is -0.04505830561066022 with a P-value of P = 0.47818530989627384\n",
      "The Pearson Correlation Coefficient for 203 is 0.04826272202395775 with a P-value of P = 0.44742246463844226\n",
      "The Pearson Correlation Coefficient for 204 is 0.015753710383806508 with a P-value of P = 0.8042465211141322\n",
      "The Pearson Correlation Coefficient for 205 is 0.008821761915438377 with a P-value of P = 0.889617698858352\n",
      "The Pearson Correlation Coefficient for 206 is -0.030037005013464527 with a P-value of P = 0.6364605457474362\n",
      "The Pearson Correlation Coefficient for 207 is 0.0031398198011321123 with a P-value of P = 0.9606035293178692\n",
      "The Pearson Correlation Coefficient for 208 is 0.06499518326100276 with a P-value of P = 0.30602584914268577\n",
      "The Pearson Correlation Coefficient for 209 is -0.1273906903916033 with a P-value of P = 0.04418460620842962\n",
      "The Pearson Correlation Coefficient for 210 is -0.03280648227345726 with a P-value of P = 0.6056760018042278\n",
      "The Pearson Correlation Coefficient for 211 is -0.06569829245699603 with a P-value of P = 0.3008120261384418\n",
      "The Pearson Correlation Coefficient for 212 is -0.011207427632225224 with a P-value of P = 0.8600406196876337\n",
      "The Pearson Correlation Coefficient for 213 is -0.05324297828312539 with a P-value of P = 0.40190648372056814\n",
      "The Pearson Correlation Coefficient for 214 is 0.09706838299484916 with a P-value of P = 0.12584080612863752\n",
      "The Pearson Correlation Coefficient for 215 is 0.09095976288541224 with a P-value of P = 0.15158174954377085\n",
      "The Pearson Correlation Coefficient for 216 is -0.030695138561892717 with a P-value of P = 0.6290855620465723\n",
      "The Pearson Correlation Coefficient for 217 is -0.2072148301584481 with a P-value of P = 0.0009814931191729783\n",
      "The Pearson Correlation Coefficient for 218 is 0.017604560200817364 with a P-value of P = 0.7817962083037335\n",
      "The Pearson Correlation Coefficient for 219 is 0.011268111835842717 with a P-value of P = 0.8592906265852069\n",
      "The Pearson Correlation Coefficient for 220 is -0.13473843423111093 with a P-value of P = 0.03321748032365613\n",
      "The Pearson Correlation Coefficient for 221 is 0.0914553141702407 with a P-value of P = 0.1493554535514173\n",
      "The Pearson Correlation Coefficient for 222 is 0.02873716630960712 with a P-value of P = 0.6511315445065188\n",
      "The Pearson Correlation Coefficient for 223 is -0.005785867559009006 with a P-value of P = 0.9274728127722869\n",
      "The Pearson Correlation Coefficient for 224 is 0.0022477190625755073 with a P-value of P = 0.9717914877374734\n",
      "The Pearson Correlation Coefficient for 225 is 0.07148045829042342 with a P-value of P = 0.2601730844281418\n",
      "The Pearson Correlation Coefficient for 226 is 0.11366021785064863 with a P-value of P = 0.07282319197337182\n",
      "The Pearson Correlation Coefficient for 227 is -0.09926779077823117 with a P-value of P = 0.1174505115189448\n",
      "The Pearson Correlation Coefficient for 228 is -0.08602301326750199 with a P-value of P = 0.17514963042256146\n",
      "The Pearson Correlation Coefficient for 229 is -0.06722447101160563 with a P-value of P = 0.28969836339626087\n",
      "The Pearson Correlation Coefficient for 230 is -0.13626336576202486 with a P-value of P = 0.03125811460470337\n",
      "The Pearson Correlation Coefficient for 231 is -0.016178717014161414 with a P-value of P = 0.799076361517179\n",
      "The Pearson Correlation Coefficient for 232 is -0.015600636813347862 with a P-value of P = 0.8061107438877979\n",
      "The Pearson Correlation Coefficient for 233 is -0.022939029328416915 with a P-value of P = 0.7181534768019171\n",
      "The Pearson Correlation Coefficient for 234 is 0.0356489880930253 with a P-value of P = 0.5747882990126101\n",
      "The Pearson Correlation Coefficient for 235 is 0.06232815750568426 with a P-value of P = 0.32634106246084293\n",
      "The Pearson Correlation Coefficient for 236 is -0.00690567881289908 with a P-value of P = 0.9134860801239674\n",
      "The Pearson Correlation Coefficient for 237 is -0.13316786263628247 with a P-value of P = 0.03534372778973646\n",
      "The Pearson Correlation Coefficient for 238 is -0.009582590543231895 with a P-value of P = 0.8801664591336833\n",
      "The Pearson Correlation Coefficient for 239 is -0.11816625036876627 with a P-value of P = 0.06210431619954106\n",
      "The Pearson Correlation Coefficient for 240 is 0.09666771700424188 with a P-value of P = 0.1274180522195449\n",
      "The Pearson Correlation Coefficient for 241 is 0.07901708046851924 with a P-value of P = 0.213111125484346\n",
      "The Pearson Correlation Coefficient for 242 is -0.030501630705873455 with a P-value of P = 0.6312502223175758\n",
      "The Pearson Correlation Coefficient for 243 is -0.07361802956402284 with a P-value of P = 0.24615298663060634\n",
      "The Pearson Correlation Coefficient for 244 is 0.10814694605279851 with a P-value of P = 0.08793511054501378\n",
      "The Pearson Correlation Coefficient for 245 is -0.06693180743394411 with a P-value of P = 0.29180796485184013\n",
      "The Pearson Correlation Coefficient for 246 is 0.0883137994303436 with a P-value of P = 0.16389596849114055\n",
      "The Pearson Correlation Coefficient for 247 is 0.019638313817989344 with a P-value of P = 0.7573347028394687\n",
      "The Pearson Correlation Coefficient for 248 is 0.010336290340795093 with a P-value of P = 0.870820471615958\n",
      "The Pearson Correlation Coefficient for 249 is 0.016276068477485563 with a P-value of P = 0.7978933140496456\n",
      "The Pearson Correlation Coefficient for 250 is -0.05768825554672515 with a P-value of P = 0.3637108092158604\n",
      "The Pearson Correlation Coefficient for 251 is -0.07158256119714831 with a P-value of P = 0.2594911596023366\n",
      "The Pearson Correlation Coefficient for 252 is -0.12233479987697017 with a P-value of P = 0.053377843948291974\n",
      "The Pearson Correlation Coefficient for 253 is 0.08423544450180416 with a P-value of P = 0.1843217863763259\n",
      "The Pearson Correlation Coefficient for 254 is -0.003731847700635639 with a P-value of P = 0.9531829447199468\n",
      "The Pearson Correlation Coefficient for 255 is 0.04792256328182852 with a P-value of P = 0.4506340191037562\n",
      "The Pearson Correlation Coefficient for 256 is 0.01592931246104657 with a P-value of P = 0.8021092950829352\n",
      "The Pearson Correlation Coefficient for 257 is 0.012125756892529028 with a P-value of P = 0.848704832527276\n",
      "The Pearson Correlation Coefficient for 258 is -0.13833236919238093 with a P-value of P = 0.02875789708980129\n",
      "The Pearson Correlation Coefficient for 259 is 0.0380192347643907 with a P-value of P = 0.549612663953118\n",
      "The Pearson Correlation Coefficient for 260 is -0.04610668834275164 with a P-value of P = 0.4679961206511706\n",
      "The Pearson Correlation Coefficient for 261 is -0.05788505564074637 with a P-value of P = 0.3620737079493279\n",
      "The Pearson Correlation Coefficient for 262 is -0.07772551348593998 with a P-value of P = 0.2207098230839306\n",
      "The Pearson Correlation Coefficient for 263 is 0.01863776729645603 with a P-value of P = 0.769340542695599\n",
      "The Pearson Correlation Coefficient for 264 is 0.03651305437952315 with a P-value of P = 0.5655480088407538\n",
      "The Pearson Correlation Coefficient for 265 is 0.008512651477087619 with a P-value of P = 0.8934620974274698\n",
      "The Pearson Correlation Coefficient for 266 is 0.017778431289354576 with a P-value of P = 0.7796961399954901\n",
      "The Pearson Correlation Coefficient for 267 is 0.06998858348617529 with a P-value of P = 0.2702777192176207\n",
      "The Pearson Correlation Coefficient for 268 is -0.08261606763298411 with a P-value of P = 0.1929317516981718\n",
      "The Pearson Correlation Coefficient for 269 is -0.009661074654223303 with a P-value of P = 0.8791924474805297\n",
      "The Pearson Correlation Coefficient for 270 is -0.03586963690415713 with a P-value of P = 0.5724219519565427\n",
      "The Pearson Correlation Coefficient for 271 is 0.016825743981550217 with a P-value of P = 0.7912222018833627\n",
      "The Pearson Correlation Coefficient for 272 is 0.11390925897955151 with a P-value of P = 0.07219386005697083\n",
      "The Pearson Correlation Coefficient for 273 is 0.003967059106330639 with a P-value of P = 0.9502358461235196\n",
      "The Pearson Correlation Coefficient for 274 is 0.008407589319125396 with a P-value of P = 0.8947693241032295\n",
      "The Pearson Correlation Coefficient for 275 is -0.07091645627652338 with a P-value of P = 0.2639621463452824\n",
      "The Pearson Correlation Coefficient for 276 is -0.11746708522301456 with a P-value of P = 0.0636769696576678\n",
      "The Pearson Correlation Coefficient for 277 is -0.033894250752875145 with a P-value of P = 0.5937686985017621\n",
      "The Pearson Correlation Coefficient for 278 is 0.014398328601424744 with a P-value of P = 0.820790478821975\n",
      "The Pearson Correlation Coefficient for 279 is -0.07132987063172566 with a P-value of P = 0.2611810772568535\n",
      "The Pearson Correlation Coefficient for 280 is 0.0788113214141001 with a P-value of P = 0.2143089347552362\n",
      "The Pearson Correlation Coefficient for 281 is -0.08850041632097398 with a P-value of P = 0.16300362503857083\n",
      "The Pearson Correlation Coefficient for 282 is 0.04453038016538137 with a P-value of P = 0.483361461122253\n",
      "The Pearson Correlation Coefficient for 283 is 0.07805015574781986 with a P-value of P = 0.21878192529434998\n",
      "The Pearson Correlation Coefficient for 284 is -0.03345110940403413 with a P-value of P = 0.5986067130265846\n",
      "The Pearson Correlation Coefficient for 285 is 0.08776242914964655 with a P-value of P = 0.16655382008556072\n",
      "The Pearson Correlation Coefficient for 286 is -0.04583317987655343 with a P-value of P = 0.4706427555069329\n",
      "The Pearson Correlation Coefficient for 287 is -0.0667051319968308 with a P-value of P = 0.2934489393667395\n",
      "The Pearson Correlation Coefficient for 288 is -0.07292257217295348 with a P-value of P = 0.2506554383865107\n",
      "The Pearson Correlation Coefficient for 289 is 0.127212936940077 with a P-value of P = 0.04448367625099053\n",
      "The Pearson Correlation Coefficient for 290 is 0.03967473975504189 with a P-value of P = 0.5323551917910465\n",
      "The Pearson Correlation Coefficient for 291 is 0.05569430909332332 with a P-value of P = 0.3805561745479159\n",
      "The Pearson Correlation Coefficient for 292 is -0.08892992625614994 with a P-value of P = 0.16096368625059923\n",
      "The Pearson Correlation Coefficient for 293 is -0.03436294470861534 with a P-value of P = 0.5886711540161715\n",
      "The Pearson Correlation Coefficient for 294 is -0.03196413321921434 with a P-value of P = 0.6149691525108265\n",
      "The Pearson Correlation Coefficient for 295 is -0.17050133609223841 with a P-value of P = 0.0068889218720628925\n",
      "The Pearson Correlation Coefficient for 296 is 0.007433804289440014 with a P-value of P = 0.9068986206917549\n",
      "The Pearson Correlation Coefficient for 297 is 0.05681035394712105 with a P-value of P = 0.37106963336436616\n",
      "The Pearson Correlation Coefficient for 298 is -0.13475990289697978 with a P-value of P = 0.03318918682296229\n",
      "The Pearson Correlation Coefficient for 299 is -0.07547465326180583 with a P-value of P = 0.2344100623111424\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>pearson_coeff</th>\n",
       "      <th>p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.108966</td>\n",
       "      <td>0.085543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.073319</td>\n",
       "      <td>0.248080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.015141</td>\n",
       "      <td>0.811716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.011549</td>\n",
       "      <td>0.855821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.114726</td>\n",
       "      <td>0.070161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  column_name  pearson_coeff   p_value\n",
       "0           0       0.108966  0.085543\n",
       "1           1      -0.073319  0.248080\n",
       "2           2      -0.015141  0.811716\n",
       "3           3       0.011549  0.855821\n",
       "4           4      -0.114726  0.070161"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearson_coeff = []\n",
    "p_value = []\n",
    "col_name = []\n",
    "\n",
    "for col in [i for i in train_data.columns.tolist() if i not in ['id','target']]:\n",
    "    \n",
    "    x,y = calculateCorrelationCoefficientsAndpValues(train_data[col], train_data['target'], col)\n",
    "    pearson_coeff.append(x)\n",
    "    p_value.append(y)\n",
    "    col_name.append(col)\n",
    "    \n",
    "pearson_table = panda.DataFrame({'column_name':col_name , 'pearson_coeff':pearson_coeff, 'p_value': p_value})\n",
    "pearson_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>pearson_coeff</th>\n",
       "      <th>p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>0.373608</td>\n",
       "      <td>1.060808e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>65</td>\n",
       "      <td>0.293846</td>\n",
       "      <td>2.271695e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.173096</td>\n",
       "      <td>6.071028e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>183</td>\n",
       "      <td>0.164146</td>\n",
       "      <td>9.321805e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>199</td>\n",
       "      <td>0.159442</td>\n",
       "      <td>1.158492e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>201</td>\n",
       "      <td>0.142238</td>\n",
       "      <td>2.450232e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>0.132705</td>\n",
       "      <td>3.599201e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>289</td>\n",
       "      <td>0.127213</td>\n",
       "      <td>4.448368e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>114</td>\n",
       "      <td>0.124792</td>\n",
       "      <td>4.872907e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>164</td>\n",
       "      <td>0.124151</td>\n",
       "      <td>4.990705e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>101</td>\n",
       "      <td>0.118379</td>\n",
       "      <td>6.163324e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>272</td>\n",
       "      <td>0.113909</td>\n",
       "      <td>7.219386e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>226</td>\n",
       "      <td>0.113660</td>\n",
       "      <td>7.282319e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.110998</td>\n",
       "      <td>7.983294e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>105</td>\n",
       "      <td>0.110589</td>\n",
       "      <td>8.095715e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.108966</td>\n",
       "      <td>8.554251e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>244</td>\n",
       "      <td>0.108147</td>\n",
       "      <td>8.793511e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.107828</td>\n",
       "      <td>8.888181e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>127</td>\n",
       "      <td>-0.101558</td>\n",
       "      <td>1.091850e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>-0.101702</td>\n",
       "      <td>1.086815e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>108</td>\n",
       "      <td>-0.113875</td>\n",
       "      <td>7.228138e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.114726</td>\n",
       "      <td>7.016103e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>276</td>\n",
       "      <td>-0.117467</td>\n",
       "      <td>6.367697e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>239</td>\n",
       "      <td>-0.118166</td>\n",
       "      <td>6.210432e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>-0.121413</td>\n",
       "      <td>5.521489e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>252</td>\n",
       "      <td>-0.122335</td>\n",
       "      <td>5.337784e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>209</td>\n",
       "      <td>-0.127391</td>\n",
       "      <td>4.418461e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>63</td>\n",
       "      <td>-0.127440</td>\n",
       "      <td>4.410268e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>82</td>\n",
       "      <td>-0.128248</td>\n",
       "      <td>4.276594e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>150</td>\n",
       "      <td>-0.130454</td>\n",
       "      <td>3.928989e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>237</td>\n",
       "      <td>-0.133168</td>\n",
       "      <td>3.534373e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>134</td>\n",
       "      <td>-0.133731</td>\n",
       "      <td>3.456823e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>220</td>\n",
       "      <td>-0.134738</td>\n",
       "      <td>3.321748e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>298</td>\n",
       "      <td>-0.134760</td>\n",
       "      <td>3.318919e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>90</td>\n",
       "      <td>-0.136230</td>\n",
       "      <td>3.130031e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>230</td>\n",
       "      <td>-0.136263</td>\n",
       "      <td>3.125811e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>258</td>\n",
       "      <td>-0.138332</td>\n",
       "      <td>2.875790e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>133</td>\n",
       "      <td>-0.139100</td>\n",
       "      <td>2.787503e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>129</td>\n",
       "      <td>-0.139439</td>\n",
       "      <td>2.749274e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>165</td>\n",
       "      <td>-0.142526</td>\n",
       "      <td>2.421170e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>-0.142915</td>\n",
       "      <td>2.382252e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>-0.144267</td>\n",
       "      <td>2.251444e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>194</td>\n",
       "      <td>-0.150384</td>\n",
       "      <td>1.734030e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>189</td>\n",
       "      <td>-0.155956</td>\n",
       "      <td>1.356259e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>80</td>\n",
       "      <td>-0.162558</td>\n",
       "      <td>1.003763e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>73</td>\n",
       "      <td>-0.167557</td>\n",
       "      <td>7.934978e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>295</td>\n",
       "      <td>-0.170501</td>\n",
       "      <td>6.888922e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>91</td>\n",
       "      <td>-0.192536</td>\n",
       "      <td>2.230678e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>117</td>\n",
       "      <td>-0.197496</td>\n",
       "      <td>1.701034e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>217</td>\n",
       "      <td>-0.207215</td>\n",
       "      <td>9.814931e-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    column_name  pearson_coeff       p_value\n",
       "33           33       0.373608  1.060808e-09\n",
       "65           65       0.293846  2.271695e-06\n",
       "24           24       0.173096  6.071028e-03\n",
       "183         183       0.164146  9.321805e-03\n",
       "199         199       0.159442  1.158492e-02\n",
       "201         201       0.142238  2.450232e-02\n",
       "30           30       0.132705  3.599201e-02\n",
       "289         289       0.127213  4.448368e-02\n",
       "114         114       0.124792  4.872907e-02\n",
       "164         164       0.124151  4.990705e-02\n",
       "101         101       0.118379  6.163324e-02\n",
       "272         272       0.113909  7.219386e-02\n",
       "226         226       0.113660  7.282319e-02\n",
       "17           17       0.110998  7.983294e-02\n",
       "105         105       0.110589  8.095715e-02\n",
       "0             0       0.108966  8.554251e-02\n",
       "244         244       0.108147  8.793511e-02\n",
       "13           13       0.107828  8.888181e-02\n",
       "127         127      -0.101558  1.091850e-01\n",
       "98           98      -0.101702  1.086815e-01\n",
       "108         108      -0.113875  7.228138e-02\n",
       "4             4      -0.114726  7.016103e-02\n",
       "276         276      -0.117467  6.367697e-02\n",
       "239         239      -0.118166  6.210432e-02\n",
       "43           43      -0.121413  5.521489e-02\n",
       "252         252      -0.122335  5.337784e-02\n",
       "209         209      -0.127391  4.418461e-02\n",
       "63           63      -0.127440  4.410268e-02\n",
       "82           82      -0.128248  4.276594e-02\n",
       "150         150      -0.130454  3.928989e-02\n",
       "237         237      -0.133168  3.534373e-02\n",
       "134         134      -0.133731  3.456823e-02\n",
       "220         220      -0.134738  3.321748e-02\n",
       "298         298      -0.134760  3.318919e-02\n",
       "90           90      -0.136230  3.130031e-02\n",
       "230         230      -0.136263  3.125811e-02\n",
       "258         258      -0.138332  2.875790e-02\n",
       "133         133      -0.139100  2.787503e-02\n",
       "129         129      -0.139439  2.749274e-02\n",
       "165         165      -0.142526  2.421170e-02\n",
       "39           39      -0.142915  2.382252e-02\n",
       "16           16      -0.144267  2.251444e-02\n",
       "194         194      -0.150384  1.734030e-02\n",
       "189         189      -0.155956  1.356259e-02\n",
       "80           80      -0.162558  1.003763e-02\n",
       "73           73      -0.167557  7.934978e-03\n",
       "295         295      -0.170501  6.888922e-03\n",
       "91           91      -0.192536  2.230678e-03\n",
       "117         117      -0.197496  1.701034e-03\n",
       "217         217      -0.207215  9.814931e-04"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearson_table[(pearson_table.pearson_coeff>0.1) | (pearson_table.pearson_coeff<-0.1)].sort_values(by=['pearson_coeff'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['33', '65', '24', '183', '199'], 50)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reqd_columns = pearson_table[(pearson_table.pearson_coeff>0.1) | (pearson_table.pearson_coeff<-0.1)].sort_values(by=['pearson_coeff'], ascending=False).column_name.values.tolist()\n",
    "reqd_columns[:5], len(reqd_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of observations that we can make out of the above diagrams:\n",
    "\n",
    "1. ALmost all independent variables are distributed within -2.5 and 2.5\n",
    "2. Almost all have near normal distributions\n",
    "3. Almost none have any significant outliers\n",
    "4. Pearsons correlation coeff for each indedepent variable with target variable is pretty low\n",
    "5. There are no categorical data\n",
    "6. pearsons correlation coefficient scores are pretty less and we will take all whose scores are above 0.1 and below -0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on above data analysis, we are going to attempt 3 things\n",
    "\n",
    "1. create a baseline model out of gridsearch for the data using the usual suspects of classifiers. \n",
    "\n",
    "\n",
    "2. Plot learning curve and auc/roc scores for each\n",
    "\n",
    "\n",
    "3. try dimensionality reduction techniques such as PCA,LDA and t-SNE and run on baseline models created in step 1.\n",
    "\n",
    "\n",
    "4. try SMOTE and run on the same model created in point 1 and compare scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: Run handful of usual suspect classifiers using selected 50 features giving highest coeff scores\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeTimer:\n",
    "    \n",
    "    \"\"\"\n",
    "        Utility custom contextual class for calculating the time \n",
    "        taken for a certain code block to execute\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, name=None):\n",
    "        self.name = \" '\"  + name + \"'\" if name else ''\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = time.clock()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.took = (time.clock() - self.start) * 1000.0\n",
    "        time_taken = datetime.timedelta(milliseconds = self.took)\n",
    "        print('Code block' + self.name + ' took(HH:MM:SS): ' + str(time_taken))\n",
    "        \n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plot.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plot.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plot.title(title)\n",
    "    plot.colorbar()\n",
    "    tick_marks = arange(len(classes))\n",
    "    plot.xticks(tick_marks, classes, rotation=45)\n",
    "    plot.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plot.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plot.ylabel('True label')\n",
    "    plot.xlabel('Predicted label')\n",
    "#     plot.tight_layout()\n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plotLearningCurve(_x_train, _y_train, learning_model_pipeline,  model_name, k_fold = 10, training_sample_sizes = linspace(0.1,1.0,10), jobsInParallel = 1):\n",
    "    \n",
    "    training_size, training_score, testing_score = learning_curve(estimator = learning_model_pipeline, \\\n",
    "                                                                X = _x_train, \\\n",
    "                                                                y = _y_train, \\\n",
    "                                                                train_sizes = training_sample_sizes, \\\n",
    "                                                                cv = k_fold, \\\n",
    "                                                                n_jobs = jobsInParallel) \n",
    "\n",
    "\n",
    "    training_mean = mean(training_score, axis = 1)\n",
    "    training_std_deviation = std(training_score, axis = 1)\n",
    "    testing_std_deviation = std(testing_score, axis = 1)\n",
    "    testing_mean = mean(testing_score, axis = 1 )\n",
    "\n",
    "    ## we have got the estimator in this case the perceptron running in 10 fold validation with \n",
    "    ## equal division of sizes betwwen .1 and 1. After execution, we get the number of training sizes used, \n",
    "    ## the training scores for those sizes and the test scores for those sizes. we will plot a scatter plot \n",
    "    ## to see the accuracy results and check for bias vs variance\n",
    "\n",
    "    # training_size : essentially 10 sets of say a1, a2, a3,,...a10 sizes (this comes from train_size parameter, here we have given linespace for equal distribution betwwen 0.1 and 1 for 10 such values)\n",
    "    # training_score : training score for the a1 samples, a2 samples...a10 samples, each samples run 10 times since cv value is 10\n",
    "    # testing_score : testing score for the a1 samples, a2 samples...a10 samples, each samples run 10 times since cv value is 10\n",
    "    ## the mean and std deviation for each are calculated simply to show ranges in the graph\n",
    "\n",
    "    plot.plot(training_size, training_mean, label= \"Training Data\", marker= '+', color = 'blue', markersize = 8)\n",
    "    plot.fill_between(training_size, training_mean+ training_std_deviation, training_mean-training_std_deviation, color='blue', alpha =0.12 )\n",
    "\n",
    "    plot.plot(training_size, testing_mean, label= \"Testing/Validation Data\", marker= '*', color = 'green', markersize = 8)\n",
    "    plot.fill_between(training_size, testing_mean+ training_std_deviation, testing_mean-training_std_deviation, color='green', alpha =0.14 )\n",
    "\n",
    "    plot.title(\"Scoring of our training and testing data vs sample sizes for model:\"+model_name)\n",
    "    plot.xlabel(\"Number of Samples\")\n",
    "    plot.ylabel(\"Accuracy\")\n",
    "    plot.legend(loc= 'best')\n",
    "    plot.show()\n",
    "    \n",
    "def plot_roc_auc_curve(false_positive_rate, true_positive_rate, model_name):\n",
    "        \n",
    "    plot.figure(figsize=(10,3))\n",
    "    plot.plot(list(false_positive_rate), list(true_positive_rate),  label = \"ROC Curve for model: \"+model_name)     \n",
    "    plot.plot([0, 1], [0, 1], 'k--', label = 'Random Guessing')\n",
    "    plot.plot([0, 0, 1], [0,1, 1], ':', label = 'Perfect Score')\n",
    "    auc_score = auc(false_positive_rate, true_positive_rate)\n",
    "    plot.title('ROC Curve for model: %s with AUC %.2f'%(model_name, auc_score))\n",
    "    plot.xlabel('False Positive Rate')\n",
    "    plot.ylabel('True Positive Rate')\n",
    "    plot.legend(loc='best')\n",
    "    plot.show()\n",
    "    \n",
    "    \n",
    "def plot_precision_recall_curve(precision, recall, model_name):\n",
    "    \n",
    "    plot.figure(figsize=(10,3))\n",
    "    plot.plot(list(recall), list(precision),  label = \"Precision/Recall Curve for model: \"+model_name)     \n",
    "#     plot.plot([0, 1], [0, 1], 'k--', label = 'Random Guessing') #\n",
    "    plot.title('Precision Recall Curve for model: %s'%model_name)\n",
    "    plot.xlabel('Recall')\n",
    "    plot.ylabel('Precision')\n",
    "    plot.legend(loc='best')\n",
    "    plot.show()\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runGridSearchAndPredict(pipeline,model_name, x_train, y_train, x_test, y_test, param_grid, n_jobs = 1, cv = 10, score = 'accuracy'):\n",
    "#     pass\n",
    "\n",
    "    response =  {}\n",
    "    training_timer       = CodeTimer('training')\n",
    "    testing_timer        = CodeTimer('testing')\n",
    "    learning_curve_timer = CodeTimer('learning_curve')\n",
    "    predict_proba_timer  = CodeTimer('predict_proba')\n",
    "    \n",
    "    with training_timer:\n",
    "        \n",
    "        gridsearch = GridSearchCV(estimator = pipeline, param_grid = param_grid, cv = cv, n_jobs = n_jobs, scoring = score)\n",
    "\n",
    "        search = gridsearch.fit(x_train,y_train)\n",
    "\n",
    "        print(\"Grid Search Best parameters \", search.best_params_)\n",
    "        print(\"Grid Search Best score \", search.best_score_)\n",
    "\n",
    "    with testing_timer:\n",
    "        y_prediction = gridsearch.predict(x_test)\n",
    "            \n",
    "    print(\"F1 score %s\" %f1_score(y_test,y_prediction, average ='weighted'))\n",
    "    print(\"Classification report  \\n %s\" %(classification_report(y_test, y_prediction)))\n",
    "    \n",
    "    with learning_curve_timer:\n",
    "        plotLearningCurve(x_train, y_train, search.best_estimator_, model_name)\n",
    "#         _matrix = confusion_matrix(y_true = _y_test ,y_pred = y_prediction, labels = list(range(_y_test.shape[1])))\n",
    "        _matrix = confusion_matrix(y_true = y_test ,y_pred = y_prediction, labels = list(set(y_test)))\n",
    "        classes = list(set(y_test))\n",
    "        plot_confusion_matrix(_matrix, classes, title = \"Confusion matrix for model:\"+model_name)\n",
    "        \n",
    "    with predict_proba_timer:\n",
    "\n",
    "        if hasattr(gridsearch.best_estimator_, 'predict_proba'):\n",
    "            \n",
    "            print('inside decision function')\n",
    "            y_probability = gridsearch.predict_proba(x_test)\n",
    "            number_of_classes = len(np.unique(y_train))\n",
    "            false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_probability[:, 1])\n",
    "            response['roc_auc_score'] = roc_auc_score(y_test, y_probability[:,1])\n",
    "            response['roc_curve'] = (false_positive_rate, true_positive_rate)  \n",
    "            response['roc_curve_false_positive_rate'] = false_positive_rate\n",
    "            response['roc_curve_true_positive_rate'] = true_positive_rate\n",
    "            precision, recall, _ = precision_recall_curve(y_test, y_probability[:,1])\n",
    "            plot_roc_auc_curve(false_positive_rate, true_positive_rate, model_name)\n",
    "            plot_precision_recall_curve(precision, recall, model_name)\n",
    "            \n",
    "        else: ## eg SVM, Perceptron doesnt have predict_proba method\n",
    "            \n",
    "            response['roc_auc_score'] = 0\n",
    "            response['roc_curve'] = 0\n",
    "            response['roc_curve_false_positive_rate'] = 0\n",
    "            response['roc_curve_true_positive_rate'] = 0\n",
    "    \n",
    "    response['learning_curve_time'] = learning_curve_timer.took\n",
    "    response['testing_time'] = testing_timer.took\n",
    "    response['_y_prediction'] = y_prediction\n",
    "#     response['accuracy_score'] = accuracy_score(y_test,y_prediction)\n",
    "    response['training_time'] = training_timer.took\n",
    "    response['f1_score']  = f1_score(y_test, y_prediction, average ='weighted')\n",
    "    response['f1_score_micro']  = f1_score(y_test, y_prediction, average ='micro')\n",
    "    response['f1_score_macro']  = f1_score(y_test, y_prediction, average ='macro')\n",
    "    response['best_estimator'] = search.best_estimator_\n",
    "    response['confusion_matrix'] = _matrix\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "def plotROCCurveAcrossModels(positive_rates_sequence, model_name):\n",
    "    \n",
    "    plot.figure(figsize=(10,5))\n",
    "    for plot_values, label_name in zip(positive_rates_sequence, model_name):\n",
    "        \n",
    "        plot.plot(list(plot_values[0]), list(plot_values[1]),  label = \"ROC Curve for model: \"+label_name)\n",
    "        \n",
    "    plot.plot([0, 1], [0, 1], 'k--', label = 'Random Guessing') #\n",
    "    plot.title('ROC Curve across models')\n",
    "    plot.xlabel('False Positive Rate')\n",
    "    plot.ylabel('True Positive Rate')\n",
    "    plot.legend(loc='best')\n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def execute( _x_train,\n",
    "             _y_train,\n",
    "             _x_test,\n",
    "             _y_test, \n",
    "            classifiers, \n",
    "            classifier_names, \n",
    "            classifier_param_grid,\n",
    "            cv  = 10 , \n",
    "            score = 'accuracy',\n",
    "            scaler = StandardScaler()\n",
    "           ):\n",
    "    \n",
    "    '''\n",
    "    This method will run your data sets against the model specified \n",
    "    Models will be fed through a pipeline where the first step would be to\n",
    "    execute a scaling operation.\n",
    "    \n",
    "    Method will also call additional lower level methods in order to plot\n",
    "    precision curve, roc curve, learning curve and will also prepare a confusion matrix\n",
    "    \n",
    "    :returns: dict containing execution metrics such as time taken, accuracy scores\n",
    "    :returntype: dict\n",
    "    \n",
    "    '''\n",
    "\n",
    "    timer = CodeTimer(name='overalltime')\n",
    "    model_metrics = {}\n",
    "\n",
    "    with timer:\n",
    "        for model, model_name, model_param_grid in zip(classifiers, classifier_names, classifier_param_grid):\n",
    "\n",
    "            pipeline_steps = [('scaler', scaler),(model_name, model)] if scaler is not None else [(model_name, model)]\n",
    "            pipeline = Pipeline(pipeline_steps)\n",
    "\n",
    "            result = runGridSearchAndPredict(pipeline, \n",
    "                                             model_name,\n",
    "                                             _x_train,\n",
    "                                             _y_train,\n",
    "                                             _x_test,\n",
    "                                             _y_test, \n",
    "                                             model_param_grid ,\n",
    "                                             cv = cv,\n",
    "                                             score = score)\n",
    "\n",
    "            _y_prediction = result['_y_prediction']\n",
    "\n",
    "            model_metrics[model_name] = {}\n",
    "            model_metrics[model_name]['confusion_matrix'] = result.get('confusion_matrix')\n",
    "            model_metrics[model_name]['training_time'] = result.get('training_time')\n",
    "            model_metrics[model_name]['testing_time'] = result.get('testing_time')\n",
    "            model_metrics[model_name]['learning_curve_time'] = result.get('learning_curve_time')\n",
    "            model_metrics[model_name]['f1_score'] = result.get('f1_score')\n",
    "            model_metrics[model_name]['f1_score_macro'] = result.get('f1_score_macro')\n",
    "            model_metrics[model_name]['f1_score_micro'] = result.get('f1_score_micro')\n",
    "            model_metrics[model_name]['roc_auc_score'] = result.get('roc_auc_score')\n",
    "            model_metrics[model_name]['roc_curve_true_positive_rate'] = result.get('roc_curve_true_positive_rate')\n",
    "            model_metrics[model_name]['roc_curve_false_positive_rate'] = result.get('roc_curve_false_positive_rate')\n",
    "\n",
    "            model_metrics[model_name]['best_estimator'] = result.get('best_estimator')\n",
    "\n",
    "\n",
    "    print(timer.took)\n",
    "    \n",
    "    return model_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifiers = [\n",
    "    Perceptron(random_state = 1),\n",
    "    LogisticRegression(random_state = 1),\n",
    "    LogisticRegression(random_state = 1, solver='liblinear'),\n",
    "    LogisticRegression(random_state = 1, solver='newton-cg'),\n",
    "    LogisticRegression(random_state = 1, solver='sag'),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(random_state = 1),\n",
    "    KNeighborsClassifier(metric = 'minkowski'),\n",
    "    RidgeClassifier(random_state = 123), \n",
    "    SVC(kernel=\"linear\"),\n",
    "    SVC(),\n",
    "    ExtraTreeClassifier(random_state = 123),\n",
    "    GaussianProcessClassifier(random_state = 123),\n",
    "    BernoulliNB(),\n",
    "    BaggingClassifier(base_estimator = LogisticRegression(random_state = 1)),\n",
    "    BaggingClassifier(base_estimator = BernoulliNB()),\n",
    "    GradientBoostingClassifier(random_state= 123),\n",
    "    LGBMClassifier(objective = 'binary'),\n",
    "    XGBClassifier(objective = 'binary:logistic')\n",
    "]\n",
    "\n",
    "\n",
    "classifier_names = [\n",
    "            'perceptron',\n",
    "            'logisticregression',\n",
    "            'logisticregression_liblinear_l2',\n",
    "            'logisticregression_newton_cg',\n",
    "            'logisticregression_sag',\n",
    "            'decisiontreeclassifier',\n",
    "            'randomforestclassifier',\n",
    "            'kneighborsclassifier',\n",
    "            'ridge',\n",
    "            'linear_svc',\n",
    "            'gamma_svc',\n",
    "            'extra_trees',\n",
    "            'gaussian_process',\n",
    "            'bernoulli',\n",
    "            'bagging_logistic',\n",
    "            'bagging_bernoulli',\n",
    "            'gradient_boosting_classifier',\n",
    "            'lgbm_classifier',\n",
    "            'xgb'\n",
    "]\n",
    "\n",
    "classifier_param_grid = [\n",
    "            \n",
    "            {'perceptron__max_iter': [5,10,30], 'perceptron__eta0': [.1]},\n",
    "            {\n",
    "             'logisticregression__C':[1.2,0.02,2.2,4, 0.01, 0.05], \n",
    "             'logisticregression__penalty':['l1','l2'],\n",
    "             'logisticregression__solver':['saga','liblinear']\n",
    "            },\n",
    "            {\n",
    "             'logisticregression_liblinear_l2__C':[1.2,0.02,2.2,4, 0.01, 0.05], \n",
    "             'logisticregression_liblinear_l2__penalty':['l2'],\n",
    "             'logisticregression_liblinear_l2__dual':[True]\n",
    "            },\n",
    "            {\n",
    "             'logisticregression_newton_cg__C':[1.2,0.02,2.2,4, 0.01, 0.05], \n",
    "             'logisticregression_newton_cg__penalty':['l2'],\n",
    "            },\n",
    "            {\n",
    "             'logisticregression_sag__C':[1.2,0.02,2.2,4, 0.01, 0.05], \n",
    "             'logisticregression_sag__penalty':['l2'],\n",
    "            },\n",
    "    \n",
    "            {'decisiontreeclassifier__max_depth':[6,8,10],\n",
    "             'decisiontreeclassifier__criterion':['gini','entropy'],\n",
    "             'decisiontreeclassifier__max_features':['auto','sqrt','log2'],\n",
    "            },\n",
    "            {'randomforestclassifier__n_estimators':[6,8,12],'randomforestclassifier__criterion': ['gini','entropy']} ,\n",
    "            {'kneighborsclassifier__n_neighbors':[4,6,10]},\n",
    "            {'ridge__alpha':[1,1.2,0.9],'ridge__max_iter':[100,300,500]},\n",
    "            {'linear_svc__C':[0.025]},\n",
    "            {'gamma_svc__gamma':[2,4],'gamma_svc__C':[1,5]},\n",
    "            {'extra_trees__max_depth':[6,8,12],'extra_trees__criterion': ['gini','entropy']} ,\n",
    "            {'gaussian_process__max_iter_predict':[200,400]} ,\n",
    "            {'bernoulli__alpha':[0.2,0.6,1.2]} ,\n",
    "            {'bagging_logistic__base_estimator__C':[1.2,0.02,2.2,4], \n",
    "             'bagging_logistic__base_estimator__penalty':['l1','l2'],\n",
    "             'bagging_logistic__n_estimators': [5,8,10]\n",
    "            },\n",
    "            {'bagging_bernoulli__base_estimator__alpha':[1.2,0.02,2.2,4], \n",
    "             'bagging_bernoulli__n_estimators': [5,8,10]\n",
    "            },\n",
    "            {\n",
    "                'gradient_boosting_classifier__loss':['deviance','exponential'],\n",
    "                'gradient_boosting_classifier__learning_rate':[0.5,1.2],\n",
    "                'gradient_boosting_classifier__n_estimators':[100,500,1000],\n",
    "                'gradient_boosting_classifier__criterion':['friedman_mse','mse','mae'],\n",
    "                'gradient_boosting_classifier__max_depth':[6,8,16,20],\n",
    "            },\n",
    "            {\n",
    "                 'lgbm_classifier__num_leaves':[25,], \\\n",
    "#                  'lgbm_classifier__min_data_in_leaf':[20],\\\n",
    "                 'lgbm_classifier__max_depth':[20,], \\\n",
    "                 'lgbm_classifier__learning_rate' : [0.01,],\\\n",
    "                 'lgbm_classifier__min_child_samples' :[2,], \\\n",
    "                 'lgbm_classifier__n_estimators' : [5000,], \\\n",
    "                 'lgbm_classifier__num_boost_round' : [100], \\\n",
    "                 'lgbm_classifier__feature_fraction' : [0.9,], \\\n",
    "                 'lgbm_classifier__bagging_freq' : [1,], \\\n",
    "                 'lgbm_classifier__bagging_seed' : [123], \\\n",
    "            },\n",
    "             {\n",
    "                'xgb__max_depth':[6,8,10],\n",
    "                 'xgb__learning_rate':[0.1,0.5,1,2],\n",
    "                 'xgb__n_estimators':[100,400,1000],             \n",
    "                 'xgb__booster':['gbtree','dart'],\n",
    "                 'xgb__subsample':[0.5, 0.2,0.8]\n",
    "            },\n",
    "    \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = train_data[[i for i in train_data.columns.tolist() if i not in ['target','id']]]\n",
    "x = train_data[reqd_columns[:51]]\n",
    "# x = train_data[anova_columns]\n",
    "y = train_data['target']\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y , stratify = y, test_size = 0.3, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits = 5, shuffle= True, random_state =123)\n",
    "score= 'roc_auc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train.shape[1]\n",
    "\n",
    "def prepare_classifier(noise_level = 0.01, input_shape = input_shape, dropout_rate = 0.2, l2_penalty = 0.02, l1_penalty= 0.01):\n",
    "    \n",
    "    kernel_regularizer = regularizers.l2(l2_penalty)\n",
    "    activity_regularizer = regularizers.l1(l1_penalty)\n",
    "    \n",
    "    model = Sequential()\n",
    "#     model.add(GaussianNoise(noise_level, input_shape = (input_shape,)))\n",
    "    model.add(Dense(250, activation='relu', input_dim = input_shape, kernel_regularizer=kernel_regularizer, activity_regularizer = activity_regularizer))\n",
    "#     model.add(GaussianNoise(noise_level))\n",
    "#     model.add(Dropout(rate = dropout_rate))\n",
    "    \n",
    "    model.add(Dense(250, activation='relu', kernel_regularizer=kernel_regularizer, activity_regularizer = activity_regularizer))\n",
    "#     model.add(Dropout(rate = dropout_rate))  \n",
    "#     model.add(GaussianNoise(noise_level))              \n",
    "    model.add(Dense(1, activation='softmax'))\n",
    "    \n",
    "\n",
    "    optimizer = Adam(0.01)\n",
    "#     optimizer = RMSprop(0.001)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "keras_classifier = KerasClassifier(build_fn = prepare_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "_es_monitor = 'val_loss'\n",
    "_es_patience = 50\n",
    "es = EarlyStopping(monitor=_es_monitor, mode='min', verbose=1, patience=_es_patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "tl = TomekLinks(return_indices=True, ratio='majority')\n",
    "_x_train_tomek, _y_train_tomek, id_tl = tl.fit_sample(x, y)\n",
    "\n",
    "smt = SMOTETomek(ratio='auto')\n",
    "_x_train_smt, _y_train_smt = smt.fit_sample(x, y)\n",
    "\n",
    "smote = SMOTE(ratio='minority')\n",
    "x_train_smote, y_train_smote = smote.fit_sample(x,y)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256 samples, validate on 50 samples\n",
      "Epoch 1/500\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 447.3546 - acc: 0.5000 - val_loss: 97.7166 - val_acc: 0.6400\n",
      "Epoch 2/500\n",
      "256/256 [==============================] - 0s 32us/step - loss: 414.4879 - acc: 0.5000 - val_loss: 91.6491 - val_acc: 0.6400\n",
      "Epoch 3/500\n",
      "256/256 [==============================] - 0s 34us/step - loss: 375.6239 - acc: 0.5000 - val_loss: 86.4345 - val_acc: 0.6400\n",
      "Epoch 4/500\n",
      "256/256 [==============================] - 0s 24us/step - loss: 342.4466 - acc: 0.5000 - val_loss: 82.0395 - val_acc: 0.6400\n",
      "Epoch 5/500\n",
      "256/256 [==============================] - 0s 52us/step - loss: 314.2857 - acc: 0.5000 - val_loss: 78.3738 - val_acc: 0.6400\n",
      "Epoch 6/500\n",
      "256/256 [==============================] - 0s 42us/step - loss: 290.3719 - acc: 0.5000 - val_loss: 75.3212 - val_acc: 0.6400\n",
      "Epoch 7/500\n",
      "256/256 [==============================] - 0s 33us/step - loss: 270.1113 - acc: 0.5000 - val_loss: 72.8602 - val_acc: 0.6400\n",
      "Epoch 8/500\n",
      "256/256 [==============================] - 0s 45us/step - loss: 253.2623 - acc: 0.5000 - val_loss: 70.7956 - val_acc: 0.6400\n",
      "Epoch 9/500\n",
      "256/256 [==============================] - 0s 61us/step - loss: 238.5018 - acc: 0.5000 - val_loss: 69.1041 - val_acc: 0.6400\n",
      "Epoch 10/500\n",
      "256/256 [==============================] - 0s 40us/step - loss: 225.8030 - acc: 0.5000 - val_loss: 67.6971 - val_acc: 0.6400\n",
      "Epoch 11/500\n",
      "256/256 [==============================] - 0s 58us/step - loss: 214.7443 - acc: 0.5000 - val_loss: 66.5241 - val_acc: 0.6400\n",
      "Epoch 12/500\n",
      "256/256 [==============================] - 0s 63us/step - loss: 204.9092 - acc: 0.5000 - val_loss: 65.5434 - val_acc: 0.6400\n",
      "Epoch 13/500\n",
      "256/256 [==============================] - 0s 58us/step - loss: 196.0572 - acc: 0.5000 - val_loss: 64.7109 - val_acc: 0.6400\n",
      "Epoch 14/500\n",
      "256/256 [==============================] - 0s 41us/step - loss: 187.9714 - acc: 0.5000 - val_loss: 63.9860 - val_acc: 0.6400\n",
      "Epoch 15/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 180.5336 - acc: 0.5000 - val_loss: 63.3553 - val_acc: 0.6400\n",
      "Epoch 16/500\n",
      "256/256 [==============================] - 0s 65us/step - loss: 173.6118 - acc: 0.5000 - val_loss: 62.8012 - val_acc: 0.6400\n",
      "Epoch 17/500\n",
      "256/256 [==============================] - 0s 55us/step - loss: 167.1254 - acc: 0.5000 - val_loss: 62.3085 - val_acc: 0.6400\n",
      "Epoch 18/500\n",
      "256/256 [==============================] - 0s 55us/step - loss: 161.0147 - acc: 0.5000 - val_loss: 61.8559 - val_acc: 0.6400\n",
      "Epoch 19/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 155.1647 - acc: 0.5000 - val_loss: 61.4448 - val_acc: 0.6400\n",
      "Epoch 20/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 149.6348 - acc: 0.5000 - val_loss: 61.0635 - val_acc: 0.6400\n",
      "Epoch 21/500\n",
      "256/256 [==============================] - 0s 82us/step - loss: 144.3405 - acc: 0.5000 - val_loss: 60.7037 - val_acc: 0.6400\n",
      "Epoch 22/500\n",
      "256/256 [==============================] - 0s 69us/step - loss: 139.2025 - acc: 0.5000 - val_loss: 60.3635 - val_acc: 0.6400\n",
      "Epoch 23/500\n",
      "256/256 [==============================] - 0s 26us/step - loss: 134.2668 - acc: 0.5000 - val_loss: 60.0406 - val_acc: 0.6400\n",
      "Epoch 24/500\n",
      "256/256 [==============================] - 0s 32us/step - loss: 129.5129 - acc: 0.5000 - val_loss: 59.7386 - val_acc: 0.6400\n",
      "Epoch 25/500\n",
      "256/256 [==============================] - 0s 43us/step - loss: 124.9880 - acc: 0.5000 - val_loss: 59.4506 - val_acc: 0.6400\n",
      "Epoch 26/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 120.6292 - acc: 0.5000 - val_loss: 59.1717 - val_acc: 0.6400\n",
      "Epoch 27/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 116.4332 - acc: 0.5000 - val_loss: 58.8967 - val_acc: 0.6400\n",
      "Epoch 28/500\n",
      "256/256 [==============================] - 0s 40us/step - loss: 112.3249 - acc: 0.5000 - val_loss: 58.6290 - val_acc: 0.6400\n",
      "Epoch 29/500\n",
      "256/256 [==============================] - 0s 63us/step - loss: 108.3530 - acc: 0.5000 - val_loss: 58.3729 - val_acc: 0.6400\n",
      "Epoch 30/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 104.5610 - acc: 0.5000 - val_loss: 58.1252 - val_acc: 0.6400\n",
      "Epoch 31/500\n",
      "256/256 [==============================] - 0s 45us/step - loss: 100.9021 - acc: 0.5000 - val_loss: 57.8852 - val_acc: 0.6400\n",
      "Epoch 32/500\n",
      "256/256 [==============================] - 0s 56us/step - loss: 97.3821 - acc: 0.5000 - val_loss: 57.6528 - val_acc: 0.6400\n",
      "Epoch 33/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 94.0002 - acc: 0.5000 - val_loss: 57.4278 - val_acc: 0.6400\n",
      "Epoch 34/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 90.7398 - acc: 0.5000 - val_loss: 57.2054 - val_acc: 0.6400\n",
      "Epoch 35/500\n",
      "256/256 [==============================] - 0s 52us/step - loss: 87.5486 - acc: 0.5000 - val_loss: 56.9900 - val_acc: 0.6400\n",
      "Epoch 36/500\n",
      "256/256 [==============================] - 0s 36us/step - loss: 84.5034 - acc: 0.5000 - val_loss: 56.7784 - val_acc: 0.6400\n",
      "Epoch 37/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 81.5337 - acc: 0.5000 - val_loss: 56.5721 - val_acc: 0.6400\n",
      "Epoch 38/500\n",
      "256/256 [==============================] - 0s 67us/step - loss: 78.6674 - acc: 0.5000 - val_loss: 56.3723 - val_acc: 0.6400\n",
      "Epoch 39/500\n",
      "256/256 [==============================] - 0s 67us/step - loss: 75.9322 - acc: 0.5000 - val_loss: 56.1775 - val_acc: 0.6400\n",
      "Epoch 40/500\n",
      "256/256 [==============================] - 0s 50us/step - loss: 73.2908 - acc: 0.5000 - val_loss: 55.9871 - val_acc: 0.6400\n",
      "Epoch 41/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 70.7371 - acc: 0.5000 - val_loss: 55.8002 - val_acc: 0.6400\n",
      "Epoch 42/500\n",
      "256/256 [==============================] - 0s 38us/step - loss: 68.2697 - acc: 0.5000 - val_loss: 55.6170 - val_acc: 0.6400\n",
      "Epoch 43/500\n",
      "256/256 [==============================] - 0s 45us/step - loss: 65.8851 - acc: 0.5000 - val_loss: 55.4355 - val_acc: 0.6400\n",
      "Epoch 44/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 63.5622 - acc: 0.5000 - val_loss: 55.2577 - val_acc: 0.6400\n",
      "Epoch 45/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 61.3262 - acc: 0.5000 - val_loss: 55.0837 - val_acc: 0.6400\n",
      "Epoch 46/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 59.1728 - acc: 0.5000 - val_loss: 54.9166 - val_acc: 0.6400\n",
      "Epoch 47/500\n",
      "256/256 [==============================] - 0s 73us/step - loss: 57.1090 - acc: 0.5000 - val_loss: 54.7547 - val_acc: 0.6400\n",
      "Epoch 48/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 55.1182 - acc: 0.5000 - val_loss: 54.5976 - val_acc: 0.6400\n",
      "Epoch 49/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 53.1999 - acc: 0.5000 - val_loss: 54.4440 - val_acc: 0.6400\n",
      "Epoch 50/500\n",
      "256/256 [==============================] - 0s 36us/step - loss: 51.3291 - acc: 0.5000 - val_loss: 54.2953 - val_acc: 0.6400\n",
      "Epoch 51/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 49.5456 - acc: 0.5000 - val_loss: 54.1506 - val_acc: 0.6400\n",
      "Epoch 52/500\n",
      "256/256 [==============================] - 0s 49us/step - loss: 47.8279 - acc: 0.5000 - val_loss: 54.0092 - val_acc: 0.6400\n",
      "Epoch 53/500\n",
      "256/256 [==============================] - 0s 63us/step - loss: 46.1608 - acc: 0.5000 - val_loss: 53.8708 - val_acc: 0.6400\n",
      "Epoch 54/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 44.5499 - acc: 0.5000 - val_loss: 53.7374 - val_acc: 0.6400\n",
      "Epoch 55/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 43.0109 - acc: 0.5000 - val_loss: 53.6071 - val_acc: 0.6400\n",
      "Epoch 56/500\n",
      "256/256 [==============================] - 0s 27us/step - loss: 41.5286 - acc: 0.5000 - val_loss: 53.4801 - val_acc: 0.6400\n",
      "Epoch 57/500\n",
      "256/256 [==============================] - 0s 35us/step - loss: 40.1023 - acc: 0.5000 - val_loss: 53.3564 - val_acc: 0.6400\n",
      "Epoch 58/500\n",
      "256/256 [==============================] - 0s 26us/step - loss: 38.7345 - acc: 0.5000 - val_loss: 53.2356 - val_acc: 0.6400\n",
      "Epoch 59/500\n",
      "256/256 [==============================] - 0s 49us/step - loss: 37.4194 - acc: 0.5000 - val_loss: 53.1173 - val_acc: 0.6400\n",
      "Epoch 60/500\n",
      "256/256 [==============================] - 0s 54us/step - loss: 36.1559 - acc: 0.5000 - val_loss: 53.0025 - val_acc: 0.6400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/500\n",
      "256/256 [==============================] - 0s 43us/step - loss: 34.9343 - acc: 0.5000 - val_loss: 52.8898 - val_acc: 0.6400\n",
      "Epoch 62/500\n",
      "256/256 [==============================] - 0s 44us/step - loss: 33.7549 - acc: 0.5000 - val_loss: 52.7823 - val_acc: 0.6400\n",
      "Epoch 63/500\n",
      "256/256 [==============================] - 0s 45us/step - loss: 32.6316 - acc: 0.5000 - val_loss: 52.6782 - val_acc: 0.6400\n",
      "Epoch 64/500\n",
      "256/256 [==============================] - 0s 42us/step - loss: 31.5593 - acc: 0.5000 - val_loss: 52.5770 - val_acc: 0.6400\n",
      "Epoch 65/500\n",
      "256/256 [==============================] - 0s 54us/step - loss: 30.5301 - acc: 0.5000 - val_loss: 52.4800 - val_acc: 0.6400\n",
      "Epoch 66/500\n",
      "256/256 [==============================] - 0s 64us/step - loss: 29.5551 - acc: 0.5000 - val_loss: 52.3863 - val_acc: 0.6400\n",
      "Epoch 67/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 28.6263 - acc: 0.5000 - val_loss: 52.2957 - val_acc: 0.6400\n",
      "Epoch 68/500\n",
      "256/256 [==============================] - 0s 43us/step - loss: 27.7357 - acc: 0.5000 - val_loss: 52.2072 - val_acc: 0.6400\n",
      "Epoch 69/500\n",
      "256/256 [==============================] - 0s 52us/step - loss: 26.8881 - acc: 0.5000 - val_loss: 52.1219 - val_acc: 0.6400\n",
      "Epoch 70/500\n",
      "256/256 [==============================] - 0s 33us/step - loss: 26.0901 - acc: 0.5000 - val_loss: 52.0384 - val_acc: 0.6400\n",
      "Epoch 71/500\n",
      "256/256 [==============================] - 0s 30us/step - loss: 25.3326 - acc: 0.5000 - val_loss: 51.9572 - val_acc: 0.6400\n",
      "Epoch 72/500\n",
      "256/256 [==============================] - 0s 33us/step - loss: 24.6148 - acc: 0.5000 - val_loss: 51.8782 - val_acc: 0.6400\n",
      "Epoch 73/500\n",
      "256/256 [==============================] - 0s 39us/step - loss: 23.9353 - acc: 0.5000 - val_loss: 51.8009 - val_acc: 0.6400\n",
      "Epoch 74/500\n",
      "256/256 [==============================] - 0s 37us/step - loss: 23.2932 - acc: 0.5000 - val_loss: 51.7272 - val_acc: 0.6400\n",
      "Epoch 75/500\n",
      "256/256 [==============================] - 0s 37us/step - loss: 22.6915 - acc: 0.5000 - val_loss: 51.6557 - val_acc: 0.6400\n",
      "Epoch 76/500\n",
      "256/256 [==============================] - 0s 27us/step - loss: 22.1280 - acc: 0.5000 - val_loss: 51.5871 - val_acc: 0.6400\n",
      "Epoch 77/500\n",
      "256/256 [==============================] - 0s 27us/step - loss: 21.6050 - acc: 0.5000 - val_loss: 51.5210 - val_acc: 0.6400\n",
      "Epoch 78/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 21.1174 - acc: 0.5000 - val_loss: 51.4570 - val_acc: 0.6400\n",
      "Epoch 79/500\n",
      "256/256 [==============================] - 0s 39us/step - loss: 20.6580 - acc: 0.5000 - val_loss: 51.3939 - val_acc: 0.6400\n",
      "Epoch 80/500\n",
      "256/256 [==============================] - 0s 38us/step - loss: 20.2261 - acc: 0.5000 - val_loss: 51.3327 - val_acc: 0.6400\n",
      "Epoch 81/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 19.8265 - acc: 0.5000 - val_loss: 51.2733 - val_acc: 0.6400\n",
      "Epoch 82/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 19.4558 - acc: 0.5000 - val_loss: 51.2162 - val_acc: 0.6400\n",
      "Epoch 83/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 19.1171 - acc: 0.5000 - val_loss: 51.1603 - val_acc: 0.6400\n",
      "Epoch 84/500\n",
      "256/256 [==============================] - 0s 57us/step - loss: 18.8083 - acc: 0.5000 - val_loss: 51.1050 - val_acc: 0.6400\n",
      "Epoch 85/500\n",
      "256/256 [==============================] - 0s 49us/step - loss: 18.5238 - acc: 0.5000 - val_loss: 51.0496 - val_acc: 0.6400\n",
      "Epoch 86/500\n",
      "256/256 [==============================] - 0s 54us/step - loss: 18.2650 - acc: 0.5000 - val_loss: 50.9944 - val_acc: 0.6400\n",
      "Epoch 87/500\n",
      "256/256 [==============================] - 0s 46us/step - loss: 18.0293 - acc: 0.5000 - val_loss: 50.9404 - val_acc: 0.6400\n",
      "Epoch 88/500\n",
      "256/256 [==============================] - 0s 49us/step - loss: 17.8145 - acc: 0.5000 - val_loss: 50.8872 - val_acc: 0.6400\n",
      "Epoch 89/500\n",
      "256/256 [==============================] - 0s 58us/step - loss: 17.6226 - acc: 0.5000 - val_loss: 50.8355 - val_acc: 0.6400\n",
      "Epoch 90/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 17.4534 - acc: 0.5000 - val_loss: 50.7844 - val_acc: 0.6400\n",
      "Epoch 91/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 17.3012 - acc: 0.5000 - val_loss: 50.7339 - val_acc: 0.6400\n",
      "Epoch 92/500\n",
      "256/256 [==============================] - 0s 55us/step - loss: 17.1624 - acc: 0.5000 - val_loss: 50.6843 - val_acc: 0.6400\n",
      "Epoch 93/500\n",
      "256/256 [==============================] - 0s 50us/step - loss: 17.0399 - acc: 0.5000 - val_loss: 50.6361 - val_acc: 0.6400\n",
      "Epoch 94/500\n",
      "256/256 [==============================] - 0s 32us/step - loss: 16.9323 - acc: 0.5000 - val_loss: 50.5882 - val_acc: 0.6400\n",
      "Epoch 95/500\n",
      "256/256 [==============================] - 0s 37us/step - loss: 16.8377 - acc: 0.5000 - val_loss: 50.5409 - val_acc: 0.6400\n",
      "Epoch 96/500\n",
      "256/256 [==============================] - 0s 29us/step - loss: 16.7563 - acc: 0.5000 - val_loss: 50.4948 - val_acc: 0.6400\n",
      "Epoch 97/500\n",
      "256/256 [==============================] - 0s 32us/step - loss: 16.6856 - acc: 0.5000 - val_loss: 50.4494 - val_acc: 0.6400\n",
      "Epoch 98/500\n",
      "256/256 [==============================] - 0s 31us/step - loss: 16.6245 - acc: 0.5000 - val_loss: 50.4045 - val_acc: 0.6400\n",
      "Epoch 99/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 16.5719 - acc: 0.5000 - val_loss: 50.3603 - val_acc: 0.6400\n",
      "Epoch 100/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 16.5272 - acc: 0.5000 - val_loss: 50.3168 - val_acc: 0.6400\n",
      "Epoch 101/500\n",
      "256/256 [==============================] - 0s 40us/step - loss: 16.4897 - acc: 0.5000 - val_loss: 50.2734 - val_acc: 0.6400\n",
      "Epoch 102/500\n",
      "256/256 [==============================] - 0s 40us/step - loss: 16.4570 - acc: 0.5000 - val_loss: 50.2305 - val_acc: 0.6400\n",
      "Epoch 103/500\n",
      "256/256 [==============================] - 0s 42us/step - loss: 16.4288 - acc: 0.5000 - val_loss: 50.1874 - val_acc: 0.6400\n",
      "Epoch 104/500\n",
      "256/256 [==============================] - 0s 46us/step - loss: 16.4060 - acc: 0.5000 - val_loss: 50.1439 - val_acc: 0.6400\n",
      "Epoch 105/500\n",
      "256/256 [==============================] - 0s 38us/step - loss: 16.3852 - acc: 0.5000 - val_loss: 50.1006 - val_acc: 0.6400\n",
      "Epoch 106/500\n",
      "256/256 [==============================] - 0s 54us/step - loss: 16.3668 - acc: 0.5000 - val_loss: 50.0577 - val_acc: 0.6400\n",
      "Epoch 107/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 16.3510 - acc: 0.5000 - val_loss: 50.0145 - val_acc: 0.6400\n",
      "Epoch 108/500\n",
      "256/256 [==============================] - 0s 42us/step - loss: 16.3362 - acc: 0.5000 - val_loss: 49.9708 - val_acc: 0.6400\n",
      "Epoch 109/500\n",
      "256/256 [==============================] - 0s 41us/step - loss: 16.3222 - acc: 0.5000 - val_loss: 49.9271 - val_acc: 0.6400\n",
      "Epoch 110/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 16.3087 - acc: 0.5000 - val_loss: 49.8835 - val_acc: 0.6400\n",
      "Epoch 111/500\n",
      "256/256 [==============================] - 0s 45us/step - loss: 16.2955 - acc: 0.5000 - val_loss: 49.8397 - val_acc: 0.6400\n",
      "Epoch 112/500\n",
      "256/256 [==============================] - 0s 38us/step - loss: 16.2823 - acc: 0.5000 - val_loss: 49.7958 - val_acc: 0.6400\n",
      "Epoch 113/500\n",
      "256/256 [==============================] - 0s 58us/step - loss: 16.2698 - acc: 0.5000 - val_loss: 49.7518 - val_acc: 0.6400\n",
      "Epoch 114/500\n",
      "256/256 [==============================] - 0s 60us/step - loss: 16.2572 - acc: 0.5000 - val_loss: 49.7076 - val_acc: 0.6400\n",
      "Epoch 115/500\n",
      "256/256 [==============================] - 0s 49us/step - loss: 16.2448 - acc: 0.5000 - val_loss: 49.6631 - val_acc: 0.6400\n",
      "Epoch 116/500\n",
      "256/256 [==============================] - 0s 42us/step - loss: 16.2321 - acc: 0.5000 - val_loss: 49.6185 - val_acc: 0.6400\n",
      "Epoch 117/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 16.2194 - acc: 0.5000 - val_loss: 49.5737 - val_acc: 0.6400\n",
      "Epoch 118/500\n",
      "256/256 [==============================] - 0s 58us/step - loss: 16.2067 - acc: 0.5000 - val_loss: 49.5289 - val_acc: 0.6400\n",
      "Epoch 119/500\n",
      "256/256 [==============================] - 0s 28us/step - loss: 16.1941 - acc: 0.5000 - val_loss: 49.4840 - val_acc: 0.6400\n",
      "Epoch 120/500\n",
      "256/256 [==============================] - 0s 28us/step - loss: 16.1813 - acc: 0.5000 - val_loss: 49.4387 - val_acc: 0.6400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/500\n",
      "256/256 [==============================] - 0s 21us/step - loss: 16.1685 - acc: 0.5000 - val_loss: 49.3936 - val_acc: 0.6400\n",
      "Epoch 122/500\n",
      "256/256 [==============================] - 0s 32us/step - loss: 16.1557 - acc: 0.5000 - val_loss: 49.3485 - val_acc: 0.6400\n",
      "Epoch 123/500\n",
      "256/256 [==============================] - 0s 35us/step - loss: 16.1429 - acc: 0.5000 - val_loss: 49.3031 - val_acc: 0.6400\n",
      "Epoch 124/500\n",
      "256/256 [==============================] - 0s 34us/step - loss: 16.1300 - acc: 0.5000 - val_loss: 49.2578 - val_acc: 0.6400\n",
      "Epoch 125/500\n",
      "256/256 [==============================] - 0s 39us/step - loss: 16.1171 - acc: 0.5000 - val_loss: 49.2125 - val_acc: 0.6400\n",
      "Epoch 126/500\n",
      "256/256 [==============================] - 0s 60us/step - loss: 16.1042 - acc: 0.5000 - val_loss: 49.1671 - val_acc: 0.6400\n",
      "Epoch 127/500\n",
      "256/256 [==============================] - 0s 58us/step - loss: 16.0912 - acc: 0.5000 - val_loss: 49.1217 - val_acc: 0.6400\n",
      "Epoch 128/500\n",
      "256/256 [==============================] - 0s 60us/step - loss: 16.0782 - acc: 0.5000 - val_loss: 49.0761 - val_acc: 0.6400\n",
      "Epoch 129/500\n",
      "256/256 [==============================] - 0s 56us/step - loss: 16.0652 - acc: 0.5000 - val_loss: 49.0303 - val_acc: 0.6400\n",
      "Epoch 130/500\n",
      "256/256 [==============================] - 0s 61us/step - loss: 16.0522 - acc: 0.5000 - val_loss: 48.9844 - val_acc: 0.6400\n",
      "Epoch 131/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 16.0391 - acc: 0.5000 - val_loss: 48.9384 - val_acc: 0.6400\n",
      "Epoch 132/500\n",
      "256/256 [==============================] - 0s 58us/step - loss: 16.0260 - acc: 0.5000 - val_loss: 48.8924 - val_acc: 0.6400\n",
      "Epoch 133/500\n",
      "256/256 [==============================] - 0s 52us/step - loss: 16.0128 - acc: 0.5000 - val_loss: 48.8466 - val_acc: 0.6400\n",
      "Epoch 134/500\n",
      "256/256 [==============================] - 0s 52us/step - loss: 15.9997 - acc: 0.5000 - val_loss: 48.8005 - val_acc: 0.6400\n",
      "Epoch 135/500\n",
      "256/256 [==============================] - 0s 55us/step - loss: 15.9865 - acc: 0.5000 - val_loss: 48.7545 - val_acc: 0.6400\n",
      "Epoch 136/500\n",
      "256/256 [==============================] - 0s 45us/step - loss: 15.9734 - acc: 0.5000 - val_loss: 48.7083 - val_acc: 0.6400\n",
      "Epoch 137/500\n",
      "256/256 [==============================] - 0s 46us/step - loss: 15.9602 - acc: 0.5000 - val_loss: 48.6621 - val_acc: 0.6400\n",
      "Epoch 138/500\n",
      "256/256 [==============================] - 0s 24us/step - loss: 15.9470 - acc: 0.5000 - val_loss: 48.6156 - val_acc: 0.6400\n",
      "Epoch 139/500\n",
      "256/256 [==============================] - 0s 22us/step - loss: 15.9337 - acc: 0.5000 - val_loss: 48.5690 - val_acc: 0.6400\n",
      "Epoch 140/500\n",
      "256/256 [==============================] - 0s 58us/step - loss: 15.9204 - acc: 0.5000 - val_loss: 48.5221 - val_acc: 0.6400\n",
      "Epoch 141/500\n",
      "256/256 [==============================] - 0s 46us/step - loss: 15.9072 - acc: 0.5000 - val_loss: 48.4752 - val_acc: 0.6400\n",
      "Epoch 142/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 15.8939 - acc: 0.5000 - val_loss: 48.4283 - val_acc: 0.6400\n",
      "Epoch 143/500\n",
      "256/256 [==============================] - 0s 33us/step - loss: 15.8806 - acc: 0.5000 - val_loss: 48.3812 - val_acc: 0.6400\n",
      "Epoch 144/500\n",
      "256/256 [==============================] - 0s 55us/step - loss: 15.8673 - acc: 0.5000 - val_loss: 48.3341 - val_acc: 0.6400\n",
      "Epoch 145/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 15.8540 - acc: 0.5000 - val_loss: 48.2869 - val_acc: 0.6400\n",
      "Epoch 146/500\n",
      "256/256 [==============================] - 0s 92us/step - loss: 15.8406 - acc: 0.5000 - val_loss: 48.2396 - val_acc: 0.6400\n",
      "Epoch 147/500\n",
      "256/256 [==============================] - 0s 49us/step - loss: 15.8273 - acc: 0.5000 - val_loss: 48.1921 - val_acc: 0.6400\n",
      "Epoch 148/500\n",
      "256/256 [==============================] - 0s 75us/step - loss: 15.8139 - acc: 0.5000 - val_loss: 48.1447 - val_acc: 0.6400\n",
      "Epoch 149/500\n",
      "256/256 [==============================] - 0s 37us/step - loss: 15.8005 - acc: 0.5000 - val_loss: 48.0971 - val_acc: 0.6400\n",
      "Epoch 150/500\n",
      "256/256 [==============================] - 0s 32us/step - loss: 15.7872 - acc: 0.5000 - val_loss: 48.0493 - val_acc: 0.6400\n",
      "Epoch 151/500\n",
      "256/256 [==============================] - 0s 41us/step - loss: 15.7738 - acc: 0.5000 - val_loss: 48.0014 - val_acc: 0.6400\n",
      "Epoch 152/500\n",
      "256/256 [==============================] - 0s 25us/step - loss: 15.7603 - acc: 0.5000 - val_loss: 47.9534 - val_acc: 0.6400\n",
      "Epoch 153/500\n",
      "256/256 [==============================] - 0s 38us/step - loss: 15.7469 - acc: 0.5000 - val_loss: 47.9052 - val_acc: 0.6400\n",
      "Epoch 154/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 15.7334 - acc: 0.5000 - val_loss: 47.8568 - val_acc: 0.6400\n",
      "Epoch 155/500\n",
      "256/256 [==============================] - 0s 59us/step - loss: 15.7200 - acc: 0.5000 - val_loss: 47.8085 - val_acc: 0.6400\n",
      "Epoch 156/500\n",
      "256/256 [==============================] - 0s 65us/step - loss: 15.7065 - acc: 0.5000 - val_loss: 47.7601 - val_acc: 0.6400\n",
      "Epoch 157/500\n",
      "256/256 [==============================] - 0s 44us/step - loss: 15.6930 - acc: 0.5000 - val_loss: 47.7115 - val_acc: 0.6400\n",
      "Epoch 158/500\n",
      "256/256 [==============================] - 0s 29us/step - loss: 15.6795 - acc: 0.5000 - val_loss: 47.6629 - val_acc: 0.6400\n",
      "Epoch 159/500\n",
      "256/256 [==============================] - 0s 38us/step - loss: 15.6661 - acc: 0.5000 - val_loss: 47.6142 - val_acc: 0.6400\n",
      "Epoch 160/500\n",
      "256/256 [==============================] - 0s 37us/step - loss: 15.6526 - acc: 0.5000 - val_loss: 47.5654 - val_acc: 0.6400\n",
      "Epoch 161/500\n",
      "256/256 [==============================] - 0s 29us/step - loss: 15.6390 - acc: 0.5000 - val_loss: 47.5163 - val_acc: 0.6400\n",
      "Epoch 162/500\n",
      "256/256 [==============================] - 0s 39us/step - loss: 15.6256 - acc: 0.5000 - val_loss: 47.4672 - val_acc: 0.6400\n",
      "Epoch 163/500\n",
      "256/256 [==============================] - 0s 49us/step - loss: 15.6120 - acc: 0.5000 - val_loss: 47.4181 - val_acc: 0.6400\n",
      "Epoch 164/500\n",
      "256/256 [==============================] - 0s 57us/step - loss: 15.5985 - acc: 0.5000 - val_loss: 47.3687 - val_acc: 0.6400\n",
      "Epoch 165/500\n",
      "256/256 [==============================] - 0s 54us/step - loss: 15.5849 - acc: 0.5000 - val_loss: 47.3192 - val_acc: 0.6400\n",
      "Epoch 166/500\n",
      "256/256 [==============================] - 0s 45us/step - loss: 15.5714 - acc: 0.5000 - val_loss: 47.2695 - val_acc: 0.6400\n",
      "Epoch 167/500\n",
      "256/256 [==============================] - 0s 43us/step - loss: 15.5578 - acc: 0.5000 - val_loss: 47.2196 - val_acc: 0.6400\n",
      "Epoch 168/500\n",
      "256/256 [==============================] - 0s 57us/step - loss: 15.5442 - acc: 0.5000 - val_loss: 47.1697 - val_acc: 0.6400\n",
      "Epoch 169/500\n",
      "256/256 [==============================] - 0s 54us/step - loss: 15.5306 - acc: 0.5000 - val_loss: 47.1199 - val_acc: 0.6400\n",
      "Epoch 170/500\n",
      "256/256 [==============================] - 0s 56us/step - loss: 15.5171 - acc: 0.5000 - val_loss: 47.0703 - val_acc: 0.6400\n",
      "Epoch 171/500\n",
      "256/256 [==============================] - 0s 60us/step - loss: 15.5035 - acc: 0.5000 - val_loss: 47.0207 - val_acc: 0.6400\n",
      "Epoch 172/500\n",
      "256/256 [==============================] - 0s 65us/step - loss: 15.4899 - acc: 0.5000 - val_loss: 46.9709 - val_acc: 0.6400\n",
      "Epoch 173/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 15.4762 - acc: 0.5000 - val_loss: 46.9211 - val_acc: 0.6400\n",
      "Epoch 174/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 15.4626 - acc: 0.5000 - val_loss: 46.8712 - val_acc: 0.6400\n",
      "Epoch 175/500\n",
      "256/256 [==============================] - 0s 52us/step - loss: 15.4490 - acc: 0.5000 - val_loss: 46.8210 - val_acc: 0.6400\n",
      "Epoch 176/500\n",
      "256/256 [==============================] - 0s 66us/step - loss: 15.4354 - acc: 0.5000 - val_loss: 46.7710 - val_acc: 0.6400\n",
      "Epoch 177/500\n",
      "256/256 [==============================] - 0s 52us/step - loss: 15.4218 - acc: 0.5000 - val_loss: 46.7208 - val_acc: 0.6400\n",
      "Epoch 178/500\n",
      "256/256 [==============================] - 0s 52us/step - loss: 15.4081 - acc: 0.5000 - val_loss: 46.6705 - val_acc: 0.6400\n",
      "Epoch 179/500\n",
      "256/256 [==============================] - 0s 30us/step - loss: 15.3945 - acc: 0.5000 - val_loss: 46.6202 - val_acc: 0.6400\n",
      "Epoch 180/500\n",
      "256/256 [==============================] - 0s 27us/step - loss: 15.3809 - acc: 0.5000 - val_loss: 46.5698 - val_acc: 0.6400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/500\n",
      "256/256 [==============================] - 0s 36us/step - loss: 15.3673 - acc: 0.5000 - val_loss: 46.5192 - val_acc: 0.6400\n",
      "Epoch 182/500\n",
      "256/256 [==============================] - 0s 39us/step - loss: 15.3536 - acc: 0.5000 - val_loss: 46.4686 - val_acc: 0.6400\n",
      "Epoch 183/500\n",
      "256/256 [==============================] - 0s 38us/step - loss: 15.3400 - acc: 0.5000 - val_loss: 46.4181 - val_acc: 0.6400\n",
      "Epoch 184/500\n",
      "256/256 [==============================] - 0s 43us/step - loss: 15.3263 - acc: 0.5000 - val_loss: 46.3676 - val_acc: 0.6400\n",
      "Epoch 185/500\n",
      "256/256 [==============================] - 0s 40us/step - loss: 15.3127 - acc: 0.5000 - val_loss: 46.3170 - val_acc: 0.6400\n",
      "Epoch 186/500\n",
      "256/256 [==============================] - 0s 39us/step - loss: 15.2991 - acc: 0.5000 - val_loss: 46.2662 - val_acc: 0.6400\n",
      "Epoch 187/500\n",
      "256/256 [==============================] - 0s 35us/step - loss: 15.2854 - acc: 0.5000 - val_loss: 46.2151 - val_acc: 0.6400\n",
      "Epoch 188/500\n",
      "256/256 [==============================] - 0s 39us/step - loss: 15.2718 - acc: 0.5000 - val_loss: 46.1638 - val_acc: 0.6400\n",
      "Epoch 189/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 15.2581 - acc: 0.5000 - val_loss: 46.1127 - val_acc: 0.6400\n",
      "Epoch 190/500\n",
      "256/256 [==============================] - 0s 40us/step - loss: 15.2445 - acc: 0.5000 - val_loss: 46.0614 - val_acc: 0.6400\n",
      "Epoch 191/500\n",
      "256/256 [==============================] - 0s 55us/step - loss: 15.2308 - acc: 0.5000 - val_loss: 46.0102 - val_acc: 0.6400\n",
      "Epoch 192/500\n",
      "256/256 [==============================] - 0s 59us/step - loss: 15.2172 - acc: 0.5000 - val_loss: 45.9590 - val_acc: 0.6400\n",
      "Epoch 193/500\n",
      "256/256 [==============================] - 0s 55us/step - loss: 15.2036 - acc: 0.5000 - val_loss: 45.9078 - val_acc: 0.6400\n",
      "Epoch 194/500\n",
      "256/256 [==============================] - 0s 69us/step - loss: 15.1899 - acc: 0.5000 - val_loss: 45.8567 - val_acc: 0.6400\n",
      "Epoch 195/500\n",
      "256/256 [==============================] - 0s 69us/step - loss: 15.1763 - acc: 0.5000 - val_loss: 45.8057 - val_acc: 0.6400\n",
      "Epoch 196/500\n",
      "256/256 [==============================] - 0s 31us/step - loss: 15.1627 - acc: 0.5000 - val_loss: 45.7546 - val_acc: 0.6400\n",
      "Epoch 197/500\n",
      "256/256 [==============================] - 0s 35us/step - loss: 15.1490 - acc: 0.5000 - val_loss: 45.7039 - val_acc: 0.6400\n",
      "Epoch 198/500\n",
      "256/256 [==============================] - 0s 26us/step - loss: 15.1354 - acc: 0.5000 - val_loss: 45.6530 - val_acc: 0.6400\n",
      "Epoch 199/500\n",
      "256/256 [==============================] - 0s 43us/step - loss: 15.1218 - acc: 0.5000 - val_loss: 45.6021 - val_acc: 0.6400\n",
      "Epoch 200/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 15.1082 - acc: 0.5000 - val_loss: 45.5509 - val_acc: 0.6400\n",
      "Epoch 201/500\n",
      "256/256 [==============================] - 0s 65us/step - loss: 15.0946 - acc: 0.5000 - val_loss: 45.4999 - val_acc: 0.6400\n",
      "Epoch 202/500\n",
      "256/256 [==============================] - 0s 55us/step - loss: 15.0809 - acc: 0.5000 - val_loss: 45.4488 - val_acc: 0.6400\n",
      "Epoch 203/500\n",
      "256/256 [==============================] - 0s 60us/step - loss: 15.0674 - acc: 0.5000 - val_loss: 45.3979 - val_acc: 0.6400\n",
      "Epoch 204/500\n",
      "256/256 [==============================] - 0s 57us/step - loss: 15.0537 - acc: 0.5000 - val_loss: 45.3470 - val_acc: 0.6400\n",
      "Epoch 205/500\n",
      "256/256 [==============================] - 0s 64us/step - loss: 15.0402 - acc: 0.5000 - val_loss: 45.2962 - val_acc: 0.6400\n",
      "Epoch 206/500\n",
      "256/256 [==============================] - 0s 45us/step - loss: 15.0265 - acc: 0.5000 - val_loss: 45.2452 - val_acc: 0.6400\n",
      "Epoch 207/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 15.0129 - acc: 0.5000 - val_loss: 45.1942 - val_acc: 0.6400\n",
      "Epoch 208/500\n",
      "256/256 [==============================] - 0s 44us/step - loss: 14.9993 - acc: 0.5000 - val_loss: 45.1432 - val_acc: 0.6400\n",
      "Epoch 209/500\n",
      "256/256 [==============================] - 0s 28us/step - loss: 14.9857 - acc: 0.5000 - val_loss: 45.0919 - val_acc: 0.6400\n",
      "Epoch 210/500\n",
      "256/256 [==============================] - 0s 50us/step - loss: 14.9721 - acc: 0.5000 - val_loss: 45.0407 - val_acc: 0.6400\n",
      "Epoch 211/500\n",
      "256/256 [==============================] - 0s 54us/step - loss: 14.9585 - acc: 0.5000 - val_loss: 44.9893 - val_acc: 0.6400\n",
      "Epoch 212/500\n",
      "256/256 [==============================] - 0s 60us/step - loss: 14.9449 - acc: 0.5000 - val_loss: 44.9379 - val_acc: 0.6400\n",
      "Epoch 213/500\n",
      "256/256 [==============================] - 0s 29us/step - loss: 14.9313 - acc: 0.5000 - val_loss: 44.8864 - val_acc: 0.6400\n",
      "Epoch 214/500\n",
      "256/256 [==============================] - 0s 35us/step - loss: 14.9177 - acc: 0.5000 - val_loss: 44.8348 - val_acc: 0.6400\n",
      "Epoch 215/500\n",
      "256/256 [==============================] - 0s 35us/step - loss: 14.9040 - acc: 0.5000 - val_loss: 44.7832 - val_acc: 0.6400\n",
      "Epoch 216/500\n",
      "256/256 [==============================] - 0s 30us/step - loss: 14.8905 - acc: 0.5000 - val_loss: 44.7314 - val_acc: 0.6400\n",
      "Epoch 217/500\n",
      "256/256 [==============================] - 0s 54us/step - loss: 14.8769 - acc: 0.5000 - val_loss: 44.6796 - val_acc: 0.6400\n",
      "Epoch 218/500\n",
      "256/256 [==============================] - 0s 56us/step - loss: 14.8633 - acc: 0.5000 - val_loss: 44.6278 - val_acc: 0.6400\n",
      "Epoch 219/500\n",
      "256/256 [==============================] - 0s 60us/step - loss: 14.8498 - acc: 0.5000 - val_loss: 44.5760 - val_acc: 0.6400\n",
      "Epoch 220/500\n",
      "256/256 [==============================] - 0s 50us/step - loss: 14.8362 - acc: 0.5000 - val_loss: 44.5238 - val_acc: 0.6400\n",
      "Epoch 221/500\n",
      "256/256 [==============================] - 0s 35us/step - loss: 14.8226 - acc: 0.5000 - val_loss: 44.4718 - val_acc: 0.6400\n",
      "Epoch 222/500\n",
      "256/256 [==============================] - 0s 34us/step - loss: 14.8091 - acc: 0.5000 - val_loss: 44.4198 - val_acc: 0.6400\n",
      "Epoch 223/500\n",
      "256/256 [==============================] - 0s 25us/step - loss: 14.7955 - acc: 0.5000 - val_loss: 44.3677 - val_acc: 0.6400\n",
      "Epoch 224/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 14.7819 - acc: 0.5000 - val_loss: 44.3154 - val_acc: 0.6400\n",
      "Epoch 225/500\n",
      "256/256 [==============================] - 0s 35us/step - loss: 14.7684 - acc: 0.5000 - val_loss: 44.2631 - val_acc: 0.6400\n",
      "Epoch 226/500\n",
      "256/256 [==============================] - 0s 42us/step - loss: 14.7549 - acc: 0.5000 - val_loss: 44.2108 - val_acc: 0.6400\n",
      "Epoch 227/500\n",
      "256/256 [==============================] - 0s 36us/step - loss: 14.7413 - acc: 0.5000 - val_loss: 44.1583 - val_acc: 0.6400\n",
      "Epoch 228/500\n",
      "256/256 [==============================] - 0s 56us/step - loss: 14.7278 - acc: 0.5000 - val_loss: 44.1057 - val_acc: 0.6400\n",
      "Epoch 229/500\n",
      "256/256 [==============================] - 0s 33us/step - loss: 14.7143 - acc: 0.5000 - val_loss: 44.0531 - val_acc: 0.6400\n",
      "Epoch 230/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 14.7007 - acc: 0.5000 - val_loss: 44.0007 - val_acc: 0.6400\n",
      "Epoch 231/500\n",
      "256/256 [==============================] - 0s 43us/step - loss: 14.6873 - acc: 0.5000 - val_loss: 43.9481 - val_acc: 0.6400\n",
      "Epoch 232/500\n",
      "256/256 [==============================] - 0s 54us/step - loss: 14.6738 - acc: 0.5000 - val_loss: 43.8952 - val_acc: 0.6400\n",
      "Epoch 233/500\n",
      "256/256 [==============================] - 0s 37us/step - loss: 14.6603 - acc: 0.5000 - val_loss: 43.8424 - val_acc: 0.6400\n",
      "Epoch 234/500\n",
      "256/256 [==============================] - 0s 24us/step - loss: 14.6468 - acc: 0.5000 - val_loss: 43.7896 - val_acc: 0.6400\n",
      "Epoch 235/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 14.6333 - acc: 0.5000 - val_loss: 43.7367 - val_acc: 0.6400\n",
      "Epoch 236/500\n",
      "256/256 [==============================] - 0s 34us/step - loss: 14.6198 - acc: 0.5000 - val_loss: 43.6840 - val_acc: 0.6400\n",
      "Epoch 237/500\n",
      "256/256 [==============================] - 0s 41us/step - loss: 14.6064 - acc: 0.5000 - val_loss: 43.6313 - val_acc: 0.6400\n",
      "Epoch 238/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 14.5929 - acc: 0.5000 - val_loss: 43.5784 - val_acc: 0.6400\n",
      "Epoch 239/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 14.5795 - acc: 0.5000 - val_loss: 43.5259 - val_acc: 0.6400\n",
      "Epoch 240/500\n",
      "256/256 [==============================] - 0s 42us/step - loss: 14.5661 - acc: 0.5000 - val_loss: 43.4735 - val_acc: 0.6400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241/500\n",
      "256/256 [==============================] - 0s 38us/step - loss: 14.5528 - acc: 0.5000 - val_loss: 43.4212 - val_acc: 0.6400\n",
      "Epoch 242/500\n",
      "256/256 [==============================] - 0s 45us/step - loss: 14.5393 - acc: 0.5000 - val_loss: 43.3689 - val_acc: 0.6400\n",
      "Epoch 243/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 14.5258 - acc: 0.5000 - val_loss: 43.3165 - val_acc: 0.6400\n",
      "Epoch 244/500\n",
      "256/256 [==============================] - 0s 42us/step - loss: 14.5125 - acc: 0.5000 - val_loss: 43.2641 - val_acc: 0.6400\n",
      "Epoch 245/500\n",
      "256/256 [==============================] - 0s 39us/step - loss: 14.4991 - acc: 0.5000 - val_loss: 43.2118 - val_acc: 0.6400\n",
      "Epoch 246/500\n",
      "256/256 [==============================] - 0s 54us/step - loss: 14.4857 - acc: 0.5000 - val_loss: 43.1594 - val_acc: 0.6400\n",
      "Epoch 247/500\n",
      "256/256 [==============================] - 0s 43us/step - loss: 14.4724 - acc: 0.5000 - val_loss: 43.1070 - val_acc: 0.6400\n",
      "Epoch 248/500\n",
      "256/256 [==============================] - 0s 70us/step - loss: 14.4590 - acc: 0.5000 - val_loss: 43.0546 - val_acc: 0.6400\n",
      "Epoch 249/500\n",
      "256/256 [==============================] - 0s 79us/step - loss: 14.4456 - acc: 0.5000 - val_loss: 43.0018 - val_acc: 0.6400\n",
      "Epoch 250/500\n",
      "256/256 [==============================] - 0s 69us/step - loss: 14.4323 - acc: 0.5000 - val_loss: 42.9494 - val_acc: 0.6400\n",
      "Epoch 251/500\n",
      "256/256 [==============================] - 0s 65us/step - loss: 14.4190 - acc: 0.5000 - val_loss: 42.8969 - val_acc: 0.6400\n",
      "Epoch 252/500\n",
      "256/256 [==============================] - 0s 69us/step - loss: 14.4056 - acc: 0.5000 - val_loss: 42.8446 - val_acc: 0.6400\n",
      "Epoch 253/500\n",
      "256/256 [==============================] - 0s 61us/step - loss: 14.3924 - acc: 0.5000 - val_loss: 42.7923 - val_acc: 0.6400\n",
      "Epoch 254/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 14.3791 - acc: 0.5000 - val_loss: 42.7400 - val_acc: 0.6400\n",
      "Epoch 255/500\n",
      "256/256 [==============================] - 0s 62us/step - loss: 14.3658 - acc: 0.5000 - val_loss: 42.6875 - val_acc: 0.6400\n",
      "Epoch 256/500\n",
      "256/256 [==============================] - 0s 72us/step - loss: 14.3525 - acc: 0.5000 - val_loss: 42.6351 - val_acc: 0.6400\n",
      "Epoch 257/500\n",
      "256/256 [==============================] - 0s 76us/step - loss: 14.3392 - acc: 0.5000 - val_loss: 42.5826 - val_acc: 0.6400\n",
      "Epoch 258/500\n",
      "256/256 [==============================] - 0s 59us/step - loss: 14.3260 - acc: 0.5000 - val_loss: 42.5300 - val_acc: 0.6400\n",
      "Epoch 259/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 14.3127 - acc: 0.5000 - val_loss: 42.4775 - val_acc: 0.6400\n",
      "Epoch 260/500\n",
      "256/256 [==============================] - 0s 46us/step - loss: 14.2995 - acc: 0.5000 - val_loss: 42.4249 - val_acc: 0.6400\n",
      "Epoch 261/500\n",
      "256/256 [==============================] - 0s 28us/step - loss: 14.2862 - acc: 0.5000 - val_loss: 42.3723 - val_acc: 0.6400\n",
      "Epoch 262/500\n",
      "256/256 [==============================] - 0s 37us/step - loss: 14.2731 - acc: 0.5000 - val_loss: 42.3196 - val_acc: 0.6400\n",
      "Epoch 263/500\n",
      "256/256 [==============================] - 0s 38us/step - loss: 14.2598 - acc: 0.5000 - val_loss: 42.2669 - val_acc: 0.6400\n",
      "Epoch 264/500\n",
      "256/256 [==============================] - 0s 55us/step - loss: 14.2466 - acc: 0.5000 - val_loss: 42.2142 - val_acc: 0.6400\n",
      "Epoch 265/500\n",
      "256/256 [==============================] - 0s 55us/step - loss: 14.2335 - acc: 0.5000 - val_loss: 42.1616 - val_acc: 0.6400\n",
      "Epoch 266/500\n",
      "256/256 [==============================] - 0s 57us/step - loss: 14.2203 - acc: 0.5000 - val_loss: 42.1091 - val_acc: 0.6400\n",
      "Epoch 267/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 14.2072 - acc: 0.5000 - val_loss: 42.0568 - val_acc: 0.6400\n",
      "Epoch 268/500\n",
      "256/256 [==============================] - 0s 68us/step - loss: 14.1940 - acc: 0.5000 - val_loss: 42.0041 - val_acc: 0.6400\n",
      "Epoch 269/500\n",
      "256/256 [==============================] - 0s 41us/step - loss: 14.1809 - acc: 0.5000 - val_loss: 41.9512 - val_acc: 0.6400\n",
      "Epoch 270/500\n",
      "256/256 [==============================] - 0s 50us/step - loss: 14.1677 - acc: 0.5000 - val_loss: 41.8981 - val_acc: 0.6400\n",
      "Epoch 271/500\n",
      "256/256 [==============================] - 0s 50us/step - loss: 14.1546 - acc: 0.5000 - val_loss: 41.8452 - val_acc: 0.6400\n",
      "Epoch 272/500\n",
      "256/256 [==============================] - 0s 54us/step - loss: 14.1415 - acc: 0.5000 - val_loss: 41.7922 - val_acc: 0.6400\n",
      "Epoch 273/500\n",
      "256/256 [==============================] - 0s 62us/step - loss: 14.1284 - acc: 0.5000 - val_loss: 41.7393 - val_acc: 0.6400\n",
      "Epoch 274/500\n",
      "256/256 [==============================] - 0s 55us/step - loss: 14.1153 - acc: 0.5000 - val_loss: 41.6862 - val_acc: 0.6400\n",
      "Epoch 275/500\n",
      "256/256 [==============================] - 0s 52us/step - loss: 14.1023 - acc: 0.5000 - val_loss: 41.6333 - val_acc: 0.6400\n",
      "Epoch 276/500\n",
      "256/256 [==============================] - 0s 64us/step - loss: 14.0892 - acc: 0.5000 - val_loss: 41.5804 - val_acc: 0.6400\n",
      "Epoch 277/500\n",
      "256/256 [==============================] - 0s 44us/step - loss: 14.0761 - acc: 0.5000 - val_loss: 41.5275 - val_acc: 0.6400\n",
      "Epoch 278/500\n",
      "256/256 [==============================] - 0s 54us/step - loss: 14.0632 - acc: 0.5000 - val_loss: 41.4748 - val_acc: 0.6400\n",
      "Epoch 279/500\n",
      "256/256 [==============================] - 0s 49us/step - loss: 14.0501 - acc: 0.5000 - val_loss: 41.4219 - val_acc: 0.6400\n",
      "Epoch 280/500\n",
      "256/256 [==============================] - 0s 52us/step - loss: 14.0371 - acc: 0.5000 - val_loss: 41.3687 - val_acc: 0.6400\n",
      "Epoch 281/500\n",
      "256/256 [==============================] - 0s 23us/step - loss: 14.0241 - acc: 0.5000 - val_loss: 41.3154 - val_acc: 0.6400\n",
      "Epoch 282/500\n",
      "256/256 [==============================] - 0s 54us/step - loss: 14.0112 - acc: 0.5000 - val_loss: 41.2620 - val_acc: 0.6400\n",
      "Epoch 283/500\n",
      "256/256 [==============================] - 0s 39us/step - loss: 13.9981 - acc: 0.5000 - val_loss: 41.2087 - val_acc: 0.6400\n",
      "Epoch 284/500\n",
      "256/256 [==============================] - 0s 38us/step - loss: 13.9853 - acc: 0.5000 - val_loss: 41.1554 - val_acc: 0.6400\n",
      "Epoch 285/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 13.9722 - acc: 0.5000 - val_loss: 41.1025 - val_acc: 0.6400\n",
      "Epoch 286/500\n",
      "256/256 [==============================] - 0s 57us/step - loss: 13.9594 - acc: 0.5000 - val_loss: 41.0495 - val_acc: 0.6400\n",
      "Epoch 287/500\n",
      "256/256 [==============================] - 0s 50us/step - loss: 13.9464 - acc: 0.5000 - val_loss: 40.9963 - val_acc: 0.6400\n",
      "Epoch 288/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 13.9335 - acc: 0.5000 - val_loss: 40.9432 - val_acc: 0.6400\n",
      "Epoch 289/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 13.9206 - acc: 0.5000 - val_loss: 40.8902 - val_acc: 0.6400\n",
      "Epoch 290/500\n",
      "256/256 [==============================] - 0s 60us/step - loss: 13.9078 - acc: 0.5000 - val_loss: 40.8376 - val_acc: 0.6400\n",
      "Epoch 291/500\n",
      "256/256 [==============================] - 0s 40us/step - loss: 13.8949 - acc: 0.5000 - val_loss: 40.7850 - val_acc: 0.6400\n",
      "Epoch 292/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 13.8821 - acc: 0.5000 - val_loss: 40.7323 - val_acc: 0.6400\n",
      "Epoch 293/500\n",
      "256/256 [==============================] - 0s 25us/step - loss: 13.8692 - acc: 0.5000 - val_loss: 40.6797 - val_acc: 0.6400\n",
      "Epoch 294/500\n",
      "256/256 [==============================] - 0s 16us/step - loss: 13.8565 - acc: 0.5000 - val_loss: 40.6270 - val_acc: 0.6400\n",
      "Epoch 295/500\n",
      "256/256 [==============================] - 0s 41us/step - loss: 13.8436 - acc: 0.5000 - val_loss: 40.5744 - val_acc: 0.6400\n",
      "Epoch 296/500\n",
      "256/256 [==============================] - 0s 46us/step - loss: 13.8308 - acc: 0.5000 - val_loss: 40.5217 - val_acc: 0.6400\n",
      "Epoch 297/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 13.8181 - acc: 0.5000 - val_loss: 40.4691 - val_acc: 0.6400\n",
      "Epoch 298/500\n",
      "256/256 [==============================] - 0s 60us/step - loss: 13.8053 - acc: 0.5000 - val_loss: 40.4163 - val_acc: 0.6400\n",
      "Epoch 299/500\n",
      "256/256 [==============================] - 0s 67us/step - loss: 13.7926 - acc: 0.5000 - val_loss: 40.3633 - val_acc: 0.6400\n",
      "Epoch 300/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 13.7798 - acc: 0.5000 - val_loss: 40.3102 - val_acc: 0.6400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 301/500\n",
      "256/256 [==============================] - 0s 79us/step - loss: 13.7671 - acc: 0.5000 - val_loss: 40.2570 - val_acc: 0.6400\n",
      "Epoch 302/500\n",
      "256/256 [==============================] - 0s 52us/step - loss: 13.7544 - acc: 0.5000 - val_loss: 40.2038 - val_acc: 0.6400\n",
      "Epoch 303/500\n",
      "256/256 [==============================] - 0s 28us/step - loss: 13.7417 - acc: 0.5000 - val_loss: 40.1507 - val_acc: 0.6400\n",
      "Epoch 304/500\n",
      "256/256 [==============================] - 0s 30us/step - loss: 13.7290 - acc: 0.5000 - val_loss: 40.0971 - val_acc: 0.6400\n",
      "Epoch 305/500\n",
      "256/256 [==============================] - 0s 25us/step - loss: 13.7164 - acc: 0.5000 - val_loss: 40.0440 - val_acc: 0.6400\n",
      "Epoch 306/500\n",
      "256/256 [==============================] - 0s 29us/step - loss: 13.7038 - acc: 0.5000 - val_loss: 39.9906 - val_acc: 0.6400\n",
      "Epoch 307/500\n",
      "256/256 [==============================] - 0s 38us/step - loss: 13.6911 - acc: 0.5000 - val_loss: 39.9375 - val_acc: 0.6400\n",
      "Epoch 308/500\n",
      "256/256 [==============================] - 0s 41us/step - loss: 13.6786 - acc: 0.5000 - val_loss: 39.8842 - val_acc: 0.6400\n",
      "Epoch 309/500\n",
      "256/256 [==============================] - 0s 61us/step - loss: 13.6660 - acc: 0.5000 - val_loss: 39.8313 - val_acc: 0.6400\n",
      "Epoch 310/500\n",
      "256/256 [==============================] - 0s 55us/step - loss: 13.6534 - acc: 0.5000 - val_loss: 39.7788 - val_acc: 0.6400\n",
      "Epoch 311/500\n",
      "256/256 [==============================] - 0s 54us/step - loss: 13.6408 - acc: 0.5000 - val_loss: 39.7266 - val_acc: 0.6400\n",
      "Epoch 312/500\n",
      "256/256 [==============================] - 0s 63us/step - loss: 13.6283 - acc: 0.5000 - val_loss: 39.6742 - val_acc: 0.6400\n",
      "Epoch 313/500\n",
      "256/256 [==============================] - 0s 60us/step - loss: 13.6157 - acc: 0.5000 - val_loss: 39.6218 - val_acc: 0.6400\n",
      "Epoch 314/500\n",
      "256/256 [==============================] - 0s 56us/step - loss: 13.6031 - acc: 0.5000 - val_loss: 39.5693 - val_acc: 0.6400\n",
      "Epoch 315/500\n",
      "256/256 [==============================] - 0s 66us/step - loss: 13.5907 - acc: 0.5000 - val_loss: 39.5171 - val_acc: 0.6400\n",
      "Epoch 316/500\n",
      "256/256 [==============================] - 0s 60us/step - loss: 13.5782 - acc: 0.5000 - val_loss: 39.4648 - val_acc: 0.6400\n",
      "Epoch 317/500\n",
      "256/256 [==============================] - 0s 59us/step - loss: 13.5658 - acc: 0.5000 - val_loss: 39.4126 - val_acc: 0.6400\n",
      "Epoch 318/500\n",
      "256/256 [==============================] - 0s 68us/step - loss: 13.5532 - acc: 0.5000 - val_loss: 39.3603 - val_acc: 0.6400\n",
      "Epoch 319/500\n",
      "256/256 [==============================] - 0s 55us/step - loss: 13.5408 - acc: 0.5000 - val_loss: 39.3081 - val_acc: 0.6400\n",
      "Epoch 320/500\n",
      "256/256 [==============================] - 0s 56us/step - loss: 13.5284 - acc: 0.5000 - val_loss: 39.2560 - val_acc: 0.6400\n",
      "Epoch 321/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 13.5159 - acc: 0.5000 - val_loss: 39.2036 - val_acc: 0.6400\n",
      "Epoch 322/500\n",
      "256/256 [==============================] - 0s 67us/step - loss: 13.5036 - acc: 0.5000 - val_loss: 39.1510 - val_acc: 0.6400\n",
      "Epoch 323/500\n",
      "256/256 [==============================] - 0s 49us/step - loss: 13.4912 - acc: 0.5000 - val_loss: 39.0983 - val_acc: 0.6400\n",
      "Epoch 324/500\n",
      "256/256 [==============================] - 0s 59us/step - loss: 13.4788 - acc: 0.5000 - val_loss: 39.0457 - val_acc: 0.6400\n",
      "Epoch 325/500\n",
      "256/256 [==============================] - 0s 41us/step - loss: 13.4665 - acc: 0.5000 - val_loss: 38.9929 - val_acc: 0.6400\n",
      "Epoch 326/500\n",
      "256/256 [==============================] - 0s 39us/step - loss: 13.4542 - acc: 0.5000 - val_loss: 38.9399 - val_acc: 0.6400\n",
      "Epoch 327/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 13.4418 - acc: 0.5000 - val_loss: 38.8868 - val_acc: 0.6400\n",
      "Epoch 328/500\n",
      "256/256 [==============================] - 0s 37us/step - loss: 13.4295 - acc: 0.5000 - val_loss: 38.8337 - val_acc: 0.6400\n",
      "Epoch 329/500\n",
      "256/256 [==============================] - 0s 59us/step - loss: 13.4173 - acc: 0.5000 - val_loss: 38.7811 - val_acc: 0.6400\n",
      "Epoch 330/500\n",
      "256/256 [==============================] - 0s 61us/step - loss: 13.4050 - acc: 0.5000 - val_loss: 38.7286 - val_acc: 0.6400\n",
      "Epoch 331/500\n",
      "256/256 [==============================] - 0s 58us/step - loss: 13.3927 - acc: 0.5000 - val_loss: 38.6762 - val_acc: 0.6400\n",
      "Epoch 332/500\n",
      "256/256 [==============================] - 0s 70us/step - loss: 13.3805 - acc: 0.5000 - val_loss: 38.6240 - val_acc: 0.6400\n",
      "Epoch 333/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 13.3683 - acc: 0.5000 - val_loss: 38.5717 - val_acc: 0.6400\n",
      "Epoch 334/500\n",
      "256/256 [==============================] - 0s 57us/step - loss: 13.3561 - acc: 0.5000 - val_loss: 38.5196 - val_acc: 0.6400\n",
      "Epoch 335/500\n",
      "256/256 [==============================] - 0s 49us/step - loss: 13.3439 - acc: 0.5000 - val_loss: 38.4674 - val_acc: 0.6400\n",
      "Epoch 336/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 13.3318 - acc: 0.5000 - val_loss: 38.4153 - val_acc: 0.6400\n",
      "Epoch 337/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 13.3196 - acc: 0.5000 - val_loss: 38.3635 - val_acc: 0.6400\n",
      "Epoch 338/500\n",
      "256/256 [==============================] - 0s 59us/step - loss: 13.3075 - acc: 0.5000 - val_loss: 38.3116 - val_acc: 0.6400\n",
      "Epoch 339/500\n",
      "256/256 [==============================] - 0s 65us/step - loss: 13.2955 - acc: 0.5000 - val_loss: 38.2599 - val_acc: 0.6400\n",
      "Epoch 340/500\n",
      "256/256 [==============================] - 0s 39us/step - loss: 13.2833 - acc: 0.5000 - val_loss: 38.2083 - val_acc: 0.6400\n",
      "Epoch 341/500\n",
      "256/256 [==============================] - 0s 31us/step - loss: 13.2712 - acc: 0.5000 - val_loss: 38.1567 - val_acc: 0.6400\n",
      "Epoch 342/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 13.2591 - acc: 0.5000 - val_loss: 38.1055 - val_acc: 0.6400\n",
      "Epoch 343/500\n",
      "256/256 [==============================] - 0s 44us/step - loss: 13.2470 - acc: 0.5000 - val_loss: 38.0542 - val_acc: 0.6400\n",
      "Epoch 344/500\n",
      "256/256 [==============================] - 0s 56us/step - loss: 13.2351 - acc: 0.5000 - val_loss: 38.0030 - val_acc: 0.6400\n",
      "Epoch 345/500\n",
      "256/256 [==============================] - 0s 49us/step - loss: 13.2231 - acc: 0.5000 - val_loss: 37.9514 - val_acc: 0.6400\n",
      "Epoch 346/500\n",
      "256/256 [==============================] - 0s 56us/step - loss: 13.2112 - acc: 0.5000 - val_loss: 37.8999 - val_acc: 0.6400\n",
      "Epoch 347/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 13.1992 - acc: 0.5000 - val_loss: 37.8486 - val_acc: 0.6400\n",
      "Epoch 348/500\n",
      "256/256 [==============================] - 0s 45us/step - loss: 13.1873 - acc: 0.5000 - val_loss: 37.7977 - val_acc: 0.6400\n",
      "Epoch 349/500\n",
      "256/256 [==============================] - 0s 49us/step - loss: 13.1752 - acc: 0.5000 - val_loss: 37.7467 - val_acc: 0.6400\n",
      "Epoch 350/500\n",
      "256/256 [==============================] - 0s 39us/step - loss: 13.1633 - acc: 0.5000 - val_loss: 37.6954 - val_acc: 0.6400\n",
      "Epoch 351/500\n",
      "256/256 [==============================] - 0s 60us/step - loss: 13.1514 - acc: 0.5000 - val_loss: 37.6444 - val_acc: 0.6400\n",
      "Epoch 352/500\n",
      "256/256 [==============================] - 0s 35us/step - loss: 13.1396 - acc: 0.5000 - val_loss: 37.5932 - val_acc: 0.6400\n",
      "Epoch 353/500\n",
      "256/256 [==============================] - 0s 17us/step - loss: 13.1277 - acc: 0.5000 - val_loss: 37.5415 - val_acc: 0.6400\n",
      "Epoch 354/500\n",
      "256/256 [==============================] - 0s 43us/step - loss: 13.1158 - acc: 0.5000 - val_loss: 37.4897 - val_acc: 0.6400\n",
      "Epoch 355/500\n",
      "256/256 [==============================] - 0s 36us/step - loss: 13.1041 - acc: 0.5000 - val_loss: 37.4382 - val_acc: 0.6400\n",
      "Epoch 356/500\n",
      "256/256 [==============================] - 0s 46us/step - loss: 13.0922 - acc: 0.5000 - val_loss: 37.3869 - val_acc: 0.6400\n",
      "Epoch 357/500\n",
      "256/256 [==============================] - 0s 34us/step - loss: 13.0805 - acc: 0.5000 - val_loss: 37.3357 - val_acc: 0.6400\n",
      "Epoch 358/500\n",
      "256/256 [==============================] - 0s 30us/step - loss: 13.0688 - acc: 0.5000 - val_loss: 37.2847 - val_acc: 0.6400\n",
      "Epoch 359/500\n",
      "256/256 [==============================] - 0s 31us/step - loss: 13.0569 - acc: 0.5000 - val_loss: 37.2337 - val_acc: 0.6400\n",
      "Epoch 360/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 13.0452 - acc: 0.5000 - val_loss: 37.1827 - val_acc: 0.6400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 361/500\n",
      "256/256 [==============================] - 0s 41us/step - loss: 13.0335 - acc: 0.5000 - val_loss: 37.1316 - val_acc: 0.6400\n",
      "Epoch 362/500\n",
      "256/256 [==============================] - 0s 56us/step - loss: 13.0218 - acc: 0.5000 - val_loss: 37.0807 - val_acc: 0.6400\n",
      "Epoch 363/500\n",
      "256/256 [==============================] - 0s 56us/step - loss: 13.0102 - acc: 0.5000 - val_loss: 37.0299 - val_acc: 0.6400\n",
      "Epoch 364/500\n",
      "256/256 [==============================] - 0s 49us/step - loss: 12.9985 - acc: 0.5000 - val_loss: 36.9791 - val_acc: 0.6400\n",
      "Epoch 365/500\n",
      "256/256 [==============================] - 0s 55us/step - loss: 12.9868 - acc: 0.5000 - val_loss: 36.9281 - val_acc: 0.6400\n",
      "Epoch 366/500\n",
      "256/256 [==============================] - 0s 35us/step - loss: 12.9752 - acc: 0.5000 - val_loss: 36.8775 - val_acc: 0.6400\n",
      "Epoch 367/500\n",
      "256/256 [==============================] - 0s 43us/step - loss: 12.9636 - acc: 0.5000 - val_loss: 36.8272 - val_acc: 0.6400\n",
      "Epoch 368/500\n",
      "256/256 [==============================] - 0s 40us/step - loss: 12.9519 - acc: 0.5000 - val_loss: 36.7771 - val_acc: 0.6400\n",
      "Epoch 369/500\n",
      "256/256 [==============================] - 0s 46us/step - loss: 12.9404 - acc: 0.5000 - val_loss: 36.7263 - val_acc: 0.6400\n",
      "Epoch 370/500\n",
      "256/256 [==============================] - 0s 41us/step - loss: 12.9288 - acc: 0.5000 - val_loss: 36.6752 - val_acc: 0.6400\n",
      "Epoch 371/500\n",
      "256/256 [==============================] - 0s 61us/step - loss: 12.9172 - acc: 0.5000 - val_loss: 36.6242 - val_acc: 0.6400\n",
      "Epoch 372/500\n",
      "256/256 [==============================] - 0s 62us/step - loss: 12.9058 - acc: 0.5000 - val_loss: 36.5735 - val_acc: 0.6400\n",
      "Epoch 373/500\n",
      "256/256 [==============================] - 0s 65us/step - loss: 12.8942 - acc: 0.5000 - val_loss: 36.5228 - val_acc: 0.6400\n",
      "Epoch 374/500\n",
      "256/256 [==============================] - 0s 68us/step - loss: 12.8828 - acc: 0.5000 - val_loss: 36.4723 - val_acc: 0.6400\n",
      "Epoch 375/500\n",
      "256/256 [==============================] - 0s 50us/step - loss: 12.8714 - acc: 0.5000 - val_loss: 36.4217 - val_acc: 0.6400\n",
      "Epoch 376/500\n",
      "256/256 [==============================] - 0s 54us/step - loss: 12.8598 - acc: 0.5000 - val_loss: 36.3709 - val_acc: 0.6400\n",
      "Epoch 377/500\n",
      "256/256 [==============================] - 0s 26us/step - loss: 12.8483 - acc: 0.5000 - val_loss: 36.3202 - val_acc: 0.6400\n",
      "Epoch 378/500\n",
      "256/256 [==============================] - 0s 37us/step - loss: 12.8369 - acc: 0.5000 - val_loss: 36.2694 - val_acc: 0.6400\n",
      "Epoch 379/500\n",
      "256/256 [==============================] - 0s 42us/step - loss: 12.8255 - acc: 0.5000 - val_loss: 36.2192 - val_acc: 0.6400\n",
      "Epoch 380/500\n",
      "256/256 [==============================] - 0s 30us/step - loss: 12.8142 - acc: 0.5000 - val_loss: 36.1693 - val_acc: 0.6400\n",
      "Epoch 381/500\n",
      "256/256 [==============================] - 0s 70us/step - loss: 12.8027 - acc: 0.5000 - val_loss: 36.1191 - val_acc: 0.6400\n",
      "Epoch 382/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 12.7913 - acc: 0.5000 - val_loss: 36.0689 - val_acc: 0.6400\n",
      "Epoch 383/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 12.7800 - acc: 0.5000 - val_loss: 36.0190 - val_acc: 0.6400\n",
      "Epoch 384/500\n",
      "256/256 [==============================] - 0s 63us/step - loss: 12.7688 - acc: 0.5000 - val_loss: 35.9688 - val_acc: 0.6400\n",
      "Epoch 385/500\n",
      "256/256 [==============================] - 0s 78us/step - loss: 12.7574 - acc: 0.5000 - val_loss: 35.9185 - val_acc: 0.6400\n",
      "Epoch 386/500\n",
      "256/256 [==============================] - 0s 26us/step - loss: 12.7461 - acc: 0.5000 - val_loss: 35.8681 - val_acc: 0.6400\n",
      "Epoch 387/500\n",
      "256/256 [==============================] - 0s 59us/step - loss: 12.7347 - acc: 0.5000 - val_loss: 35.8175 - val_acc: 0.6400\n",
      "Epoch 388/500\n",
      "256/256 [==============================] - 0s 29us/step - loss: 12.7236 - acc: 0.5000 - val_loss: 35.7665 - val_acc: 0.6400\n",
      "Epoch 389/500\n",
      "256/256 [==============================] - 0s 44us/step - loss: 12.7123 - acc: 0.5000 - val_loss: 35.7155 - val_acc: 0.6400\n",
      "Epoch 390/500\n",
      "256/256 [==============================] - 0s 31us/step - loss: 12.7012 - acc: 0.5000 - val_loss: 35.6644 - val_acc: 0.6400\n",
      "Epoch 391/500\n",
      "256/256 [==============================] - 0s 57us/step - loss: 12.6899 - acc: 0.5000 - val_loss: 35.6136 - val_acc: 0.6400\n",
      "Epoch 392/500\n",
      "256/256 [==============================] - 0s 54us/step - loss: 12.6787 - acc: 0.5000 - val_loss: 35.5630 - val_acc: 0.6400\n",
      "Epoch 393/500\n",
      "256/256 [==============================] - 0s 60us/step - loss: 12.6676 - acc: 0.5000 - val_loss: 35.5125 - val_acc: 0.6400\n",
      "Epoch 394/500\n",
      "256/256 [==============================] - 0s 45us/step - loss: 12.6563 - acc: 0.5000 - val_loss: 35.4621 - val_acc: 0.6400\n",
      "Epoch 395/500\n",
      "256/256 [==============================] - 0s 60us/step - loss: 12.6452 - acc: 0.5000 - val_loss: 35.4115 - val_acc: 0.6400\n",
      "Epoch 396/500\n",
      "256/256 [==============================] - 0s 58us/step - loss: 12.6341 - acc: 0.5000 - val_loss: 35.3609 - val_acc: 0.6400\n",
      "Epoch 397/500\n",
      "256/256 [==============================] - 0s 56us/step - loss: 12.6230 - acc: 0.5000 - val_loss: 35.3101 - val_acc: 0.6400\n",
      "Epoch 398/500\n",
      "256/256 [==============================] - 0s 55us/step - loss: 12.6120 - acc: 0.5000 - val_loss: 35.2597 - val_acc: 0.6400\n",
      "Epoch 399/500\n",
      "256/256 [==============================] - 0s 70us/step - loss: 12.6008 - acc: 0.5000 - val_loss: 35.2095 - val_acc: 0.6400\n",
      "Epoch 400/500\n",
      "256/256 [==============================] - 0s 58us/step - loss: 12.5898 - acc: 0.5000 - val_loss: 35.1595 - val_acc: 0.6400\n",
      "Epoch 401/500\n",
      "256/256 [==============================] - 0s 56us/step - loss: 12.5788 - acc: 0.5000 - val_loss: 35.1097 - val_acc: 0.6400\n",
      "Epoch 402/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 12.5679 - acc: 0.5000 - val_loss: 35.0601 - val_acc: 0.6400\n",
      "Epoch 403/500\n",
      "256/256 [==============================] - 0s 58us/step - loss: 12.5569 - acc: 0.5000 - val_loss: 35.0101 - val_acc: 0.6400\n",
      "Epoch 404/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 12.5459 - acc: 0.5000 - val_loss: 34.9602 - val_acc: 0.6400\n",
      "Epoch 405/500\n",
      "256/256 [==============================] - 0s 45us/step - loss: 12.5351 - acc: 0.5000 - val_loss: 34.9109 - val_acc: 0.6400\n",
      "Epoch 406/500\n",
      "256/256 [==============================] - 0s 75us/step - loss: 12.5241 - acc: 0.5000 - val_loss: 34.8611 - val_acc: 0.6400\n",
      "Epoch 407/500\n",
      "256/256 [==============================] - 0s 58us/step - loss: 12.5132 - acc: 0.5000 - val_loss: 34.8116 - val_acc: 0.6400\n",
      "Epoch 408/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 12.5023 - acc: 0.5000 - val_loss: 34.7620 - val_acc: 0.6400\n",
      "Epoch 409/500\n",
      "256/256 [==============================] - 0s 27us/step - loss: 12.4915 - acc: 0.5000 - val_loss: 34.7126 - val_acc: 0.6400\n",
      "Epoch 410/500\n",
      "256/256 [==============================] - 0s 41us/step - loss: 12.4807 - acc: 0.5000 - val_loss: 34.6635 - val_acc: 0.6400\n",
      "Epoch 411/500\n",
      "256/256 [==============================] - 0s 27us/step - loss: 12.4698 - acc: 0.5000 - val_loss: 34.6148 - val_acc: 0.6400\n",
      "Epoch 412/500\n",
      "256/256 [==============================] - 0s 27us/step - loss: 12.4590 - acc: 0.5000 - val_loss: 34.5662 - val_acc: 0.6400\n",
      "Epoch 413/500\n",
      "256/256 [==============================] - 0s 28us/step - loss: 12.4482 - acc: 0.5000 - val_loss: 34.5173 - val_acc: 0.6400\n",
      "Epoch 414/500\n",
      "256/256 [==============================] - 0s 34us/step - loss: 12.4375 - acc: 0.5000 - val_loss: 34.4687 - val_acc: 0.6400\n",
      "Epoch 415/500\n",
      "256/256 [==============================] - 0s 39us/step - loss: 12.4268 - acc: 0.5000 - val_loss: 34.4206 - val_acc: 0.6400\n",
      "Epoch 416/500\n",
      "256/256 [==============================] - 0s 45us/step - loss: 12.4161 - acc: 0.5000 - val_loss: 34.3720 - val_acc: 0.6400\n",
      "Epoch 417/500\n",
      "256/256 [==============================] - 0s 58us/step - loss: 12.4054 - acc: 0.5000 - val_loss: 34.3227 - val_acc: 0.6400\n",
      "Epoch 418/500\n",
      "256/256 [==============================] - 0s 41us/step - loss: 12.3947 - acc: 0.5000 - val_loss: 34.2733 - val_acc: 0.6400\n",
      "Epoch 419/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 12.3840 - acc: 0.5000 - val_loss: 34.2240 - val_acc: 0.6400\n",
      "Epoch 420/500\n",
      "256/256 [==============================] - 0s 64us/step - loss: 12.3734 - acc: 0.5000 - val_loss: 34.1744 - val_acc: 0.6400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 421/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 12.3627 - acc: 0.5000 - val_loss: 34.1249 - val_acc: 0.6400\n",
      "Epoch 422/500\n",
      "256/256 [==============================] - 0s 49us/step - loss: 12.3522 - acc: 0.5000 - val_loss: 34.0750 - val_acc: 0.6400\n",
      "Epoch 423/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 12.3416 - acc: 0.5000 - val_loss: 34.0253 - val_acc: 0.6400\n",
      "Epoch 424/500\n",
      "256/256 [==============================] - 0s 43us/step - loss: 12.3310 - acc: 0.5000 - val_loss: 33.9757 - val_acc: 0.6400\n",
      "Epoch 425/500\n",
      "256/256 [==============================] - 0s 30us/step - loss: 12.3205 - acc: 0.5000 - val_loss: 33.9261 - val_acc: 0.6400\n",
      "Epoch 426/500\n",
      "256/256 [==============================] - 0s 23us/step - loss: 12.3099 - acc: 0.5000 - val_loss: 33.8763 - val_acc: 0.6400\n",
      "Epoch 427/500\n",
      "256/256 [==============================] - 0s 27us/step - loss: 12.2994 - acc: 0.5000 - val_loss: 33.8269 - val_acc: 0.6400\n",
      "Epoch 428/500\n",
      "256/256 [==============================] - 0s 45us/step - loss: 12.2889 - acc: 0.5000 - val_loss: 33.7776 - val_acc: 0.6400\n",
      "Epoch 429/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 12.2785 - acc: 0.5000 - val_loss: 33.7285 - val_acc: 0.6400\n",
      "Epoch 430/500\n",
      "256/256 [==============================] - 0s 70us/step - loss: 12.2680 - acc: 0.5000 - val_loss: 33.6796 - val_acc: 0.6400\n",
      "Epoch 431/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 12.2576 - acc: 0.5000 - val_loss: 33.6310 - val_acc: 0.6400\n",
      "Epoch 432/500\n",
      "256/256 [==============================] - 0s 71us/step - loss: 12.2472 - acc: 0.5000 - val_loss: 33.5824 - val_acc: 0.6400\n",
      "Epoch 433/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 12.2368 - acc: 0.5000 - val_loss: 33.5337 - val_acc: 0.6400\n",
      "Epoch 434/500\n",
      "256/256 [==============================] - 0s 44us/step - loss: 12.2265 - acc: 0.5000 - val_loss: 33.4852 - val_acc: 0.6400\n",
      "Epoch 435/500\n",
      "256/256 [==============================] - 0s 46us/step - loss: 12.2160 - acc: 0.5000 - val_loss: 33.4368 - val_acc: 0.6400\n",
      "Epoch 436/500\n",
      "256/256 [==============================] - 0s 69us/step - loss: 12.2058 - acc: 0.5000 - val_loss: 33.3883 - val_acc: 0.6400\n",
      "Epoch 437/500\n",
      "256/256 [==============================] - 0s 57us/step - loss: 12.1955 - acc: 0.5000 - val_loss: 33.3402 - val_acc: 0.6400\n",
      "Epoch 438/500\n",
      "256/256 [==============================] - 0s 46us/step - loss: 12.1851 - acc: 0.5000 - val_loss: 33.2919 - val_acc: 0.6400\n",
      "Epoch 439/500\n",
      "256/256 [==============================] - 0s 52us/step - loss: 12.1748 - acc: 0.5000 - val_loss: 33.2436 - val_acc: 0.6400\n",
      "Epoch 440/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 12.1646 - acc: 0.5000 - val_loss: 33.1954 - val_acc: 0.6400\n",
      "Epoch 441/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 12.1544 - acc: 0.5000 - val_loss: 33.1469 - val_acc: 0.6400\n",
      "Epoch 442/500\n",
      "256/256 [==============================] - 0s 44us/step - loss: 12.1441 - acc: 0.5000 - val_loss: 33.0985 - val_acc: 0.6400\n",
      "Epoch 443/500\n",
      "256/256 [==============================] - 0s 58us/step - loss: 12.1338 - acc: 0.5000 - val_loss: 33.0499 - val_acc: 0.6400\n",
      "Epoch 444/500\n",
      "256/256 [==============================] - 0s 57us/step - loss: 12.1238 - acc: 0.5000 - val_loss: 33.0012 - val_acc: 0.6400\n",
      "Epoch 445/500\n",
      "256/256 [==============================] - 0s 49us/step - loss: 12.1135 - acc: 0.5000 - val_loss: 32.9525 - val_acc: 0.6400\n",
      "Epoch 446/500\n",
      "256/256 [==============================] - 0s 64us/step - loss: 12.1034 - acc: 0.5000 - val_loss: 32.9037 - val_acc: 0.6400\n",
      "Epoch 447/500\n",
      "256/256 [==============================] - 0s 45us/step - loss: 12.0933 - acc: 0.5000 - val_loss: 32.8545 - val_acc: 0.6400\n",
      "Epoch 448/500\n",
      "256/256 [==============================] - 0s 32us/step - loss: 12.0832 - acc: 0.5000 - val_loss: 32.8055 - val_acc: 0.6400\n",
      "Epoch 449/500\n",
      "256/256 [==============================] - 0s 42us/step - loss: 12.0731 - acc: 0.5000 - val_loss: 32.7567 - val_acc: 0.6400\n",
      "Epoch 450/500\n",
      "256/256 [==============================] - 0s 29us/step - loss: 12.0631 - acc: 0.5000 - val_loss: 32.7080 - val_acc: 0.6400\n",
      "Epoch 451/500\n",
      "256/256 [==============================] - 0s 45us/step - loss: 12.0529 - acc: 0.5000 - val_loss: 32.6595 - val_acc: 0.6400\n",
      "Epoch 452/500\n",
      "256/256 [==============================] - 0s 46us/step - loss: 12.0428 - acc: 0.5000 - val_loss: 32.6110 - val_acc: 0.6400\n",
      "Epoch 453/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 12.0329 - acc: 0.5000 - val_loss: 32.5627 - val_acc: 0.6400\n",
      "Epoch 454/500\n",
      "256/256 [==============================] - 0s 25us/step - loss: 12.0229 - acc: 0.5000 - val_loss: 32.5142 - val_acc: 0.6400\n",
      "Epoch 455/500\n",
      "256/256 [==============================] - 0s 40us/step - loss: 12.0128 - acc: 0.5000 - val_loss: 32.4657 - val_acc: 0.6400\n",
      "Epoch 456/500\n",
      "256/256 [==============================] - 0s 25us/step - loss: 12.0028 - acc: 0.5000 - val_loss: 32.4178 - val_acc: 0.6400\n",
      "Epoch 457/500\n",
      "256/256 [==============================] - 0s 37us/step - loss: 11.9929 - acc: 0.5000 - val_loss: 32.3696 - val_acc: 0.6400\n",
      "Epoch 458/500\n",
      "256/256 [==============================] - 0s 60us/step - loss: 11.9830 - acc: 0.5000 - val_loss: 32.3216 - val_acc: 0.6400\n",
      "Epoch 459/500\n",
      "256/256 [==============================] - 0s 60us/step - loss: 11.9731 - acc: 0.5000 - val_loss: 32.2739 - val_acc: 0.6400\n",
      "Epoch 460/500\n",
      "256/256 [==============================] - 0s 64us/step - loss: 11.9631 - acc: 0.5000 - val_loss: 32.2267 - val_acc: 0.6400\n",
      "Epoch 461/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 11.9534 - acc: 0.5000 - val_loss: 32.1792 - val_acc: 0.6400\n",
      "Epoch 462/500\n",
      "256/256 [==============================] - 0s 42us/step - loss: 11.9435 - acc: 0.5000 - val_loss: 32.1319 - val_acc: 0.6400\n",
      "Epoch 463/500\n",
      "256/256 [==============================] - 0s 28us/step - loss: 11.9335 - acc: 0.5000 - val_loss: 32.0846 - val_acc: 0.6400\n",
      "Epoch 464/500\n",
      "256/256 [==============================] - 0s 32us/step - loss: 11.9237 - acc: 0.5000 - val_loss: 32.0368 - val_acc: 0.6400\n",
      "Epoch 465/500\n",
      "256/256 [==============================] - 0s 34us/step - loss: 11.9140 - acc: 0.5000 - val_loss: 31.9892 - val_acc: 0.6400\n",
      "Epoch 466/500\n",
      "256/256 [==============================] - 0s 39us/step - loss: 11.9042 - acc: 0.5000 - val_loss: 31.9416 - val_acc: 0.6400\n",
      "Epoch 467/500\n",
      "256/256 [==============================] - 0s 36us/step - loss: 11.8944 - acc: 0.5000 - val_loss: 31.8941 - val_acc: 0.6400\n",
      "Epoch 468/500\n",
      "256/256 [==============================] - 0s 50us/step - loss: 11.8847 - acc: 0.5000 - val_loss: 31.8467 - val_acc: 0.6400\n",
      "Epoch 469/500\n",
      "256/256 [==============================] - 0s 28us/step - loss: 11.8750 - acc: 0.5000 - val_loss: 31.7998 - val_acc: 0.6400\n",
      "Epoch 470/500\n",
      "256/256 [==============================] - 0s 62us/step - loss: 11.8652 - acc: 0.5000 - val_loss: 31.7526 - val_acc: 0.6400\n",
      "Epoch 471/500\n",
      "256/256 [==============================] - 0s 31us/step - loss: 11.8555 - acc: 0.5000 - val_loss: 31.7050 - val_acc: 0.6400\n",
      "Epoch 472/500\n",
      "256/256 [==============================] - 0s 33us/step - loss: 11.8460 - acc: 0.5000 - val_loss: 31.6571 - val_acc: 0.6400\n",
      "Epoch 473/500\n",
      "256/256 [==============================] - 0s 30us/step - loss: 11.8363 - acc: 0.5000 - val_loss: 31.6100 - val_acc: 0.6400\n",
      "Epoch 474/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 11.8266 - acc: 0.5000 - val_loss: 31.5629 - val_acc: 0.6400\n",
      "Epoch 475/500\n",
      "256/256 [==============================] - 0s 43us/step - loss: 11.8171 - acc: 0.5000 - val_loss: 31.5163 - val_acc: 0.6400\n",
      "Epoch 476/500\n",
      "256/256 [==============================] - 0s 42us/step - loss: 11.8075 - acc: 0.5000 - val_loss: 31.4693 - val_acc: 0.6400\n",
      "Epoch 477/500\n",
      "256/256 [==============================] - 0s 62us/step - loss: 11.7979 - acc: 0.5000 - val_loss: 31.4226 - val_acc: 0.6400\n",
      "Epoch 478/500\n",
      "256/256 [==============================] - 0s 54us/step - loss: 11.7884 - acc: 0.5000 - val_loss: 31.3758 - val_acc: 0.6400\n",
      "Epoch 479/500\n",
      "256/256 [==============================] - 0s 37us/step - loss: 11.7788 - acc: 0.5000 - val_loss: 31.3294 - val_acc: 0.6400\n",
      "Epoch 480/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 0s 42us/step - loss: 11.7693 - acc: 0.5000 - val_loss: 31.2828 - val_acc: 0.6400\n",
      "Epoch 481/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 11.7598 - acc: 0.5000 - val_loss: 31.2366 - val_acc: 0.6400\n",
      "Epoch 482/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 11.7504 - acc: 0.5000 - val_loss: 31.1908 - val_acc: 0.6400\n",
      "Epoch 483/500\n",
      "256/256 [==============================] - 0s 46us/step - loss: 11.7410 - acc: 0.5000 - val_loss: 31.1451 - val_acc: 0.6400\n",
      "Epoch 484/500\n",
      "256/256 [==============================] - 0s 63us/step - loss: 11.7314 - acc: 0.5000 - val_loss: 31.0996 - val_acc: 0.6400\n",
      "Epoch 485/500\n",
      "256/256 [==============================] - 0s 65us/step - loss: 11.7221 - acc: 0.5000 - val_loss: 31.0540 - val_acc: 0.6400\n",
      "Epoch 486/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 11.7127 - acc: 0.5000 - val_loss: 31.0079 - val_acc: 0.6400\n",
      "Epoch 487/500\n",
      "256/256 [==============================] - 0s 52us/step - loss: 11.7033 - acc: 0.5000 - val_loss: 30.9618 - val_acc: 0.6400\n",
      "Epoch 488/500\n",
      "256/256 [==============================] - 0s 60us/step - loss: 11.6941 - acc: 0.5000 - val_loss: 30.9157 - val_acc: 0.6400\n",
      "Epoch 489/500\n",
      "256/256 [==============================] - 0s 43us/step - loss: 11.6846 - acc: 0.5000 - val_loss: 30.8694 - val_acc: 0.6400\n",
      "Epoch 490/500\n",
      "256/256 [==============================] - 0s 36us/step - loss: 11.6753 - acc: 0.5000 - val_loss: 30.8232 - val_acc: 0.6400\n",
      "Epoch 491/500\n",
      "256/256 [==============================] - 0s 39us/step - loss: 11.6659 - acc: 0.5000 - val_loss: 30.7775 - val_acc: 0.6400\n",
      "Epoch 492/500\n",
      "256/256 [==============================] - 0s 36us/step - loss: 11.6568 - acc: 0.5000 - val_loss: 30.7317 - val_acc: 0.6400\n",
      "Epoch 493/500\n",
      "256/256 [==============================] - 0s 36us/step - loss: 11.6475 - acc: 0.5000 - val_loss: 30.6862 - val_acc: 0.6400\n",
      "Epoch 494/500\n",
      "256/256 [==============================] - 0s 54us/step - loss: 11.6382 - acc: 0.5000 - val_loss: 30.6406 - val_acc: 0.6400\n",
      "Epoch 495/500\n",
      "256/256 [==============================] - 0s 57us/step - loss: 11.6290 - acc: 0.5000 - val_loss: 30.5944 - val_acc: 0.6400\n",
      "Epoch 496/500\n",
      "256/256 [==============================] - 0s 49us/step - loss: 11.6198 - acc: 0.5000 - val_loss: 30.5483 - val_acc: 0.6400\n",
      "Epoch 497/500\n",
      "256/256 [==============================] - 0s 59us/step - loss: 11.6106 - acc: 0.5000 - val_loss: 30.5020 - val_acc: 0.6400\n",
      "Epoch 498/500\n",
      "256/256 [==============================] - 0s 58us/step - loss: 11.6015 - acc: 0.5000 - val_loss: 30.4560 - val_acc: 0.6400\n",
      "Epoch 499/500\n",
      "256/256 [==============================] - 0s 65us/step - loss: 11.5923 - acc: 0.5000 - val_loss: 30.4101 - val_acc: 0.6400\n",
      "Epoch 500/500\n",
      "256/256 [==============================] - 0s 71us/step - loss: 11.5832 - acc: 0.5000 - val_loss: 30.3641 - val_acc: 0.6400\n",
      "Train on 256 samples, validate on 50 samples\n",
      "Epoch 1/500\n",
      "256/256 [==============================] - 0s 29us/step - loss: 36.0249 - acc: 0.5000 - val_loss: 9.3304 - val_acc: 0.6400\n",
      "Epoch 2/500\n",
      "256/256 [==============================] - 0s 31us/step - loss: 34.8466 - acc: 0.5000 - val_loss: 9.3246 - val_acc: 0.6400\n",
      "Epoch 3/500\n",
      "256/256 [==============================] - 0s 59us/step - loss: 32.8776 - acc: 0.5000 - val_loss: 9.3418 - val_acc: 0.6400\n",
      "Epoch 4/500\n",
      "256/256 [==============================] - 0s 21us/step - loss: 30.4800 - acc: 0.5000 - val_loss: 9.3972 - val_acc: 0.6400\n",
      "Epoch 5/500\n",
      "256/256 [==============================] - 0s 54us/step - loss: 27.9649 - acc: 0.5000 - val_loss: 9.5014 - val_acc: 0.6400\n",
      "Epoch 6/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 25.5940 - acc: 0.5000 - val_loss: 9.6576 - val_acc: 0.6400\n",
      "Epoch 7/500\n",
      "256/256 [==============================] - 0s 57us/step - loss: 23.4244 - acc: 0.5000 - val_loss: 9.8629 - val_acc: 0.6400\n",
      "Epoch 8/500\n",
      "256/256 [==============================] - 0s 35us/step - loss: 21.4610 - acc: 0.5000 - val_loss: 10.1087 - val_acc: 0.6400\n",
      "Epoch 9/500\n",
      "256/256 [==============================] - 0s 31us/step - loss: 19.7332 - acc: 0.5000 - val_loss: 10.3855 - val_acc: 0.6400\n",
      "Epoch 10/500\n",
      "256/256 [==============================] - 0s 39us/step - loss: 18.2421 - acc: 0.5000 - val_loss: 10.6865 - val_acc: 0.6400\n",
      "Epoch 11/500\n",
      "256/256 [==============================] - 0s 34us/step - loss: 16.9482 - acc: 0.5000 - val_loss: 11.0019 - val_acc: 0.6400\n",
      "Epoch 12/500\n",
      "256/256 [==============================] - 0s 49us/step - loss: 15.8283 - acc: 0.5000 - val_loss: 11.3234 - val_acc: 0.6400\n",
      "Epoch 13/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 14.8693 - acc: 0.5000 - val_loss: 11.6416 - val_acc: 0.6400\n",
      "Epoch 14/500\n",
      "256/256 [==============================] - 0s 55us/step - loss: 14.0820 - acc: 0.5000 - val_loss: 11.9532 - val_acc: 0.6400\n",
      "Epoch 15/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 13.4508 - acc: 0.5000 - val_loss: 12.2554 - val_acc: 0.6400\n",
      "Epoch 16/500\n",
      "256/256 [==============================] - 0s 64us/step - loss: 12.9461 - acc: 0.5000 - val_loss: 12.5428 - val_acc: 0.6400\n",
      "Epoch 17/500\n",
      "256/256 [==============================] - 0s 24us/step - loss: 12.5414 - acc: 0.5000 - val_loss: 12.8163 - val_acc: 0.6400\n",
      "Epoch 18/500\n",
      "256/256 [==============================] - 0s 31us/step - loss: 12.2286 - acc: 0.5000 - val_loss: 13.0740 - val_acc: 0.6400\n",
      "Epoch 19/500\n",
      "256/256 [==============================] - 0s 41us/step - loss: 11.9832 - acc: 0.5000 - val_loss: 13.3159 - val_acc: 0.6400\n",
      "Epoch 20/500\n",
      "256/256 [==============================] - 0s 34us/step - loss: 11.7935 - acc: 0.5000 - val_loss: 13.5407 - val_acc: 0.6400\n",
      "Epoch 21/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 11.6553 - acc: 0.5000 - val_loss: 13.7475 - val_acc: 0.6400\n",
      "Epoch 22/500\n",
      "256/256 [==============================] - 0s 43us/step - loss: 11.5646 - acc: 0.5000 - val_loss: 13.9369 - val_acc: 0.6400\n",
      "Epoch 23/500\n",
      "256/256 [==============================] - 0s 65us/step - loss: 11.5038 - acc: 0.5000 - val_loss: 14.1077 - val_acc: 0.6400\n",
      "Epoch 24/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 11.4587 - acc: 0.5000 - val_loss: 14.2618 - val_acc: 0.6400\n",
      "Epoch 25/500\n",
      "256/256 [==============================] - 0s 61us/step - loss: 11.4273 - acc: 0.5000 - val_loss: 14.4002 - val_acc: 0.6400\n",
      "Epoch 26/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 11.4067 - acc: 0.5000 - val_loss: 14.5245 - val_acc: 0.6400\n",
      "Epoch 27/500\n",
      "256/256 [==============================] - 0s 57us/step - loss: 11.3921 - acc: 0.5000 - val_loss: 14.6372 - val_acc: 0.6400\n",
      "Epoch 28/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 11.3810 - acc: 0.5000 - val_loss: 14.7375 - val_acc: 0.6400\n",
      "Epoch 29/500\n",
      "256/256 [==============================] - 0s 57us/step - loss: 11.3733 - acc: 0.5000 - val_loss: 14.8266 - val_acc: 0.6400\n",
      "Epoch 30/500\n",
      "256/256 [==============================] - 0s 54us/step - loss: 11.3666 - acc: 0.5000 - val_loss: 14.9054 - val_acc: 0.6400\n",
      "Epoch 31/500\n",
      "256/256 [==============================] - 0s 61us/step - loss: 11.3603 - acc: 0.5000 - val_loss: 14.9754 - val_acc: 0.6400\n",
      "Epoch 32/500\n",
      "256/256 [==============================] - 0s 57us/step - loss: 11.3549 - acc: 0.5000 - val_loss: 15.0370 - val_acc: 0.6400\n",
      "Epoch 33/500\n",
      "256/256 [==============================] - 0s 46us/step - loss: 11.3489 - acc: 0.5000 - val_loss: 15.0906 - val_acc: 0.6400\n",
      "Epoch 34/500\n",
      "256/256 [==============================] - 0s 57us/step - loss: 11.3430 - acc: 0.5000 - val_loss: 15.1372 - val_acc: 0.6400\n",
      "Epoch 35/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 11.3366 - acc: 0.5000 - val_loss: 15.1771 - val_acc: 0.6400\n",
      "Epoch 36/500\n",
      "256/256 [==============================] - 0s 65us/step - loss: 11.3300 - acc: 0.5000 - val_loss: 15.2111 - val_acc: 0.6400\n",
      "Epoch 37/500\n",
      "256/256 [==============================] - 0s 32us/step - loss: 11.3235 - acc: 0.5000 - val_loss: 15.2396 - val_acc: 0.6400\n",
      "Epoch 38/500\n",
      "256/256 [==============================] - 0s 38us/step - loss: 11.3170 - acc: 0.5000 - val_loss: 15.2632 - val_acc: 0.6400\n",
      "Epoch 39/500\n",
      "256/256 [==============================] - 0s 41us/step - loss: 11.3100 - acc: 0.5000 - val_loss: 15.2821 - val_acc: 0.6400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/500\n",
      "256/256 [==============================] - 0s 34us/step - loss: 11.3031 - acc: 0.5000 - val_loss: 15.2968 - val_acc: 0.6400\n",
      "Epoch 41/500\n",
      "256/256 [==============================] - 0s 36us/step - loss: 11.2960 - acc: 0.5000 - val_loss: 15.3081 - val_acc: 0.6400\n",
      "Epoch 42/500\n",
      "256/256 [==============================] - 0s 43us/step - loss: 11.2886 - acc: 0.5000 - val_loss: 15.3163 - val_acc: 0.6400\n",
      "Epoch 43/500\n",
      "256/256 [==============================] - 0s 26us/step - loss: 11.2813 - acc: 0.5000 - val_loss: 15.3216 - val_acc: 0.6400\n",
      "Epoch 44/500\n",
      "256/256 [==============================] - 0s 42us/step - loss: 11.2738 - acc: 0.5000 - val_loss: 15.3241 - val_acc: 0.6400\n",
      "Epoch 45/500\n",
      "256/256 [==============================] - 0s 41us/step - loss: 11.2663 - acc: 0.5000 - val_loss: 15.3240 - val_acc: 0.6400\n",
      "Epoch 46/500\n",
      "256/256 [==============================] - 0s 40us/step - loss: 11.2586 - acc: 0.5000 - val_loss: 15.3217 - val_acc: 0.6400\n",
      "Epoch 47/500\n",
      "256/256 [==============================] - 0s 31us/step - loss: 11.2508 - acc: 0.5000 - val_loss: 15.3176 - val_acc: 0.6400\n",
      "Epoch 48/500\n",
      "256/256 [==============================] - 0s 44us/step - loss: 11.2430 - acc: 0.5000 - val_loss: 15.3115 - val_acc: 0.6400\n",
      "Epoch 49/500\n",
      "256/256 [==============================] - 0s 37us/step - loss: 11.2352 - acc: 0.5000 - val_loss: 15.3037 - val_acc: 0.6400\n",
      "Epoch 50/500\n",
      "256/256 [==============================] - 0s 75us/step - loss: 11.2272 - acc: 0.5000 - val_loss: 15.2944 - val_acc: 0.6400\n",
      "Epoch 51/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 11.2192 - acc: 0.5000 - val_loss: 15.2837 - val_acc: 0.6400\n",
      "Epoch 52/500\n",
      "256/256 [==============================] - 0s 69us/step - loss: 11.2113 - acc: 0.5000 - val_loss: 15.2720 - val_acc: 0.6400\n",
      "Epoch 00052: early stopping\n",
      "Train on 256 samples, validate on 50 samples\n",
      "Epoch 1/500\n",
      "256/256 [==============================] - 0s 23us/step - loss: 19.0117 - acc: 0.5000 - val_loss: 8.9621 - val_acc: 0.6400\n",
      "Epoch 2/500\n",
      "256/256 [==============================] - 0s 50us/step - loss: 18.1611 - acc: 0.5000 - val_loss: 8.9555 - val_acc: 0.6400\n",
      "Epoch 3/500\n",
      "256/256 [==============================] - 0s 41us/step - loss: 16.8257 - acc: 0.5000 - val_loss: 8.9567 - val_acc: 0.6400\n",
      "Epoch 4/500\n",
      "256/256 [==============================] - 0s 56us/step - loss: 15.4849 - acc: 0.5000 - val_loss: 8.9732 - val_acc: 0.6400\n",
      "Epoch 5/500\n",
      "256/256 [==============================] - 0s 61us/step - loss: 14.3990 - acc: 0.5000 - val_loss: 9.0059 - val_acc: 0.6400\n",
      "Epoch 6/500\n",
      "256/256 [==============================] - 0s 50us/step - loss: 13.5523 - acc: 0.5000 - val_loss: 9.0528 - val_acc: 0.6400\n",
      "Epoch 7/500\n",
      "256/256 [==============================] - 0s 67us/step - loss: 12.8603 - acc: 0.5000 - val_loss: 9.1152 - val_acc: 0.6400\n",
      "Epoch 8/500\n",
      "256/256 [==============================] - 0s 59us/step - loss: 12.3257 - acc: 0.5000 - val_loss: 9.1886 - val_acc: 0.6400\n",
      "Epoch 9/500\n",
      "256/256 [==============================] - 0s 63us/step - loss: 11.9251 - acc: 0.5000 - val_loss: 9.2717 - val_acc: 0.6400\n",
      "Epoch 10/500\n",
      "256/256 [==============================] - 0s 37us/step - loss: 11.6410 - acc: 0.5000 - val_loss: 9.3598 - val_acc: 0.6400\n",
      "Epoch 11/500\n",
      "256/256 [==============================] - 0s 30us/step - loss: 11.4412 - acc: 0.5000 - val_loss: 9.4494 - val_acc: 0.6400\n",
      "Epoch 12/500\n",
      "256/256 [==============================] - 0s 33us/step - loss: 11.3036 - acc: 0.5000 - val_loss: 9.5398 - val_acc: 0.6400\n",
      "Epoch 13/500\n",
      "256/256 [==============================] - 0s 45us/step - loss: 11.2215 - acc: 0.5000 - val_loss: 9.6302 - val_acc: 0.6400\n",
      "Epoch 14/500\n",
      "256/256 [==============================] - 0s 41us/step - loss: 11.1694 - acc: 0.5000 - val_loss: 9.7175 - val_acc: 0.6400\n",
      "Epoch 15/500\n",
      "256/256 [==============================] - 0s 38us/step - loss: 11.1333 - acc: 0.5000 - val_loss: 9.8006 - val_acc: 0.6400\n",
      "Epoch 16/500\n",
      "256/256 [==============================] - 0s 56us/step - loss: 11.1096 - acc: 0.5000 - val_loss: 9.8777 - val_acc: 0.6400\n",
      "Epoch 17/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 11.0964 - acc: 0.5000 - val_loss: 9.9498 - val_acc: 0.6400\n",
      "Epoch 18/500\n",
      "256/256 [==============================] - 0s 54us/step - loss: 11.0850 - acc: 0.5000 - val_loss: 10.0156 - val_acc: 0.6400\n",
      "Epoch 19/500\n",
      "256/256 [==============================] - 0s 55us/step - loss: 11.0751 - acc: 0.5000 - val_loss: 10.0757 - val_acc: 0.6400\n",
      "Epoch 20/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 11.0647 - acc: 0.5000 - val_loss: 10.1297 - val_acc: 0.6400\n",
      "Epoch 21/500\n",
      "256/256 [==============================] - 0s 28us/step - loss: 11.0580 - acc: 0.5000 - val_loss: 10.1794 - val_acc: 0.6400\n",
      "Epoch 22/500\n",
      "256/256 [==============================] - 0s 30us/step - loss: 11.0509 - acc: 0.5000 - val_loss: 10.2245 - val_acc: 0.6400\n",
      "Epoch 23/500\n",
      "256/256 [==============================] - 0s 43us/step - loss: 11.0444 - acc: 0.5000 - val_loss: 10.2646 - val_acc: 0.6400\n",
      "Epoch 24/500\n",
      "256/256 [==============================] - 0s 22us/step - loss: 11.0368 - acc: 0.5000 - val_loss: 10.3006 - val_acc: 0.6400\n",
      "Epoch 25/500\n",
      "256/256 [==============================] - 0s 39us/step - loss: 11.0310 - acc: 0.5000 - val_loss: 10.3324 - val_acc: 0.6400\n",
      "Epoch 26/500\n",
      "256/256 [==============================] - 0s 39us/step - loss: 11.0239 - acc: 0.5000 - val_loss: 10.3602 - val_acc: 0.6400\n",
      "Epoch 27/500\n",
      "256/256 [==============================] - 0s 40us/step - loss: 11.0170 - acc: 0.5000 - val_loss: 10.3842 - val_acc: 0.6400\n",
      "Epoch 28/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 11.0101 - acc: 0.5000 - val_loss: 10.4045 - val_acc: 0.6400\n",
      "Epoch 29/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 11.0031 - acc: 0.5000 - val_loss: 10.4217 - val_acc: 0.6400\n",
      "Epoch 30/500\n",
      "256/256 [==============================] - 0s 45us/step - loss: 10.9961 - acc: 0.5000 - val_loss: 10.4362 - val_acc: 0.6400\n",
      "Epoch 31/500\n",
      "256/256 [==============================] - 0s 34us/step - loss: 10.9893 - acc: 0.5000 - val_loss: 10.4484 - val_acc: 0.6400\n",
      "Epoch 32/500\n",
      "256/256 [==============================] - 0s 35us/step - loss: 10.9820 - acc: 0.5000 - val_loss: 10.4580 - val_acc: 0.6400\n",
      "Epoch 33/500\n",
      "256/256 [==============================] - 0s 33us/step - loss: 10.9751 - acc: 0.5000 - val_loss: 10.4655 - val_acc: 0.6400\n",
      "Epoch 34/500\n",
      "256/256 [==============================] - 0s 54us/step - loss: 10.9677 - acc: 0.5000 - val_loss: 10.4706 - val_acc: 0.6400\n",
      "Epoch 35/500\n",
      "256/256 [==============================] - 0s 38us/step - loss: 10.9605 - acc: 0.5000 - val_loss: 10.4739 - val_acc: 0.6400\n",
      "Epoch 36/500\n",
      "256/256 [==============================] - 0s 31us/step - loss: 10.9531 - acc: 0.5000 - val_loss: 10.4758 - val_acc: 0.6400\n",
      "Epoch 37/500\n",
      "256/256 [==============================] - 0s 32us/step - loss: 10.9458 - acc: 0.5000 - val_loss: 10.4760 - val_acc: 0.6400\n",
      "Epoch 38/500\n",
      "256/256 [==============================] - 0s 37us/step - loss: 10.9384 - acc: 0.5000 - val_loss: 10.4752 - val_acc: 0.6400\n",
      "Epoch 39/500\n",
      "256/256 [==============================] - 0s 42us/step - loss: 10.9310 - acc: 0.5000 - val_loss: 10.4731 - val_acc: 0.6400\n",
      "Epoch 40/500\n",
      "256/256 [==============================] - 0s 30us/step - loss: 10.9236 - acc: 0.5000 - val_loss: 10.4699 - val_acc: 0.6400\n",
      "Epoch 41/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 10.9161 - acc: 0.5000 - val_loss: 10.4658 - val_acc: 0.6400\n",
      "Epoch 42/500\n",
      "256/256 [==============================] - 0s 50us/step - loss: 10.9086 - acc: 0.5000 - val_loss: 10.4610 - val_acc: 0.6400\n",
      "Epoch 43/500\n",
      "256/256 [==============================] - 0s 29us/step - loss: 10.9011 - acc: 0.5000 - val_loss: 10.4553 - val_acc: 0.6400\n",
      "Epoch 44/500\n",
      "256/256 [==============================] - 0s 25us/step - loss: 10.8935 - acc: 0.5000 - val_loss: 10.4489 - val_acc: 0.6400\n",
      "Epoch 45/500\n",
      "256/256 [==============================] - 0s 45us/step - loss: 10.8860 - acc: 0.5000 - val_loss: 10.4417 - val_acc: 0.6400\n",
      "Epoch 46/500\n",
      "256/256 [==============================] - 0s 24us/step - loss: 10.8785 - acc: 0.5000 - val_loss: 10.4341 - val_acc: 0.6400\n",
      "Epoch 47/500\n",
      "256/256 [==============================] - 0s 66us/step - loss: 10.8710 - acc: 0.5000 - val_loss: 10.4259 - val_acc: 0.6400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/500\n",
      "256/256 [==============================] - 0s 43us/step - loss: 10.8634 - acc: 0.5000 - val_loss: 10.4170 - val_acc: 0.6400\n",
      "Epoch 49/500\n",
      "256/256 [==============================] - 0s 45us/step - loss: 10.8558 - acc: 0.5000 - val_loss: 10.4078 - val_acc: 0.6400\n",
      "Epoch 50/500\n",
      "256/256 [==============================] - 0s 44us/step - loss: 10.8483 - acc: 0.5000 - val_loss: 10.3982 - val_acc: 0.6400\n",
      "Epoch 51/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 10.8407 - acc: 0.5000 - val_loss: 10.3883 - val_acc: 0.6400\n",
      "Epoch 52/500\n",
      "256/256 [==============================] - 0s 70us/step - loss: 10.8331 - acc: 0.5000 - val_loss: 10.3781 - val_acc: 0.6400\n",
      "Epoch 00052: early stopping\n",
      "Train on 256 samples, validate on 50 samples\n",
      "Epoch 1/500\n",
      "256/256 [==============================] - 0s 32us/step - loss: 12.8547 - acc: 0.5000 - val_loss: 8.5856 - val_acc: 0.6400\n",
      "Epoch 2/500\n",
      "256/256 [==============================] - 0s 33us/step - loss: 12.5289 - acc: 0.5000 - val_loss: 8.5782 - val_acc: 0.6400\n",
      "Epoch 3/500\n",
      "256/256 [==============================] - 0s 36us/step - loss: 12.0359 - acc: 0.5000 - val_loss: 8.5739 - val_acc: 0.6400\n",
      "Epoch 4/500\n",
      "256/256 [==============================] - 0s 76us/step - loss: 11.5567 - acc: 0.5000 - val_loss: 8.5737 - val_acc: 0.6400\n",
      "Epoch 5/500\n",
      "256/256 [==============================] - 0s 61us/step - loss: 11.1956 - acc: 0.5000 - val_loss: 8.5783 - val_acc: 0.6400\n",
      "Epoch 6/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 10.9840 - acc: 0.5000 - val_loss: 8.5866 - val_acc: 0.6400\n",
      "Epoch 7/500\n",
      "256/256 [==============================] - 0s 55us/step - loss: 10.8721 - acc: 0.5000 - val_loss: 8.5986 - val_acc: 0.6400\n",
      "Epoch 8/500\n",
      "256/256 [==============================] - 0s 70us/step - loss: 10.8103 - acc: 0.5000 - val_loss: 8.6135 - val_acc: 0.6400\n",
      "Epoch 9/500\n",
      "256/256 [==============================] - 0s 58us/step - loss: 10.7830 - acc: 0.5000 - val_loss: 8.6302 - val_acc: 0.6400\n",
      "Epoch 10/500\n",
      "256/256 [==============================] - 0s 55us/step - loss: 10.7647 - acc: 0.5000 - val_loss: 8.6473 - val_acc: 0.6400\n",
      "Epoch 11/500\n",
      "256/256 [==============================] - 0s 41us/step - loss: 10.7548 - acc: 0.5000 - val_loss: 8.6648 - val_acc: 0.6400\n",
      "Epoch 12/500\n",
      "256/256 [==============================] - 0s 54us/step - loss: 10.7471 - acc: 0.5000 - val_loss: 8.6824 - val_acc: 0.6400\n",
      "Epoch 13/500\n",
      "256/256 [==============================] - 0s 62us/step - loss: 10.7390 - acc: 0.5000 - val_loss: 8.6996 - val_acc: 0.6400\n",
      "Epoch 14/500\n",
      "256/256 [==============================] - 0s 55us/step - loss: 10.7319 - acc: 0.5000 - val_loss: 8.7159 - val_acc: 0.6400\n",
      "Epoch 15/500\n",
      "256/256 [==============================] - 0s 72us/step - loss: 10.7247 - acc: 0.5000 - val_loss: 8.7310 - val_acc: 0.6400\n",
      "Epoch 16/500\n",
      "256/256 [==============================] - 0s 61us/step - loss: 10.7183 - acc: 0.5000 - val_loss: 8.7449 - val_acc: 0.6400\n",
      "Epoch 17/500\n",
      "256/256 [==============================] - 0s 52us/step - loss: 10.7113 - acc: 0.5000 - val_loss: 8.7575 - val_acc: 0.6400\n",
      "Epoch 18/500\n",
      "256/256 [==============================] - 0s 66us/step - loss: 10.7042 - acc: 0.5000 - val_loss: 8.7686 - val_acc: 0.6400\n",
      "Epoch 19/500\n",
      "256/256 [==============================] - 0s 52us/step - loss: 10.6978 - acc: 0.5000 - val_loss: 8.7785 - val_acc: 0.6400\n",
      "Epoch 20/500\n",
      "256/256 [==============================] - 0s 44us/step - loss: 10.6912 - acc: 0.5000 - val_loss: 8.7872 - val_acc: 0.6400\n",
      "Epoch 21/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 10.6840 - acc: 0.5000 - val_loss: 8.7944 - val_acc: 0.6400\n",
      "Epoch 22/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 10.6772 - acc: 0.5000 - val_loss: 8.8004 - val_acc: 0.6400\n",
      "Epoch 23/500\n",
      "256/256 [==============================] - 0s 68us/step - loss: 10.6704 - acc: 0.5000 - val_loss: 8.8053 - val_acc: 0.6400\n",
      "Epoch 24/500\n",
      "256/256 [==============================] - 0s 28us/step - loss: 10.6637 - acc: 0.5000 - val_loss: 8.8092 - val_acc: 0.6400\n",
      "Epoch 25/500\n",
      "256/256 [==============================] - 0s 42us/step - loss: 10.6568 - acc: 0.5000 - val_loss: 8.8121 - val_acc: 0.6400\n",
      "Epoch 26/500\n",
      "256/256 [==============================] - 0s 43us/step - loss: 10.6501 - acc: 0.5000 - val_loss: 8.8140 - val_acc: 0.6400\n",
      "Epoch 27/500\n",
      "256/256 [==============================] - 0s 31us/step - loss: 10.6430 - acc: 0.5000 - val_loss: 8.8150 - val_acc: 0.6400\n",
      "Epoch 28/500\n",
      "256/256 [==============================] - 0s 29us/step - loss: 10.6362 - acc: 0.5000 - val_loss: 8.8151 - val_acc: 0.6400\n",
      "Epoch 29/500\n",
      "256/256 [==============================] - 0s 24us/step - loss: 10.6294 - acc: 0.5000 - val_loss: 8.8144 - val_acc: 0.6400\n",
      "Epoch 30/500\n",
      "256/256 [==============================] - 0s 62us/step - loss: 10.6224 - acc: 0.5000 - val_loss: 8.8130 - val_acc: 0.6400\n",
      "Epoch 31/500\n",
      "256/256 [==============================] - 0s 41us/step - loss: 10.6156 - acc: 0.5000 - val_loss: 8.8110 - val_acc: 0.6400\n",
      "Epoch 32/500\n",
      "256/256 [==============================] - 0s 57us/step - loss: 10.6086 - acc: 0.5000 - val_loss: 8.8082 - val_acc: 0.6400\n",
      "Epoch 33/500\n",
      "256/256 [==============================] - 0s 49us/step - loss: 10.6017 - acc: 0.5000 - val_loss: 8.8049 - val_acc: 0.6400\n",
      "Epoch 34/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 10.5947 - acc: 0.5000 - val_loss: 8.8013 - val_acc: 0.6400\n",
      "Epoch 35/500\n",
      "256/256 [==============================] - 0s 42us/step - loss: 10.5878 - acc: 0.5000 - val_loss: 8.7972 - val_acc: 0.6400\n",
      "Epoch 36/500\n",
      "256/256 [==============================] - 0s 38us/step - loss: 10.5808 - acc: 0.5000 - val_loss: 8.7927 - val_acc: 0.6400\n",
      "Epoch 37/500\n",
      "256/256 [==============================] - 0s 40us/step - loss: 10.5739 - acc: 0.5000 - val_loss: 8.7877 - val_acc: 0.6400\n",
      "Epoch 38/500\n",
      "256/256 [==============================] - 0s 35us/step - loss: 10.5670 - acc: 0.5000 - val_loss: 8.7824 - val_acc: 0.6400\n",
      "Epoch 39/500\n",
      "256/256 [==============================] - 0s 43us/step - loss: 10.5600 - acc: 0.5000 - val_loss: 8.7768 - val_acc: 0.6400\n",
      "Epoch 40/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 10.5531 - acc: 0.5000 - val_loss: 8.7710 - val_acc: 0.6400\n",
      "Epoch 41/500\n",
      "256/256 [==============================] - 0s 50us/step - loss: 10.5462 - acc: 0.5000 - val_loss: 8.7649 - val_acc: 0.6400\n",
      "Epoch 42/500\n",
      "256/256 [==============================] - 0s 27us/step - loss: 10.5393 - acc: 0.5000 - val_loss: 8.7585 - val_acc: 0.6400\n",
      "Epoch 43/500\n",
      "256/256 [==============================] - 0s 40us/step - loss: 10.5324 - acc: 0.5000 - val_loss: 8.7520 - val_acc: 0.6400\n",
      "Epoch 44/500\n",
      "256/256 [==============================] - 0s 30us/step - loss: 10.5254 - acc: 0.5000 - val_loss: 8.7453 - val_acc: 0.6400\n",
      "Epoch 45/500\n",
      "256/256 [==============================] - 0s 41us/step - loss: 10.5185 - acc: 0.5000 - val_loss: 8.7384 - val_acc: 0.6400\n",
      "Epoch 46/500\n",
      "256/256 [==============================] - 0s 50us/step - loss: 10.5116 - acc: 0.5000 - val_loss: 8.7314 - val_acc: 0.6400\n",
      "Epoch 47/500\n",
      "256/256 [==============================] - 0s 42us/step - loss: 10.5047 - acc: 0.5000 - val_loss: 8.7241 - val_acc: 0.6400\n",
      "Epoch 48/500\n",
      "256/256 [==============================] - 0s 56us/step - loss: 10.4979 - acc: 0.5000 - val_loss: 8.7167 - val_acc: 0.6400\n",
      "Epoch 49/500\n",
      "256/256 [==============================] - 0s 57us/step - loss: 10.4910 - acc: 0.5000 - val_loss: 8.7092 - val_acc: 0.6400\n",
      "Epoch 50/500\n",
      "256/256 [==============================] - 0s 58us/step - loss: 10.4842 - acc: 0.5000 - val_loss: 8.7017 - val_acc: 0.6400\n",
      "Epoch 51/500\n",
      "256/256 [==============================] - 0s 33us/step - loss: 10.4773 - acc: 0.5000 - val_loss: 8.6941 - val_acc: 0.6400\n",
      "Epoch 52/500\n",
      "256/256 [==============================] - 0s 45us/step - loss: 10.4705 - acc: 0.5000 - val_loss: 8.6865 - val_acc: 0.6400\n",
      "Epoch 53/500\n",
      "256/256 [==============================] - 0s 38us/step - loss: 10.4637 - acc: 0.5000 - val_loss: 8.6788 - val_acc: 0.6400\n",
      "Epoch 54/500\n",
      "256/256 [==============================] - 0s 26us/step - loss: 10.4569 - acc: 0.5000 - val_loss: 8.6710 - val_acc: 0.6400\n",
      "Epoch 00054: early stopping\n",
      "Train on 256 samples, validate on 50 samples\n",
      "Epoch 1/500\n",
      "256/256 [==============================] - 0s 41us/step - loss: 10.9465 - acc: 0.5000 - val_loss: 8.2112 - val_acc: 0.6400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/500\n",
      "256/256 [==============================] - 0s 26us/step - loss: 10.8016 - acc: 0.5000 - val_loss: 8.2046 - val_acc: 0.6400\n",
      "Epoch 3/500\n",
      "256/256 [==============================] - 0s 40us/step - loss: 10.6294 - acc: 0.5000 - val_loss: 8.1989 - val_acc: 0.6400\n",
      "Epoch 4/500\n",
      "256/256 [==============================] - 0s 32us/step - loss: 10.5074 - acc: 0.5000 - val_loss: 8.1943 - val_acc: 0.6400\n",
      "Epoch 5/500\n",
      "256/256 [==============================] - 0s 39us/step - loss: 10.4478 - acc: 0.5000 - val_loss: 8.1906 - val_acc: 0.6400\n",
      "Epoch 6/500\n",
      "256/256 [==============================] - 0s 26us/step - loss: 10.4230 - acc: 0.5000 - val_loss: 8.1875 - val_acc: 0.6400\n",
      "Epoch 7/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 10.4126 - acc: 0.5000 - val_loss: 8.1849 - val_acc: 0.6400\n",
      "Epoch 8/500\n",
      "256/256 [==============================] - 0s 39us/step - loss: 10.4046 - acc: 0.5000 - val_loss: 8.1825 - val_acc: 0.6400\n",
      "Epoch 9/500\n",
      "256/256 [==============================] - 0s 50us/step - loss: 10.3976 - acc: 0.5000 - val_loss: 8.1803 - val_acc: 0.6400\n",
      "Epoch 10/500\n",
      "256/256 [==============================] - 0s 68us/step - loss: 10.3910 - acc: 0.5000 - val_loss: 8.1785 - val_acc: 0.6400\n",
      "Epoch 11/500\n",
      "256/256 [==============================] - 0s 54us/step - loss: 10.3846 - acc: 0.5000 - val_loss: 8.1769 - val_acc: 0.6400\n",
      "Epoch 12/500\n",
      "256/256 [==============================] - 0s 60us/step - loss: 10.3778 - acc: 0.5000 - val_loss: 8.1754 - val_acc: 0.6400\n",
      "Epoch 13/500\n",
      "256/256 [==============================] - 0s 60us/step - loss: 10.3717 - acc: 0.5000 - val_loss: 8.1740 - val_acc: 0.6400\n",
      "Epoch 14/500\n",
      "256/256 [==============================] - 0s 49us/step - loss: 10.3652 - acc: 0.5000 - val_loss: 8.1723 - val_acc: 0.6400\n",
      "Epoch 15/500\n",
      "256/256 [==============================] - 0s 62us/step - loss: 10.3587 - acc: 0.5000 - val_loss: 8.1703 - val_acc: 0.6400\n",
      "Epoch 16/500\n",
      "256/256 [==============================] - 0s 55us/step - loss: 10.3524 - acc: 0.5000 - val_loss: 8.1681 - val_acc: 0.6400\n",
      "Epoch 17/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 10.3460 - acc: 0.5000 - val_loss: 8.1656 - val_acc: 0.6400\n",
      "Epoch 18/500\n",
      "256/256 [==============================] - 0s 46us/step - loss: 10.3398 - acc: 0.5000 - val_loss: 8.1627 - val_acc: 0.6400\n",
      "Epoch 19/500\n",
      "256/256 [==============================] - 0s 38us/step - loss: 10.3334 - acc: 0.5000 - val_loss: 8.1596 - val_acc: 0.6400\n",
      "Epoch 20/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 10.3271 - acc: 0.5000 - val_loss: 8.1563 - val_acc: 0.6400\n",
      "Epoch 21/500\n",
      "256/256 [==============================] - 0s 44us/step - loss: 10.3208 - acc: 0.5000 - val_loss: 8.1526 - val_acc: 0.6400\n",
      "Epoch 22/500\n",
      "256/256 [==============================] - 0s 62us/step - loss: 10.3145 - acc: 0.5000 - val_loss: 8.1486 - val_acc: 0.6400\n",
      "Epoch 23/500\n",
      "256/256 [==============================] - 0s 52us/step - loss: 10.3081 - acc: 0.5000 - val_loss: 8.1444 - val_acc: 0.6400\n",
      "Epoch 24/500\n",
      "256/256 [==============================] - 0s 60us/step - loss: 10.3018 - acc: 0.5000 - val_loss: 8.1399 - val_acc: 0.6400\n",
      "Epoch 25/500\n",
      "256/256 [==============================] - 0s 56us/step - loss: 10.2955 - acc: 0.5000 - val_loss: 8.1352 - val_acc: 0.6400\n",
      "Epoch 26/500\n",
      "256/256 [==============================] - 0s 66us/step - loss: 10.2892 - acc: 0.5000 - val_loss: 8.1303 - val_acc: 0.6400\n",
      "Epoch 27/500\n",
      "256/256 [==============================] - 0s 30us/step - loss: 10.2828 - acc: 0.5000 - val_loss: 8.1253 - val_acc: 0.6400\n",
      "Epoch 28/500\n",
      "256/256 [==============================] - 0s 55us/step - loss: 10.2766 - acc: 0.5000 - val_loss: 8.1201 - val_acc: 0.6400\n",
      "Epoch 29/500\n",
      "256/256 [==============================] - 0s 35us/step - loss: 10.2703 - acc: 0.5000 - val_loss: 8.1148 - val_acc: 0.6400\n",
      "Epoch 30/500\n",
      "256/256 [==============================] - 0s 50us/step - loss: 10.2640 - acc: 0.5000 - val_loss: 8.1094 - val_acc: 0.6400\n",
      "Epoch 31/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 10.2577 - acc: 0.5000 - val_loss: 8.1040 - val_acc: 0.6400\n",
      "Epoch 32/500\n",
      "256/256 [==============================] - 0s 61us/step - loss: 10.2515 - acc: 0.5000 - val_loss: 8.0985 - val_acc: 0.6400\n",
      "Epoch 33/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 10.2452 - acc: 0.5000 - val_loss: 8.0930 - val_acc: 0.6400\n",
      "Epoch 34/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 10.2389 - acc: 0.5000 - val_loss: 8.0873 - val_acc: 0.6400\n",
      "Epoch 35/500\n",
      "256/256 [==============================] - 0s 65us/step - loss: 10.2327 - acc: 0.5000 - val_loss: 8.0817 - val_acc: 0.6400\n",
      "Epoch 36/500\n",
      "256/256 [==============================] - 0s 39us/step - loss: 10.2265 - acc: 0.5000 - val_loss: 8.0760 - val_acc: 0.6400\n",
      "Epoch 37/500\n",
      "256/256 [==============================] - 0s 27us/step - loss: 10.2203 - acc: 0.5000 - val_loss: 8.0702 - val_acc: 0.6400\n",
      "Epoch 38/500\n",
      "256/256 [==============================] - 0s 44us/step - loss: 10.2141 - acc: 0.5000 - val_loss: 8.0643 - val_acc: 0.6400\n",
      "Epoch 39/500\n",
      "256/256 [==============================] - 0s 52us/step - loss: 10.2079 - acc: 0.5000 - val_loss: 8.0583 - val_acc: 0.6400\n",
      "Epoch 40/500\n",
      "256/256 [==============================] - 0s 63us/step - loss: 10.2017 - acc: 0.5000 - val_loss: 8.0523 - val_acc: 0.6400\n",
      "Epoch 41/500\n",
      "256/256 [==============================] - 0s 52us/step - loss: 10.1955 - acc: 0.5000 - val_loss: 8.0462 - val_acc: 0.6400\n",
      "Epoch 42/500\n",
      "256/256 [==============================] - 0s 60us/step - loss: 10.1894 - acc: 0.5000 - val_loss: 8.0401 - val_acc: 0.6400\n",
      "Epoch 43/500\n",
      "256/256 [==============================] - 0s 39us/step - loss: 10.1832 - acc: 0.5000 - val_loss: 8.0340 - val_acc: 0.6400\n",
      "Epoch 44/500\n",
      "256/256 [==============================] - 0s 54us/step - loss: 10.1771 - acc: 0.5000 - val_loss: 8.0278 - val_acc: 0.6400\n",
      "Epoch 45/500\n",
      "256/256 [==============================] - 0s 52us/step - loss: 10.1709 - acc: 0.5000 - val_loss: 8.0216 - val_acc: 0.6400\n",
      "Epoch 46/500\n",
      "256/256 [==============================] - 0s 62us/step - loss: 10.1648 - acc: 0.5000 - val_loss: 8.0155 - val_acc: 0.6400\n",
      "Epoch 47/500\n",
      "256/256 [==============================] - 0s 33us/step - loss: 10.1587 - acc: 0.5000 - val_loss: 8.0093 - val_acc: 0.6400\n",
      "Epoch 48/500\n",
      "256/256 [==============================] - 0s 45us/step - loss: 10.1526 - acc: 0.5000 - val_loss: 8.0031 - val_acc: 0.6400\n",
      "Epoch 49/500\n",
      "256/256 [==============================] - 0s 36us/step - loss: 10.1466 - acc: 0.5000 - val_loss: 7.9970 - val_acc: 0.6400\n",
      "Epoch 50/500\n",
      "256/256 [==============================] - 0s 26us/step - loss: 10.1405 - acc: 0.5000 - val_loss: 7.9908 - val_acc: 0.6400\n",
      "Epoch 51/500\n",
      "256/256 [==============================] - 0s 41us/step - loss: 10.1345 - acc: 0.5000 - val_loss: 7.9846 - val_acc: 0.6400\n",
      "Epoch 52/500\n",
      "256/256 [==============================] - 0s 39us/step - loss: 10.1284 - acc: 0.5000 - val_loss: 7.9784 - val_acc: 0.6400\n",
      "Epoch 53/500\n",
      "256/256 [==============================] - 0s 29us/step - loss: 10.1224 - acc: 0.5000 - val_loss: 7.9722 - val_acc: 0.6400\n",
      "Epoch 54/500\n",
      "256/256 [==============================] - 0s 40us/step - loss: 10.1164 - acc: 0.5000 - val_loss: 7.9661 - val_acc: 0.6400\n",
      "Epoch 55/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 10.1104 - acc: 0.5000 - val_loss: 7.9599 - val_acc: 0.6400\n",
      "Epoch 56/500\n",
      "256/256 [==============================] - 0s 58us/step - loss: 10.1044 - acc: 0.5000 - val_loss: 7.9538 - val_acc: 0.6400\n",
      "Epoch 57/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 10.0985 - acc: 0.5000 - val_loss: 7.9477 - val_acc: 0.6400\n",
      "Epoch 58/500\n",
      "256/256 [==============================] - 0s 42us/step - loss: 10.0925 - acc: 0.5000 - val_loss: 7.9416 - val_acc: 0.6400\n",
      "Epoch 59/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 10.0866 - acc: 0.5000 - val_loss: 7.9355 - val_acc: 0.6400\n",
      "Epoch 60/500\n",
      "256/256 [==============================] - 0s 79us/step - loss: 10.0807 - acc: 0.5000 - val_loss: 7.9294 - val_acc: 0.6400\n",
      "Epoch 61/500\n",
      "256/256 [==============================] - 0s 58us/step - loss: 10.0748 - acc: 0.5000 - val_loss: 7.9233 - val_acc: 0.6400\n",
      "Epoch 62/500\n",
      "256/256 [==============================] - 0s 46us/step - loss: 10.0689 - acc: 0.5000 - val_loss: 7.9172 - val_acc: 0.6400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/500\n",
      "256/256 [==============================] - 0s 67us/step - loss: 10.0630 - acc: 0.5000 - val_loss: 7.9111 - val_acc: 0.6400\n",
      "Epoch 64/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 10.0571 - acc: 0.5000 - val_loss: 7.9050 - val_acc: 0.6400\n",
      "Epoch 65/500\n",
      "256/256 [==============================] - 0s 60us/step - loss: 10.0513 - acc: 0.5000 - val_loss: 7.8990 - val_acc: 0.6400\n",
      "Epoch 66/500\n",
      "256/256 [==============================] - 0s 60us/step - loss: 10.0454 - acc: 0.5000 - val_loss: 7.8930 - val_acc: 0.6400\n",
      "Epoch 67/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 10.0396 - acc: 0.5000 - val_loss: 7.8870 - val_acc: 0.6400\n",
      "Epoch 68/500\n",
      "256/256 [==============================] - 0s 52us/step - loss: 10.0338 - acc: 0.5000 - val_loss: 7.8811 - val_acc: 0.6400\n",
      "Epoch 69/500\n",
      "256/256 [==============================] - 0s 69us/step - loss: 10.0280 - acc: 0.5000 - val_loss: 7.8752 - val_acc: 0.6400\n",
      "Epoch 70/500\n",
      "256/256 [==============================] - 0s 65us/step - loss: 10.0222 - acc: 0.5000 - val_loss: 7.8693 - val_acc: 0.6400\n",
      "Epoch 71/500\n",
      "256/256 [==============================] - 0s 29us/step - loss: 10.0165 - acc: 0.5000 - val_loss: 7.8634 - val_acc: 0.6400\n",
      "Epoch 72/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 10.0107 - acc: 0.5000 - val_loss: 7.8575 - val_acc: 0.6400\n",
      "Epoch 73/500\n",
      "256/256 [==============================] - 0s 41us/step - loss: 10.0050 - acc: 0.5000 - val_loss: 7.8517 - val_acc: 0.6400\n",
      "Epoch 74/500\n",
      "256/256 [==============================] - 0s 25us/step - loss: 9.9993 - acc: 0.5000 - val_loss: 7.8458 - val_acc: 0.6400\n",
      "Epoch 75/500\n",
      "256/256 [==============================] - 0s 50us/step - loss: 9.9935 - acc: 0.5000 - val_loss: 7.8400 - val_acc: 0.6400\n",
      "Epoch 76/500\n",
      "256/256 [==============================] - 0s 55us/step - loss: 9.9878 - acc: 0.5000 - val_loss: 7.8342 - val_acc: 0.6400\n",
      "Epoch 77/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 9.9822 - acc: 0.5000 - val_loss: 7.8284 - val_acc: 0.6400\n",
      "Epoch 78/500\n",
      "256/256 [==============================] - 0s 61us/step - loss: 9.9765 - acc: 0.5000 - val_loss: 7.8225 - val_acc: 0.6400\n",
      "Epoch 79/500\n",
      "256/256 [==============================] - 0s 59us/step - loss: 9.9708 - acc: 0.5000 - val_loss: 7.8167 - val_acc: 0.6400\n",
      "Epoch 80/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 9.9652 - acc: 0.5000 - val_loss: 7.8109 - val_acc: 0.6400\n",
      "Epoch 81/500\n",
      "256/256 [==============================] - 0s 52us/step - loss: 9.9596 - acc: 0.5000 - val_loss: 7.8051 - val_acc: 0.6400\n",
      "Epoch 82/500\n",
      "256/256 [==============================] - 0s 59us/step - loss: 9.9540 - acc: 0.5000 - val_loss: 7.7994 - val_acc: 0.6400\n",
      "Epoch 83/500\n",
      "256/256 [==============================] - 0s 59us/step - loss: 9.9484 - acc: 0.5000 - val_loss: 7.7936 - val_acc: 0.6400\n",
      "Epoch 84/500\n",
      "256/256 [==============================] - 0s 71us/step - loss: 9.9428 - acc: 0.5000 - val_loss: 7.7879 - val_acc: 0.6400\n",
      "Epoch 85/500\n",
      "256/256 [==============================] - 0s 68us/step - loss: 9.9372 - acc: 0.5000 - val_loss: 7.7822 - val_acc: 0.6400\n",
      "Epoch 86/500\n",
      "256/256 [==============================] - 0s 50us/step - loss: 9.9317 - acc: 0.5000 - val_loss: 7.7765 - val_acc: 0.6400\n",
      "Epoch 87/500\n",
      "256/256 [==============================] - 0s 42us/step - loss: 9.9261 - acc: 0.5000 - val_loss: 7.7709 - val_acc: 0.6400\n",
      "Epoch 88/500\n",
      "256/256 [==============================] - 0s 29us/step - loss: 9.9206 - acc: 0.5000 - val_loss: 7.7653 - val_acc: 0.6400\n",
      "Epoch 89/500\n",
      "256/256 [==============================] - 0s 33us/step - loss: 9.9151 - acc: 0.5000 - val_loss: 7.7597 - val_acc: 0.6400\n",
      "Epoch 90/500\n",
      "256/256 [==============================] - 0s 54us/step - loss: 9.9096 - acc: 0.5000 - val_loss: 7.7542 - val_acc: 0.6400\n",
      "Epoch 91/500\n",
      "256/256 [==============================] - 0s 57us/step - loss: 9.9041 - acc: 0.5000 - val_loss: 7.7487 - val_acc: 0.6400\n",
      "Epoch 92/500\n",
      "256/256 [==============================] - 0s 55us/step - loss: 9.8986 - acc: 0.5000 - val_loss: 7.7432 - val_acc: 0.6400\n",
      "Epoch 93/500\n",
      "256/256 [==============================] - 0s 73us/step - loss: 9.8932 - acc: 0.5000 - val_loss: 7.7377 - val_acc: 0.6400\n",
      "Epoch 94/500\n",
      "256/256 [==============================] - 0s 62us/step - loss: 9.8877 - acc: 0.5000 - val_loss: 7.7322 - val_acc: 0.6400\n",
      "Epoch 95/500\n",
      "256/256 [==============================] - 0s 72us/step - loss: 9.8823 - acc: 0.5000 - val_loss: 7.7267 - val_acc: 0.6400\n",
      "Epoch 96/500\n",
      "256/256 [==============================] - 0s 39us/step - loss: 9.8769 - acc: 0.5000 - val_loss: 7.7212 - val_acc: 0.6400\n",
      "Epoch 97/500\n",
      "256/256 [==============================] - 0s 29us/step - loss: 9.8715 - acc: 0.5000 - val_loss: 7.7157 - val_acc: 0.6400\n",
      "Epoch 98/500\n",
      "256/256 [==============================] - 0s 43us/step - loss: 9.8661 - acc: 0.5000 - val_loss: 7.7102 - val_acc: 0.6400\n",
      "Epoch 99/500\n",
      "256/256 [==============================] - 0s 35us/step - loss: 9.8607 - acc: 0.5000 - val_loss: 7.7048 - val_acc: 0.6400\n",
      "Epoch 100/500\n",
      "256/256 [==============================] - 0s 33us/step - loss: 9.8553 - acc: 0.5000 - val_loss: 7.6994 - val_acc: 0.6400\n",
      "Epoch 101/500\n",
      "256/256 [==============================] - 0s 44us/step - loss: 9.8500 - acc: 0.5000 - val_loss: 7.6941 - val_acc: 0.6400\n",
      "Epoch 102/500\n",
      "256/256 [==============================] - 0s 60us/step - loss: 9.8447 - acc: 0.5000 - val_loss: 7.6887 - val_acc: 0.6400\n",
      "Epoch 103/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 9.8394 - acc: 0.5000 - val_loss: 7.6835 - val_acc: 0.6400\n",
      "Epoch 104/500\n",
      "256/256 [==============================] - 0s 45us/step - loss: 9.8341 - acc: 0.5000 - val_loss: 7.6782 - val_acc: 0.6400\n",
      "Epoch 105/500\n",
      "256/256 [==============================] - 0s 38us/step - loss: 9.8288 - acc: 0.5000 - val_loss: 7.6729 - val_acc: 0.6400\n",
      "Epoch 106/500\n",
      "256/256 [==============================] - 0s 30us/step - loss: 9.8235 - acc: 0.5000 - val_loss: 7.6676 - val_acc: 0.6400\n",
      "Epoch 107/500\n",
      "256/256 [==============================] - 0s 45us/step - loss: 9.8182 - acc: 0.5000 - val_loss: 7.6624 - val_acc: 0.6400\n",
      "Epoch 108/500\n",
      "256/256 [==============================] - 0s 34us/step - loss: 9.8130 - acc: 0.5000 - val_loss: 7.6571 - val_acc: 0.6400\n",
      "Epoch 109/500\n",
      "256/256 [==============================] - 0s 33us/step - loss: 9.8077 - acc: 0.5000 - val_loss: 7.6519 - val_acc: 0.6400\n",
      "Epoch 110/500\n",
      "256/256 [==============================] - 0s 33us/step - loss: 9.8025 - acc: 0.5000 - val_loss: 7.6467 - val_acc: 0.6400\n",
      "Epoch 111/500\n",
      "256/256 [==============================] - 0s 58us/step - loss: 9.7973 - acc: 0.5000 - val_loss: 7.6416 - val_acc: 0.6400\n",
      "Epoch 112/500\n",
      "256/256 [==============================] - 0s 40us/step - loss: 9.7921 - acc: 0.5000 - val_loss: 7.6365 - val_acc: 0.6400\n",
      "Epoch 113/500\n",
      "256/256 [==============================] - 0s 63us/step - loss: 9.7869 - acc: 0.5000 - val_loss: 7.6314 - val_acc: 0.6400\n",
      "Epoch 114/500\n",
      "256/256 [==============================] - 0s 41us/step - loss: 9.7818 - acc: 0.5000 - val_loss: 7.6264 - val_acc: 0.6400\n",
      "Epoch 115/500\n",
      "256/256 [==============================] - 0s 42us/step - loss: 9.7766 - acc: 0.5000 - val_loss: 7.6213 - val_acc: 0.6400\n",
      "Epoch 116/500\n",
      "256/256 [==============================] - 0s 33us/step - loss: 9.7715 - acc: 0.5000 - val_loss: 7.6163 - val_acc: 0.6400\n",
      "Epoch 117/500\n",
      "256/256 [==============================] - 0s 56us/step - loss: 9.7664 - acc: 0.5000 - val_loss: 7.6112 - val_acc: 0.6400\n",
      "Epoch 118/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 9.7612 - acc: 0.5000 - val_loss: 7.6062 - val_acc: 0.6400\n",
      "Epoch 119/500\n",
      "256/256 [==============================] - 0s 37us/step - loss: 9.7561 - acc: 0.5000 - val_loss: 7.6012 - val_acc: 0.6400\n",
      "Epoch 120/500\n",
      "256/256 [==============================] - 0s 43us/step - loss: 9.7510 - acc: 0.5000 - val_loss: 7.5962 - val_acc: 0.6400\n",
      "Epoch 121/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 9.7460 - acc: 0.5000 - val_loss: 7.5912 - val_acc: 0.6400\n",
      "Epoch 122/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 9.7409 - acc: 0.5000 - val_loss: 7.5863 - val_acc: 0.6400\n",
      "Epoch 123/500\n",
      "256/256 [==============================] - 0s 39us/step - loss: 9.7359 - acc: 0.5000 - val_loss: 7.5814 - val_acc: 0.6400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124/500\n",
      "256/256 [==============================] - 0s 43us/step - loss: 9.7308 - acc: 0.5000 - val_loss: 7.5765 - val_acc: 0.6400\n",
      "Epoch 125/500\n",
      "256/256 [==============================] - 0s 56us/step - loss: 9.7258 - acc: 0.5000 - val_loss: 7.5716 - val_acc: 0.6400\n",
      "Epoch 126/500\n",
      "256/256 [==============================] - 0s 54us/step - loss: 9.7208 - acc: 0.5000 - val_loss: 7.5667 - val_acc: 0.6400\n",
      "Epoch 127/500\n",
      "256/256 [==============================] - 0s 44us/step - loss: 9.7158 - acc: 0.5000 - val_loss: 7.5618 - val_acc: 0.6400\n",
      "Epoch 128/500\n",
      "256/256 [==============================] - 0s 35us/step - loss: 9.7108 - acc: 0.5000 - val_loss: 7.5570 - val_acc: 0.6400\n",
      "Epoch 129/500\n",
      "256/256 [==============================] - 0s 50us/step - loss: 9.7058 - acc: 0.5000 - val_loss: 7.5522 - val_acc: 0.6400\n",
      "Epoch 130/500\n",
      "256/256 [==============================] - 0s 31us/step - loss: 9.7009 - acc: 0.5000 - val_loss: 7.5474 - val_acc: 0.6400\n",
      "Epoch 131/500\n",
      "256/256 [==============================] - 0s 44us/step - loss: 9.6959 - acc: 0.5000 - val_loss: 7.5426 - val_acc: 0.6400\n",
      "Epoch 132/500\n",
      "256/256 [==============================] - 0s 31us/step - loss: 9.6910 - acc: 0.5000 - val_loss: 7.5379 - val_acc: 0.6400\n",
      "Epoch 133/500\n",
      "256/256 [==============================] - 0s 35us/step - loss: 9.6861 - acc: 0.5000 - val_loss: 7.5331 - val_acc: 0.6400\n",
      "Epoch 134/500\n",
      "256/256 [==============================] - 0s 61us/step - loss: 9.6812 - acc: 0.5000 - val_loss: 7.5284 - val_acc: 0.6400\n",
      "Epoch 135/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 9.6763 - acc: 0.5000 - val_loss: 7.5236 - val_acc: 0.6400\n",
      "Epoch 136/500\n",
      "256/256 [==============================] - 0s 49us/step - loss: 9.6714 - acc: 0.5000 - val_loss: 7.5190 - val_acc: 0.6400\n",
      "Epoch 137/500\n",
      "256/256 [==============================] - 0s 56us/step - loss: 9.6665 - acc: 0.5000 - val_loss: 7.5143 - val_acc: 0.6400\n",
      "Epoch 138/500\n",
      "256/256 [==============================] - 0s 63us/step - loss: 9.6616 - acc: 0.5000 - val_loss: 7.5096 - val_acc: 0.6400\n",
      "Epoch 139/500\n",
      "256/256 [==============================] - 0s 40us/step - loss: 9.6568 - acc: 0.5000 - val_loss: 7.5050 - val_acc: 0.6400\n",
      "Epoch 140/500\n",
      "256/256 [==============================] - 0s 45us/step - loss: 9.6520 - acc: 0.5000 - val_loss: 7.5003 - val_acc: 0.6400\n",
      "Epoch 141/500\n",
      "256/256 [==============================] - 0s 42us/step - loss: 9.6472 - acc: 0.5000 - val_loss: 7.4957 - val_acc: 0.6400\n",
      "Epoch 142/500\n",
      "256/256 [==============================] - 0s 62us/step - loss: 9.6423 - acc: 0.5000 - val_loss: 7.4910 - val_acc: 0.6400\n",
      "Epoch 143/500\n",
      "256/256 [==============================] - 0s 44us/step - loss: 9.6375 - acc: 0.5000 - val_loss: 7.4864 - val_acc: 0.6400\n",
      "Epoch 144/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 9.6328 - acc: 0.5000 - val_loss: 7.4818 - val_acc: 0.6400\n",
      "Epoch 145/500\n",
      "256/256 [==============================] - 0s 49us/step - loss: 9.6280 - acc: 0.5000 - val_loss: 7.4771 - val_acc: 0.6400\n",
      "Epoch 146/500\n",
      "256/256 [==============================] - 0s 59us/step - loss: 9.6232 - acc: 0.5000 - val_loss: 7.4726 - val_acc: 0.6400\n",
      "Epoch 147/500\n",
      "256/256 [==============================] - 0s 64us/step - loss: 9.6185 - acc: 0.5000 - val_loss: 7.4680 - val_acc: 0.6400\n",
      "Epoch 148/500\n",
      "256/256 [==============================] - 0s 67us/step - loss: 9.6137 - acc: 0.5000 - val_loss: 7.4635 - val_acc: 0.6400\n",
      "Epoch 149/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 9.6090 - acc: 0.5000 - val_loss: 7.4589 - val_acc: 0.6400\n",
      "Epoch 150/500\n",
      "256/256 [==============================] - 0s 30us/step - loss: 9.6043 - acc: 0.5000 - val_loss: 7.4543 - val_acc: 0.6400\n",
      "Epoch 151/500\n",
      "256/256 [==============================] - 0s 36us/step - loss: 9.5996 - acc: 0.5000 - val_loss: 7.4497 - val_acc: 0.6400\n",
      "Epoch 152/500\n",
      "256/256 [==============================] - 0s 42us/step - loss: 9.5949 - acc: 0.5000 - val_loss: 7.4453 - val_acc: 0.6400\n",
      "Epoch 153/500\n",
      "256/256 [==============================] - 0s 52us/step - loss: 9.5903 - acc: 0.5000 - val_loss: 7.4409 - val_acc: 0.6400\n",
      "Epoch 154/500\n",
      "256/256 [==============================] - 0s 58us/step - loss: 9.5856 - acc: 0.5000 - val_loss: 7.4364 - val_acc: 0.6400\n",
      "Epoch 155/500\n",
      "256/256 [==============================] - 0s 56us/step - loss: 9.5809 - acc: 0.5000 - val_loss: 7.4320 - val_acc: 0.6400\n",
      "Epoch 156/500\n",
      "256/256 [==============================] - 0s 49us/step - loss: 9.5763 - acc: 0.5000 - val_loss: 7.4276 - val_acc: 0.6400\n",
      "Epoch 157/500\n",
      "256/256 [==============================] - 0s 60us/step - loss: 9.5717 - acc: 0.5000 - val_loss: 7.4233 - val_acc: 0.6400\n",
      "Epoch 158/500\n",
      "256/256 [==============================] - 0s 62us/step - loss: 9.5671 - acc: 0.5000 - val_loss: 7.4190 - val_acc: 0.6400\n",
      "Epoch 159/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 9.5625 - acc: 0.5000 - val_loss: 7.4147 - val_acc: 0.6400\n",
      "Epoch 160/500\n",
      "256/256 [==============================] - 0s 66us/step - loss: 9.5579 - acc: 0.5000 - val_loss: 7.4103 - val_acc: 0.6400\n",
      "Epoch 161/500\n",
      "256/256 [==============================] - 0s 63us/step - loss: 9.5533 - acc: 0.5000 - val_loss: 7.4061 - val_acc: 0.6400\n",
      "Epoch 162/500\n",
      "256/256 [==============================] - 0s 41us/step - loss: 9.5488 - acc: 0.5000 - val_loss: 7.4018 - val_acc: 0.6400\n",
      "Epoch 163/500\n",
      "256/256 [==============================] - 0s 43us/step - loss: 9.5443 - acc: 0.5000 - val_loss: 7.3975 - val_acc: 0.6400\n",
      "Epoch 164/500\n",
      "256/256 [==============================] - 0s 22us/step - loss: 9.5397 - acc: 0.5000 - val_loss: 7.3932 - val_acc: 0.6400\n",
      "Epoch 165/500\n",
      "256/256 [==============================] - 0s 41us/step - loss: 9.5351 - acc: 0.5000 - val_loss: 7.3890 - val_acc: 0.6400\n",
      "Epoch 166/500\n",
      "256/256 [==============================] - 0s 38us/step - loss: 9.5306 - acc: 0.5000 - val_loss: 7.3848 - val_acc: 0.6400\n",
      "Epoch 167/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 9.5261 - acc: 0.5000 - val_loss: 7.3806 - val_acc: 0.6400\n",
      "Epoch 168/500\n",
      "256/256 [==============================] - 0s 64us/step - loss: 9.5216 - acc: 0.5000 - val_loss: 7.3764 - val_acc: 0.6400\n",
      "Epoch 169/500\n",
      "256/256 [==============================] - 0s 35us/step - loss: 9.5172 - acc: 0.5000 - val_loss: 7.3722 - val_acc: 0.6400\n",
      "Epoch 170/500\n",
      "256/256 [==============================] - 0s 46us/step - loss: 9.5127 - acc: 0.5000 - val_loss: 7.3680 - val_acc: 0.6400\n",
      "Epoch 171/500\n",
      "256/256 [==============================] - 0s 28us/step - loss: 9.5082 - acc: 0.5000 - val_loss: 7.3638 - val_acc: 0.6400\n",
      "Epoch 172/500\n",
      "256/256 [==============================] - 0s 40us/step - loss: 9.5038 - acc: 0.5000 - val_loss: 7.3596 - val_acc: 0.6400\n",
      "Epoch 173/500\n",
      "256/256 [==============================] - 0s 26us/step - loss: 9.4994 - acc: 0.5000 - val_loss: 7.3553 - val_acc: 0.6400\n",
      "Epoch 174/500\n",
      "256/256 [==============================] - 0s 38us/step - loss: 9.4949 - acc: 0.5000 - val_loss: 7.3511 - val_acc: 0.6400\n",
      "Epoch 175/500\n",
      "256/256 [==============================] - 0s 75us/step - loss: 9.4906 - acc: 0.5000 - val_loss: 7.3470 - val_acc: 0.6400\n",
      "Epoch 176/500\n",
      "256/256 [==============================] - 0s 54us/step - loss: 9.4861 - acc: 0.5000 - val_loss: 7.3429 - val_acc: 0.6400\n",
      "Epoch 177/500\n",
      "256/256 [==============================] - 0s 55us/step - loss: 9.4818 - acc: 0.5000 - val_loss: 7.3388 - val_acc: 0.6400\n",
      "Epoch 178/500\n",
      "256/256 [==============================] - 0s 41us/step - loss: 9.4774 - acc: 0.5000 - val_loss: 7.3347 - val_acc: 0.6400\n",
      "Epoch 179/500\n",
      "256/256 [==============================] - 0s 33us/step - loss: 9.4730 - acc: 0.5000 - val_loss: 7.3306 - val_acc: 0.6400\n",
      "Epoch 180/500\n",
      "256/256 [==============================] - 0s 29us/step - loss: 9.4687 - acc: 0.5000 - val_loss: 7.3265 - val_acc: 0.6400\n",
      "Epoch 181/500\n",
      "256/256 [==============================] - 0s 38us/step - loss: 9.4644 - acc: 0.5000 - val_loss: 7.3224 - val_acc: 0.6400\n",
      "Epoch 182/500\n",
      "256/256 [==============================] - 0s 28us/step - loss: 9.4600 - acc: 0.5000 - val_loss: 7.3183 - val_acc: 0.6400\n",
      "Epoch 183/500\n",
      "256/256 [==============================] - 0s 33us/step - loss: 9.4557 - acc: 0.5000 - val_loss: 7.3142 - val_acc: 0.6400\n",
      "Epoch 184/500\n",
      "256/256 [==============================] - 0s 62us/step - loss: 9.4514 - acc: 0.5000 - val_loss: 7.3102 - val_acc: 0.6400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 185/500\n",
      "256/256 [==============================] - 0s 43us/step - loss: 9.4471 - acc: 0.5000 - val_loss: 7.3061 - val_acc: 0.6400\n",
      "Epoch 186/500\n",
      "256/256 [==============================] - 0s 66us/step - loss: 9.4428 - acc: 0.5000 - val_loss: 7.3021 - val_acc: 0.6400\n",
      "Epoch 187/500\n",
      "256/256 [==============================] - 0s 43us/step - loss: 9.4386 - acc: 0.5000 - val_loss: 7.2982 - val_acc: 0.6400\n",
      "Epoch 188/500\n",
      "256/256 [==============================] - 0s 44us/step - loss: 9.4343 - acc: 0.5000 - val_loss: 7.2942 - val_acc: 0.6400\n",
      "Epoch 189/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 9.4301 - acc: 0.5000 - val_loss: 7.2903 - val_acc: 0.6400\n",
      "Epoch 190/500\n",
      "256/256 [==============================] - 0s 50us/step - loss: 9.4259 - acc: 0.5000 - val_loss: 7.2864 - val_acc: 0.6400\n",
      "Epoch 191/500\n",
      "256/256 [==============================] - 0s 56us/step - loss: 9.4216 - acc: 0.5000 - val_loss: 7.2825 - val_acc: 0.6400\n",
      "Epoch 192/500\n",
      "256/256 [==============================] - 0s 67us/step - loss: 9.4174 - acc: 0.5000 - val_loss: 7.2785 - val_acc: 0.6400\n",
      "Epoch 193/500\n",
      "256/256 [==============================] - 0s 62us/step - loss: 9.4132 - acc: 0.5000 - val_loss: 7.2746 - val_acc: 0.6400\n",
      "Epoch 194/500\n",
      "256/256 [==============================] - 0s 54us/step - loss: 9.4090 - acc: 0.5000 - val_loss: 7.2707 - val_acc: 0.6400\n",
      "Epoch 195/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 9.4048 - acc: 0.5000 - val_loss: 7.2669 - val_acc: 0.6400\n",
      "Epoch 196/500\n",
      "256/256 [==============================] - 0s 67us/step - loss: 9.4007 - acc: 0.5000 - val_loss: 7.2631 - val_acc: 0.6400\n",
      "Epoch 197/500\n",
      "256/256 [==============================] - 0s 40us/step - loss: 9.3965 - acc: 0.5000 - val_loss: 7.2592 - val_acc: 0.6400\n",
      "Epoch 198/500\n",
      "256/256 [==============================] - 0s 33us/step - loss: 9.3924 - acc: 0.5000 - val_loss: 7.2554 - val_acc: 0.6400\n",
      "Epoch 199/500\n",
      "256/256 [==============================] - 0s 36us/step - loss: 9.3882 - acc: 0.5000 - val_loss: 7.2516 - val_acc: 0.6400\n",
      "Epoch 200/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 9.3841 - acc: 0.5000 - val_loss: 7.2478 - val_acc: 0.6400\n",
      "Epoch 201/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 9.3800 - acc: 0.5000 - val_loss: 7.2440 - val_acc: 0.6400\n",
      "Epoch 202/500\n",
      "256/256 [==============================] - 0s 46us/step - loss: 9.3759 - acc: 0.5000 - val_loss: 7.2403 - val_acc: 0.6400\n",
      "Epoch 203/500\n",
      "256/256 [==============================] - 0s 37us/step - loss: 9.3718 - acc: 0.5000 - val_loss: 7.2366 - val_acc: 0.6400\n",
      "Epoch 204/500\n",
      "256/256 [==============================] - 0s 44us/step - loss: 9.3678 - acc: 0.5000 - val_loss: 7.2328 - val_acc: 0.6400\n",
      "Epoch 205/500\n",
      "256/256 [==============================] - 0s 61us/step - loss: 9.3637 - acc: 0.5000 - val_loss: 7.2291 - val_acc: 0.6400\n",
      "Epoch 206/500\n",
      "256/256 [==============================] - 0s 52us/step - loss: 9.3596 - acc: 0.5000 - val_loss: 7.2255 - val_acc: 0.6400\n",
      "Epoch 207/500\n",
      "256/256 [==============================] - 0s 64us/step - loss: 9.3556 - acc: 0.5000 - val_loss: 7.2218 - val_acc: 0.6400\n",
      "Epoch 208/500\n",
      "256/256 [==============================] - 0s 61us/step - loss: 9.3515 - acc: 0.5000 - val_loss: 7.2180 - val_acc: 0.6400\n",
      "Epoch 209/500\n",
      "256/256 [==============================] - 0s 29us/step - loss: 9.3475 - acc: 0.5000 - val_loss: 7.2143 - val_acc: 0.6400\n",
      "Epoch 210/500\n",
      "256/256 [==============================] - 0s 44us/step - loss: 9.3435 - acc: 0.5000 - val_loss: 7.2107 - val_acc: 0.6400\n",
      "Epoch 211/500\n",
      "256/256 [==============================] - 0s 42us/step - loss: 9.3395 - acc: 0.5000 - val_loss: 7.2071 - val_acc: 0.6400\n",
      "Epoch 212/500\n",
      "256/256 [==============================] - 0s 34us/step - loss: 9.3355 - acc: 0.5000 - val_loss: 7.2034 - val_acc: 0.6400\n",
      "Epoch 213/500\n",
      "256/256 [==============================] - 0s 50us/step - loss: 9.3315 - acc: 0.5000 - val_loss: 7.1998 - val_acc: 0.6400\n",
      "Epoch 214/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 9.3276 - acc: 0.5000 - val_loss: 7.1962 - val_acc: 0.6400\n",
      "Epoch 215/500\n",
      "256/256 [==============================] - 0s 41us/step - loss: 9.3236 - acc: 0.5000 - val_loss: 7.1927 - val_acc: 0.6400\n",
      "Epoch 216/500\n",
      "256/256 [==============================] - 0s 35us/step - loss: 9.3197 - acc: 0.5000 - val_loss: 7.1892 - val_acc: 0.6400\n",
      "Epoch 217/500\n",
      "256/256 [==============================] - 0s 34us/step - loss: 9.3157 - acc: 0.5000 - val_loss: 7.1857 - val_acc: 0.6400\n",
      "Epoch 218/500\n",
      "256/256 [==============================] - 0s 18us/step - loss: 9.3118 - acc: 0.5000 - val_loss: 7.1822 - val_acc: 0.6400\n",
      "Epoch 219/500\n",
      "256/256 [==============================] - 0s 35us/step - loss: 9.3079 - acc: 0.5000 - val_loss: 7.1788 - val_acc: 0.6400\n",
      "Epoch 220/500\n",
      "256/256 [==============================] - 0s 33us/step - loss: 9.3040 - acc: 0.5000 - val_loss: 7.1753 - val_acc: 0.6400\n",
      "Epoch 221/500\n",
      "256/256 [==============================] - 0s 58us/step - loss: 9.3001 - acc: 0.5000 - val_loss: 7.1718 - val_acc: 0.6400\n",
      "Epoch 222/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 9.2962 - acc: 0.5000 - val_loss: 7.1683 - val_acc: 0.6400\n",
      "Epoch 223/500\n",
      "256/256 [==============================] - 0s 35us/step - loss: 9.2924 - acc: 0.5000 - val_loss: 7.1649 - val_acc: 0.6400\n",
      "Epoch 224/500\n",
      "256/256 [==============================] - 0s 39us/step - loss: 9.2885 - acc: 0.5000 - val_loss: 7.1614 - val_acc: 0.6400\n",
      "Epoch 225/500\n",
      "256/256 [==============================] - 0s 65us/step - loss: 9.2847 - acc: 0.5000 - val_loss: 7.1580 - val_acc: 0.6400\n",
      "Epoch 226/500\n",
      "256/256 [==============================] - 0s 45us/step - loss: 9.2808 - acc: 0.5000 - val_loss: 7.1545 - val_acc: 0.6400\n",
      "Epoch 227/500\n",
      "256/256 [==============================] - 0s 41us/step - loss: 9.2770 - acc: 0.5000 - val_loss: 7.1511 - val_acc: 0.6400\n",
      "Epoch 228/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 9.2731 - acc: 0.5000 - val_loss: 7.1477 - val_acc: 0.6400\n",
      "Epoch 229/500\n",
      "256/256 [==============================] - 0s 43us/step - loss: 9.2694 - acc: 0.5000 - val_loss: 7.1443 - val_acc: 0.6400\n",
      "Epoch 230/500\n",
      "256/256 [==============================] - 0s 55us/step - loss: 9.2655 - acc: 0.5000 - val_loss: 7.1409 - val_acc: 0.6400\n",
      "Epoch 231/500\n",
      "256/256 [==============================] - 0s 60us/step - loss: 9.2618 - acc: 0.5000 - val_loss: 7.1375 - val_acc: 0.6400\n",
      "Epoch 232/500\n",
      "256/256 [==============================] - 0s 55us/step - loss: 9.2580 - acc: 0.5000 - val_loss: 7.1341 - val_acc: 0.6400\n",
      "Epoch 233/500\n",
      "256/256 [==============================] - 0s 50us/step - loss: 9.2542 - acc: 0.5000 - val_loss: 7.1307 - val_acc: 0.6400\n",
      "Epoch 234/500\n",
      "256/256 [==============================] - 0s 28us/step - loss: 9.2504 - acc: 0.5000 - val_loss: 7.1272 - val_acc: 0.6400\n",
      "Epoch 235/500\n",
      "256/256 [==============================] - 0s 21us/step - loss: 9.2467 - acc: 0.5000 - val_loss: 7.1238 - val_acc: 0.6400\n",
      "Epoch 236/500\n",
      "256/256 [==============================] - 0s 38us/step - loss: 9.2430 - acc: 0.5000 - val_loss: 7.1203 - val_acc: 0.6400\n",
      "Epoch 237/500\n",
      "256/256 [==============================] - 0s 28us/step - loss: 9.2392 - acc: 0.5000 - val_loss: 7.1169 - val_acc: 0.6400\n",
      "Epoch 238/500\n",
      "256/256 [==============================] - 0s 31us/step - loss: 9.2355 - acc: 0.5000 - val_loss: 7.1135 - val_acc: 0.6400\n",
      "Epoch 239/500\n",
      "256/256 [==============================] - 0s 36us/step - loss: 9.2318 - acc: 0.5000 - val_loss: 7.1101 - val_acc: 0.6400\n",
      "Epoch 240/500\n",
      "256/256 [==============================] - 0s 42us/step - loss: 9.2281 - acc: 0.5000 - val_loss: 7.1067 - val_acc: 0.6400\n",
      "Epoch 241/500\n",
      "256/256 [==============================] - 0s 45us/step - loss: 9.2244 - acc: 0.5000 - val_loss: 7.1034 - val_acc: 0.6400\n",
      "Epoch 242/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 9.2207 - acc: 0.5000 - val_loss: 7.1001 - val_acc: 0.6400\n",
      "Epoch 243/500\n",
      "256/256 [==============================] - 0s 56us/step - loss: 9.2170 - acc: 0.5000 - val_loss: 7.0969 - val_acc: 0.6400\n",
      "Epoch 244/500\n",
      "256/256 [==============================] - 0s 46us/step - loss: 9.2134 - acc: 0.5000 - val_loss: 7.0936 - val_acc: 0.6400\n",
      "Epoch 245/500\n",
      "256/256 [==============================] - 0s 33us/step - loss: 9.2097 - acc: 0.5000 - val_loss: 7.0903 - val_acc: 0.6400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 246/500\n",
      "256/256 [==============================] - 0s 37us/step - loss: 9.2061 - acc: 0.5000 - val_loss: 7.0870 - val_acc: 0.6400\n",
      "Epoch 247/500\n",
      "256/256 [==============================] - 0s 39us/step - loss: 9.2025 - acc: 0.5000 - val_loss: 7.0836 - val_acc: 0.6400\n",
      "Epoch 248/500\n",
      "256/256 [==============================] - 0s 31us/step - loss: 9.1988 - acc: 0.5000 - val_loss: 7.0803 - val_acc: 0.6400\n",
      "Epoch 249/500\n",
      "256/256 [==============================] - 0s 70us/step - loss: 9.1952 - acc: 0.5000 - val_loss: 7.0770 - val_acc: 0.6400\n",
      "Epoch 250/500\n",
      "256/256 [==============================] - 0s 56us/step - loss: 9.1916 - acc: 0.5000 - val_loss: 7.0736 - val_acc: 0.6400\n",
      "Epoch 251/500\n",
      "256/256 [==============================] - 0s 59us/step - loss: 9.1880 - acc: 0.5000 - val_loss: 7.0704 - val_acc: 0.6400\n",
      "Epoch 252/500\n",
      "256/256 [==============================] - 0s 57us/step - loss: 9.1844 - acc: 0.5000 - val_loss: 7.0672 - val_acc: 0.6400\n",
      "Epoch 253/500\n",
      "256/256 [==============================] - 0s 59us/step - loss: 9.1809 - acc: 0.5000 - val_loss: 7.0640 - val_acc: 0.6400\n",
      "Epoch 254/500\n",
      "256/256 [==============================] - 0s 65us/step - loss: 9.1773 - acc: 0.5000 - val_loss: 7.0608 - val_acc: 0.6400\n",
      "Epoch 255/500\n",
      "256/256 [==============================] - 0s 56us/step - loss: 9.1738 - acc: 0.5000 - val_loss: 7.0575 - val_acc: 0.6400\n",
      "Epoch 256/500\n",
      "256/256 [==============================] - 0s 58us/step - loss: 9.1702 - acc: 0.5000 - val_loss: 7.0543 - val_acc: 0.6400\n",
      "Epoch 257/500\n",
      "256/256 [==============================] - 0s 56us/step - loss: 9.1667 - acc: 0.5000 - val_loss: 7.0511 - val_acc: 0.6400\n",
      "Epoch 258/500\n",
      "256/256 [==============================] - 0s 67us/step - loss: 9.1632 - acc: 0.5000 - val_loss: 7.0480 - val_acc: 0.6400\n",
      "Epoch 259/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 9.1596 - acc: 0.5000 - val_loss: 7.0449 - val_acc: 0.6400\n",
      "Epoch 260/500\n",
      "256/256 [==============================] - 0s 59us/step - loss: 9.1561 - acc: 0.5000 - val_loss: 7.0418 - val_acc: 0.6400\n",
      "Epoch 261/500\n",
      "256/256 [==============================] - 0s 54us/step - loss: 9.1526 - acc: 0.5000 - val_loss: 7.0387 - val_acc: 0.6400\n",
      "Epoch 262/500\n",
      "256/256 [==============================] - 0s 58us/step - loss: 9.1492 - acc: 0.5000 - val_loss: 7.0357 - val_acc: 0.6400\n",
      "Epoch 263/500\n",
      "256/256 [==============================] - 0s 55us/step - loss: 9.1457 - acc: 0.5000 - val_loss: 7.0326 - val_acc: 0.6400\n",
      "Epoch 264/500\n",
      "256/256 [==============================] - 0s 49us/step - loss: 9.1422 - acc: 0.5000 - val_loss: 7.0296 - val_acc: 0.6400\n",
      "Epoch 265/500\n",
      "256/256 [==============================] - 0s 41us/step - loss: 9.1388 - acc: 0.5000 - val_loss: 7.0265 - val_acc: 0.6400\n",
      "Epoch 266/500\n",
      "256/256 [==============================] - 0s 42us/step - loss: 9.1353 - acc: 0.5000 - val_loss: 7.0235 - val_acc: 0.6400\n",
      "Epoch 267/500\n",
      "256/256 [==============================] - 0s 25us/step - loss: 9.1319 - acc: 0.5000 - val_loss: 7.0205 - val_acc: 0.6400\n",
      "Epoch 268/500\n",
      "256/256 [==============================] - 0s 27us/step - loss: 9.1284 - acc: 0.5000 - val_loss: 7.0175 - val_acc: 0.6400\n",
      "Epoch 269/500\n",
      "256/256 [==============================] - 0s 36us/step - loss: 9.1250 - acc: 0.5000 - val_loss: 7.0144 - val_acc: 0.6400\n",
      "Epoch 270/500\n",
      "256/256 [==============================] - 0s 45us/step - loss: 9.1216 - acc: 0.5000 - val_loss: 7.0114 - val_acc: 0.6400\n",
      "Epoch 271/500\n",
      "256/256 [==============================] - 0s 59us/step - loss: 9.1182 - acc: 0.5000 - val_loss: 7.0083 - val_acc: 0.6400\n",
      "Epoch 272/500\n",
      "256/256 [==============================] - 0s 59us/step - loss: 9.1148 - acc: 0.5000 - val_loss: 7.0053 - val_acc: 0.6400\n",
      "Epoch 273/500\n",
      "256/256 [==============================] - 0s 63us/step - loss: 9.1114 - acc: 0.5000 - val_loss: 7.0022 - val_acc: 0.6400\n",
      "Epoch 274/500\n",
      "256/256 [==============================] - 0s 42us/step - loss: 9.1081 - acc: 0.5000 - val_loss: 6.9992 - val_acc: 0.6400\n",
      "Epoch 275/500\n",
      "256/256 [==============================] - 0s 60us/step - loss: 9.1047 - acc: 0.5000 - val_loss: 6.9962 - val_acc: 0.6400\n",
      "Epoch 276/500\n",
      "256/256 [==============================] - 0s 46us/step - loss: 9.1013 - acc: 0.5000 - val_loss: 6.9932 - val_acc: 0.6400\n",
      "Epoch 277/500\n",
      "256/256 [==============================] - 0s 41us/step - loss: 9.0980 - acc: 0.5000 - val_loss: 6.9902 - val_acc: 0.6400\n",
      "Epoch 278/500\n",
      "256/256 [==============================] - 0s 23us/step - loss: 9.0947 - acc: 0.5000 - val_loss: 6.9873 - val_acc: 0.6400\n",
      "Epoch 279/500\n",
      "256/256 [==============================] - 0s 39us/step - loss: 9.0913 - acc: 0.5000 - val_loss: 6.9844 - val_acc: 0.6400\n",
      "Epoch 280/500\n",
      "256/256 [==============================] - 0s 31us/step - loss: 9.0880 - acc: 0.5000 - val_loss: 6.9815 - val_acc: 0.6400\n",
      "Epoch 281/500\n",
      "256/256 [==============================] - 0s 50us/step - loss: 9.0847 - acc: 0.5000 - val_loss: 6.9784 - val_acc: 0.6400\n",
      "Epoch 282/500\n",
      "256/256 [==============================] - 0s 37us/step - loss: 9.0814 - acc: 0.5000 - val_loss: 6.9753 - val_acc: 0.6400\n",
      "Epoch 283/500\n",
      "256/256 [==============================] - 0s 36us/step - loss: 9.0781 - acc: 0.5000 - val_loss: 6.9721 - val_acc: 0.6400\n",
      "Epoch 284/500\n",
      "256/256 [==============================] - 0s 34us/step - loss: 9.0748 - acc: 0.5000 - val_loss: 6.9689 - val_acc: 0.6400\n",
      "Epoch 285/500\n",
      "256/256 [==============================] - 0s 38us/step - loss: 9.0716 - acc: 0.5000 - val_loss: 6.9658 - val_acc: 0.6400\n",
      "Epoch 286/500\n",
      "256/256 [==============================] - 0s 16us/step - loss: 9.0683 - acc: 0.5000 - val_loss: 6.9627 - val_acc: 0.6400\n",
      "Epoch 287/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 9.0651 - acc: 0.5000 - val_loss: 6.9597 - val_acc: 0.6400\n",
      "Epoch 288/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 9.0618 - acc: 0.5000 - val_loss: 6.9567 - val_acc: 0.6400\n",
      "Epoch 289/500\n",
      "256/256 [==============================] - 0s 62us/step - loss: 9.0586 - acc: 0.5000 - val_loss: 6.9537 - val_acc: 0.6400\n",
      "Epoch 290/500\n",
      "256/256 [==============================] - 0s 50us/step - loss: 9.0553 - acc: 0.5000 - val_loss: 6.9507 - val_acc: 0.6400\n",
      "Epoch 291/500\n",
      "256/256 [==============================] - 0s 46us/step - loss: 9.0521 - acc: 0.5000 - val_loss: 6.9478 - val_acc: 0.6400\n",
      "Epoch 292/500\n",
      "256/256 [==============================] - 0s 43us/step - loss: 9.0489 - acc: 0.5000 - val_loss: 6.9448 - val_acc: 0.6400\n",
      "Epoch 293/500\n",
      "256/256 [==============================] - 0s 28us/step - loss: 9.0457 - acc: 0.5000 - val_loss: 6.9419 - val_acc: 0.6400\n",
      "Epoch 294/500\n",
      "256/256 [==============================] - 0s 50us/step - loss: 9.0425 - acc: 0.5000 - val_loss: 6.9389 - val_acc: 0.6400\n",
      "Epoch 295/500\n",
      "256/256 [==============================] - 0s 40us/step - loss: 9.0394 - acc: 0.5000 - val_loss: 6.9360 - val_acc: 0.6400\n",
      "Epoch 296/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 9.0362 - acc: 0.5000 - val_loss: 6.9331 - val_acc: 0.6400\n",
      "Epoch 297/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 9.0330 - acc: 0.5000 - val_loss: 6.9302 - val_acc: 0.6400\n",
      "Epoch 298/500\n",
      "256/256 [==============================] - 0s 25us/step - loss: 9.0299 - acc: 0.5000 - val_loss: 6.9274 - val_acc: 0.6400\n",
      "Epoch 299/500\n",
      "256/256 [==============================] - 0s 54us/step - loss: 9.0267 - acc: 0.5000 - val_loss: 6.9246 - val_acc: 0.6400\n",
      "Epoch 300/500\n",
      "256/256 [==============================] - 0s 29us/step - loss: 9.0236 - acc: 0.5000 - val_loss: 6.9218 - val_acc: 0.6400\n",
      "Epoch 301/500\n",
      "256/256 [==============================] - 0s 26us/step - loss: 9.0205 - acc: 0.5000 - val_loss: 6.9190 - val_acc: 0.6400\n",
      "Epoch 302/500\n",
      "256/256 [==============================] - 0s 44us/step - loss: 9.0173 - acc: 0.5000 - val_loss: 6.9162 - val_acc: 0.6400\n",
      "Epoch 303/500\n",
      "256/256 [==============================] - 0s 38us/step - loss: 9.0142 - acc: 0.5000 - val_loss: 6.9134 - val_acc: 0.6400\n",
      "Epoch 304/500\n",
      "256/256 [==============================] - 0s 54us/step - loss: 9.0111 - acc: 0.5000 - val_loss: 6.9107 - val_acc: 0.6400\n",
      "Epoch 305/500\n",
      "256/256 [==============================] - 0s 57us/step - loss: 9.0080 - acc: 0.5000 - val_loss: 6.9078 - val_acc: 0.6400\n",
      "Epoch 306/500\n",
      "256/256 [==============================] - 0s 55us/step - loss: 9.0049 - acc: 0.5000 - val_loss: 6.9050 - val_acc: 0.6400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 307/500\n",
      "256/256 [==============================] - 0s 52us/step - loss: 9.0019 - acc: 0.5000 - val_loss: 6.9022 - val_acc: 0.6400\n",
      "Epoch 308/500\n",
      "256/256 [==============================] - 0s 55us/step - loss: 8.9988 - acc: 0.5000 - val_loss: 6.8994 - val_acc: 0.6400\n",
      "Epoch 309/500\n",
      "256/256 [==============================] - 0s 24us/step - loss: 8.9957 - acc: 0.5000 - val_loss: 6.8965 - val_acc: 0.6400\n",
      "Epoch 310/500\n",
      "256/256 [==============================] - 0s 45us/step - loss: 8.9927 - acc: 0.5000 - val_loss: 6.8936 - val_acc: 0.6400\n",
      "Epoch 311/500\n",
      "256/256 [==============================] - 0s 21us/step - loss: 8.9896 - acc: 0.5000 - val_loss: 6.8908 - val_acc: 0.6400\n",
      "Epoch 312/500\n",
      "256/256 [==============================] - 0s 49us/step - loss: 8.9866 - acc: 0.5000 - val_loss: 6.8880 - val_acc: 0.6400\n",
      "Epoch 313/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 8.9836 - acc: 0.5000 - val_loss: 6.8851 - val_acc: 0.6400\n",
      "Epoch 314/500\n",
      "256/256 [==============================] - 0s 57us/step - loss: 8.9806 - acc: 0.5000 - val_loss: 6.8823 - val_acc: 0.6400\n",
      "Epoch 315/500\n",
      "256/256 [==============================] - 0s 54us/step - loss: 8.9775 - acc: 0.5000 - val_loss: 6.8795 - val_acc: 0.6400\n",
      "Epoch 316/500\n",
      "256/256 [==============================] - 0s 60us/step - loss: 8.9745 - acc: 0.5000 - val_loss: 6.8767 - val_acc: 0.6400\n",
      "Epoch 317/500\n",
      "256/256 [==============================] - 0s 65us/step - loss: 8.9716 - acc: 0.5000 - val_loss: 6.8740 - val_acc: 0.6400\n",
      "Epoch 318/500\n",
      "256/256 [==============================] - 0s 66us/step - loss: 8.9686 - acc: 0.5000 - val_loss: 6.8712 - val_acc: 0.6400\n",
      "Epoch 319/500\n",
      "256/256 [==============================] - 0s 52us/step - loss: 8.9656 - acc: 0.5000 - val_loss: 6.8686 - val_acc: 0.6400\n",
      "Epoch 320/500\n",
      "256/256 [==============================] - 0s 41us/step - loss: 8.9626 - acc: 0.5000 - val_loss: 6.8659 - val_acc: 0.6400\n",
      "Epoch 321/500\n",
      "256/256 [==============================] - 0s 22us/step - loss: 8.9597 - acc: 0.5000 - val_loss: 6.8633 - val_acc: 0.6400\n",
      "Epoch 322/500\n",
      "256/256 [==============================] - 0s 45us/step - loss: 8.9567 - acc: 0.5000 - val_loss: 6.8607 - val_acc: 0.6400\n",
      "Epoch 323/500\n",
      "256/256 [==============================] - 0s 43us/step - loss: 8.9538 - acc: 0.5000 - val_loss: 6.8581 - val_acc: 0.6400\n",
      "Epoch 324/500\n",
      "256/256 [==============================] - 0s 30us/step - loss: 8.9508 - acc: 0.5000 - val_loss: 6.8554 - val_acc: 0.6400\n",
      "Epoch 325/500\n",
      "256/256 [==============================] - 0s 65us/step - loss: 8.9479 - acc: 0.5000 - val_loss: 6.8528 - val_acc: 0.6400\n",
      "Epoch 326/500\n",
      "256/256 [==============================] - 0s 71us/step - loss: 8.9450 - acc: 0.5000 - val_loss: 6.8501 - val_acc: 0.6400\n",
      "Epoch 327/500\n",
      "256/256 [==============================] - 0s 46us/step - loss: 8.9421 - acc: 0.5000 - val_loss: 6.8474 - val_acc: 0.6400\n",
      "Epoch 328/500\n",
      "256/256 [==============================] - 0s 67us/step - loss: 8.9391 - acc: 0.5000 - val_loss: 6.8447 - val_acc: 0.6400\n",
      "Epoch 329/500\n",
      "256/256 [==============================] - 0s 61us/step - loss: 8.9363 - acc: 0.5000 - val_loss: 6.8420 - val_acc: 0.6400\n",
      "Epoch 330/500\n",
      "256/256 [==============================] - 0s 24us/step - loss: 8.9334 - acc: 0.5000 - val_loss: 6.8394 - val_acc: 0.6400\n",
      "Epoch 331/500\n",
      "256/256 [==============================] - 0s 39us/step - loss: 8.9305 - acc: 0.5000 - val_loss: 6.8368 - val_acc: 0.6400\n",
      "Epoch 332/500\n",
      "256/256 [==============================] - 0s 37us/step - loss: 8.9276 - acc: 0.5000 - val_loss: 6.8341 - val_acc: 0.6400\n",
      "Epoch 333/500\n",
      "256/256 [==============================] - 0s 41us/step - loss: 8.9247 - acc: 0.5000 - val_loss: 6.8315 - val_acc: 0.6400\n",
      "Epoch 334/500\n",
      "256/256 [==============================] - 0s 32us/step - loss: 8.9219 - acc: 0.5000 - val_loss: 6.8289 - val_acc: 0.6400\n",
      "Epoch 335/500\n",
      "256/256 [==============================] - 0s 33us/step - loss: 8.9190 - acc: 0.5000 - val_loss: 6.8264 - val_acc: 0.6400\n",
      "Epoch 336/500\n",
      "256/256 [==============================] - 0s 57us/step - loss: 8.9162 - acc: 0.5000 - val_loss: 6.8237 - val_acc: 0.6400\n",
      "Epoch 337/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 8.9134 - acc: 0.5000 - val_loss: 6.8212 - val_acc: 0.6400\n",
      "Epoch 338/500\n",
      "256/256 [==============================] - 0s 62us/step - loss: 8.9106 - acc: 0.5000 - val_loss: 6.8185 - val_acc: 0.6400\n",
      "Epoch 339/500\n",
      "256/256 [==============================] - 0s 67us/step - loss: 8.9077 - acc: 0.5000 - val_loss: 6.8159 - val_acc: 0.6400\n",
      "Epoch 340/500\n",
      "256/256 [==============================] - 0s 57us/step - loss: 8.9049 - acc: 0.5000 - val_loss: 6.8132 - val_acc: 0.6400\n",
      "Epoch 341/500\n",
      "256/256 [==============================] - 0s 41us/step - loss: 8.9021 - acc: 0.5000 - val_loss: 6.8105 - val_acc: 0.6400\n",
      "Epoch 342/500\n",
      "256/256 [==============================] - 0s 44us/step - loss: 8.8993 - acc: 0.5000 - val_loss: 6.8078 - val_acc: 0.6400\n",
      "Epoch 343/500\n",
      "256/256 [==============================] - 0s 57us/step - loss: 8.8965 - acc: 0.5000 - val_loss: 6.8052 - val_acc: 0.6400\n",
      "Epoch 344/500\n",
      "256/256 [==============================] - 0s 37us/step - loss: 8.8937 - acc: 0.5000 - val_loss: 6.8025 - val_acc: 0.6400\n",
      "Epoch 345/500\n",
      "256/256 [==============================] - 0s 69us/step - loss: 8.8910 - acc: 0.5000 - val_loss: 6.8000 - val_acc: 0.6400\n",
      "Epoch 346/500\n",
      "256/256 [==============================] - 0s 60us/step - loss: 8.8882 - acc: 0.5000 - val_loss: 6.7973 - val_acc: 0.6400\n",
      "Epoch 347/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 8.8855 - acc: 0.5000 - val_loss: 6.7947 - val_acc: 0.6400\n",
      "Epoch 348/500\n",
      "256/256 [==============================] - 0s 71us/step - loss: 8.8827 - acc: 0.5000 - val_loss: 6.7922 - val_acc: 0.6400\n",
      "Epoch 349/500\n",
      "256/256 [==============================] - 0s 56us/step - loss: 8.8800 - acc: 0.5000 - val_loss: 6.7896 - val_acc: 0.6400\n",
      "Epoch 350/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 8.8773 - acc: 0.5000 - val_loss: 6.7870 - val_acc: 0.6400\n",
      "Epoch 351/500\n",
      "256/256 [==============================] - 0s 59us/step - loss: 8.8745 - acc: 0.5000 - val_loss: 6.7844 - val_acc: 0.6400\n",
      "Epoch 352/500\n",
      "256/256 [==============================] - 0s 35us/step - loss: 8.8718 - acc: 0.5000 - val_loss: 6.7818 - val_acc: 0.6400\n",
      "Epoch 353/500\n",
      "256/256 [==============================] - 0s 24us/step - loss: 8.8691 - acc: 0.5000 - val_loss: 6.7793 - val_acc: 0.6400\n",
      "Epoch 354/500\n",
      "256/256 [==============================] - 0s 33us/step - loss: 8.8664 - acc: 0.5000 - val_loss: 6.7767 - val_acc: 0.6400\n",
      "Epoch 355/500\n",
      "256/256 [==============================] - 0s 33us/step - loss: 8.8637 - acc: 0.5000 - val_loss: 6.7741 - val_acc: 0.6400\n",
      "Epoch 356/500\n",
      "256/256 [==============================] - 0s 36us/step - loss: 8.8611 - acc: 0.5000 - val_loss: 6.7716 - val_acc: 0.6400\n",
      "Epoch 357/500\n",
      "256/256 [==============================] - 0s 44us/step - loss: 8.8584 - acc: 0.5000 - val_loss: 6.7690 - val_acc: 0.6400\n",
      "Epoch 358/500\n",
      "256/256 [==============================] - 0s 36us/step - loss: 8.8557 - acc: 0.5000 - val_loss: 6.7665 - val_acc: 0.6400\n",
      "Epoch 359/500\n",
      "256/256 [==============================] - 0s 46us/step - loss: 8.8530 - acc: 0.5000 - val_loss: 6.7641 - val_acc: 0.6400\n",
      "Epoch 360/500\n",
      "256/256 [==============================] - 0s 42us/step - loss: 8.8504 - acc: 0.5000 - val_loss: 6.7616 - val_acc: 0.6400\n",
      "Epoch 361/500\n",
      "256/256 [==============================] - 0s 26us/step - loss: 8.8478 - acc: 0.5000 - val_loss: 6.7590 - val_acc: 0.6400\n",
      "Epoch 362/500\n",
      "256/256 [==============================] - 0s 31us/step - loss: 8.8451 - acc: 0.5000 - val_loss: 6.7565 - val_acc: 0.6400\n",
      "Epoch 363/500\n",
      "256/256 [==============================] - 0s 24us/step - loss: 8.8425 - acc: 0.5000 - val_loss: 6.7541 - val_acc: 0.6400\n",
      "Epoch 364/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 8.8399 - acc: 0.5000 - val_loss: 6.7516 - val_acc: 0.6400\n",
      "Epoch 365/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 8.8372 - acc: 0.5000 - val_loss: 6.7491 - val_acc: 0.6400\n",
      "Epoch 366/500\n",
      "256/256 [==============================] - 0s 56us/step - loss: 8.8347 - acc: 0.5000 - val_loss: 6.7466 - val_acc: 0.6400\n",
      "Epoch 367/500\n",
      "256/256 [==============================] - 0s 41us/step - loss: 8.8321 - acc: 0.5000 - val_loss: 6.7440 - val_acc: 0.6400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 368/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 8.8294 - acc: 0.5000 - val_loss: 6.7414 - val_acc: 0.6400\n",
      "Epoch 369/500\n",
      "256/256 [==============================] - 0s 39us/step - loss: 8.8269 - acc: 0.5000 - val_loss: 6.7389 - val_acc: 0.6400\n",
      "Epoch 370/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 8.8243 - acc: 0.5000 - val_loss: 6.7364 - val_acc: 0.6400\n",
      "Epoch 371/500\n",
      "256/256 [==============================] - 0s 40us/step - loss: 8.8217 - acc: 0.5000 - val_loss: 6.7339 - val_acc: 0.6400\n",
      "Epoch 372/500\n",
      "256/256 [==============================] - 0s 41us/step - loss: 8.8191 - acc: 0.5000 - val_loss: 6.7315 - val_acc: 0.6400\n",
      "Epoch 373/500\n",
      "256/256 [==============================] - 0s 44us/step - loss: 8.8166 - acc: 0.5000 - val_loss: 6.7292 - val_acc: 0.6400\n",
      "Epoch 374/500\n",
      "256/256 [==============================] - 0s 43us/step - loss: 8.8140 - acc: 0.5000 - val_loss: 6.7269 - val_acc: 0.6400\n",
      "Epoch 375/500\n",
      "256/256 [==============================] - 0s 62us/step - loss: 8.8115 - acc: 0.5000 - val_loss: 6.7246 - val_acc: 0.6400\n",
      "Epoch 376/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 8.8090 - acc: 0.5000 - val_loss: 6.7222 - val_acc: 0.6400\n",
      "Epoch 377/500\n",
      "256/256 [==============================] - 0s 61us/step - loss: 8.8064 - acc: 0.5000 - val_loss: 6.7199 - val_acc: 0.6400\n",
      "Epoch 378/500\n",
      "256/256 [==============================] - 0s 45us/step - loss: 8.8039 - acc: 0.5000 - val_loss: 6.7175 - val_acc: 0.6400\n",
      "Epoch 379/500\n",
      "256/256 [==============================] - 0s 69us/step - loss: 8.8014 - acc: 0.5000 - val_loss: 6.7152 - val_acc: 0.6400\n",
      "Epoch 380/500\n",
      "256/256 [==============================] - 0s 61us/step - loss: 8.7989 - acc: 0.5000 - val_loss: 6.7128 - val_acc: 0.6400\n",
      "Epoch 381/500\n",
      "256/256 [==============================] - 0s 58us/step - loss: 8.7964 - acc: 0.5000 - val_loss: 6.7105 - val_acc: 0.6400\n",
      "Epoch 382/500\n",
      "256/256 [==============================] - 0s 66us/step - loss: 8.7939 - acc: 0.5000 - val_loss: 6.7082 - val_acc: 0.6400\n",
      "Epoch 383/500\n",
      "256/256 [==============================] - 0s 36us/step - loss: 8.7914 - acc: 0.5000 - val_loss: 6.7059 - val_acc: 0.6400\n",
      "Epoch 384/500\n",
      "256/256 [==============================] - 0s 60us/step - loss: 8.7890 - acc: 0.5000 - val_loss: 6.7037 - val_acc: 0.6400\n",
      "Epoch 385/500\n",
      "256/256 [==============================] - 0s 61us/step - loss: 8.7865 - acc: 0.5000 - val_loss: 6.7014 - val_acc: 0.6400\n",
      "Epoch 386/500\n",
      "256/256 [==============================] - 0s 64us/step - loss: 8.7840 - acc: 0.5000 - val_loss: 6.6991 - val_acc: 0.6400\n",
      "Epoch 387/500\n",
      "256/256 [==============================] - 0s 72us/step - loss: 8.7816 - acc: 0.5000 - val_loss: 6.6969 - val_acc: 0.6400\n",
      "Epoch 388/500\n",
      "256/256 [==============================] - 0s 44us/step - loss: 8.7791 - acc: 0.5000 - val_loss: 6.6947 - val_acc: 0.6400\n",
      "Epoch 389/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 8.7767 - acc: 0.5000 - val_loss: 6.6925 - val_acc: 0.6400\n",
      "Epoch 390/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 8.7742 - acc: 0.5000 - val_loss: 6.6903 - val_acc: 0.6400\n",
      "Epoch 391/500\n",
      "256/256 [==============================] - 0s 34us/step - loss: 8.7718 - acc: 0.5000 - val_loss: 6.6881 - val_acc: 0.6400\n",
      "Epoch 392/500\n",
      "256/256 [==============================] - 0s 34us/step - loss: 8.7694 - acc: 0.5000 - val_loss: 6.6860 - val_acc: 0.6400\n",
      "Epoch 393/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 8.7670 - acc: 0.5000 - val_loss: 6.6839 - val_acc: 0.6400\n",
      "Epoch 394/500\n",
      "256/256 [==============================] - 0s 29us/step - loss: 8.7646 - acc: 0.5000 - val_loss: 6.6818 - val_acc: 0.6400\n",
      "Epoch 395/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 8.7622 - acc: 0.5000 - val_loss: 6.6798 - val_acc: 0.6400\n",
      "Epoch 396/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 8.7598 - acc: 0.5000 - val_loss: 6.6779 - val_acc: 0.6400\n",
      "Epoch 397/500\n",
      "256/256 [==============================] - 0s 57us/step - loss: 8.7574 - acc: 0.5000 - val_loss: 6.6758 - val_acc: 0.6400\n",
      "Epoch 398/500\n",
      "256/256 [==============================] - 0s 38us/step - loss: 8.7550 - acc: 0.5000 - val_loss: 6.6738 - val_acc: 0.6400\n",
      "Epoch 399/500\n",
      "256/256 [==============================] - 0s 25us/step - loss: 8.7526 - acc: 0.5000 - val_loss: 6.6717 - val_acc: 0.6400\n",
      "Epoch 400/500\n",
      "256/256 [==============================] - 0s 38us/step - loss: 8.7503 - acc: 0.5000 - val_loss: 6.6697 - val_acc: 0.6400\n",
      "Epoch 401/500\n",
      "256/256 [==============================] - 0s 28us/step - loss: 8.7479 - acc: 0.5000 - val_loss: 6.6677 - val_acc: 0.6400\n",
      "Epoch 402/500\n",
      "256/256 [==============================] - 0s 40us/step - loss: 8.7455 - acc: 0.5000 - val_loss: 6.6656 - val_acc: 0.6400\n",
      "Epoch 403/500\n",
      "256/256 [==============================] - 0s 25us/step - loss: 8.7432 - acc: 0.5000 - val_loss: 6.6637 - val_acc: 0.6400\n",
      "Epoch 404/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 8.7408 - acc: 0.5000 - val_loss: 6.6617 - val_acc: 0.6400\n",
      "Epoch 405/500\n",
      "256/256 [==============================] - 0s 44us/step - loss: 8.7385 - acc: 0.5000 - val_loss: 6.6596 - val_acc: 0.6400\n",
      "Epoch 406/500\n",
      "256/256 [==============================] - 0s 55us/step - loss: 8.7362 - acc: 0.5000 - val_loss: 6.6576 - val_acc: 0.6400\n",
      "Epoch 407/500\n",
      "256/256 [==============================] - 0s 56us/step - loss: 8.7339 - acc: 0.5000 - val_loss: 6.6556 - val_acc: 0.6400\n",
      "Epoch 408/500\n",
      "256/256 [==============================] - 0s 34us/step - loss: 8.7315 - acc: 0.5000 - val_loss: 6.6535 - val_acc: 0.6400\n",
      "Epoch 409/500\n",
      "256/256 [==============================] - 0s 32us/step - loss: 8.7292 - acc: 0.5000 - val_loss: 6.6515 - val_acc: 0.6400\n",
      "Epoch 410/500\n",
      "256/256 [==============================] - 0s 32us/step - loss: 8.7269 - acc: 0.5000 - val_loss: 6.6494 - val_acc: 0.6400\n",
      "Epoch 411/500\n",
      "256/256 [==============================] - 0s 32us/step - loss: 8.7246 - acc: 0.5000 - val_loss: 6.6474 - val_acc: 0.6400\n",
      "Epoch 412/500\n",
      "256/256 [==============================] - 0s 29us/step - loss: 8.7224 - acc: 0.5000 - val_loss: 6.6454 - val_acc: 0.6400\n",
      "Epoch 413/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 8.7201 - acc: 0.5000 - val_loss: 6.6432 - val_acc: 0.6400\n",
      "Epoch 414/500\n",
      "256/256 [==============================] - 0s 35us/step - loss: 8.7178 - acc: 0.5000 - val_loss: 6.6412 - val_acc: 0.6400\n",
      "Epoch 415/500\n",
      "256/256 [==============================] - 0s 35us/step - loss: 8.7155 - acc: 0.5000 - val_loss: 6.6392 - val_acc: 0.6400\n",
      "Epoch 416/500\n",
      "256/256 [==============================] - 0s 43us/step - loss: 8.7133 - acc: 0.5000 - val_loss: 6.6372 - val_acc: 0.6400\n",
      "Epoch 417/500\n",
      "256/256 [==============================] - 0s 23us/step - loss: 8.7110 - acc: 0.5000 - val_loss: 6.6352 - val_acc: 0.6400\n",
      "Epoch 418/500\n",
      "256/256 [==============================] - 0s 52us/step - loss: 8.7088 - acc: 0.5000 - val_loss: 6.6332 - val_acc: 0.6400\n",
      "Epoch 419/500\n",
      "256/256 [==============================] - 0s 35us/step - loss: 8.7065 - acc: 0.5000 - val_loss: 6.6313 - val_acc: 0.6400\n",
      "Epoch 420/500\n",
      "256/256 [==============================] - 0s 60us/step - loss: 8.7043 - acc: 0.5000 - val_loss: 6.6293 - val_acc: 0.6400\n",
      "Epoch 421/500\n",
      "256/256 [==============================] - 0s 44us/step - loss: 8.7021 - acc: 0.5000 - val_loss: 6.6273 - val_acc: 0.6400\n",
      "Epoch 422/500\n",
      "256/256 [==============================] - 0s 33us/step - loss: 8.6998 - acc: 0.5000 - val_loss: 6.6254 - val_acc: 0.6400\n",
      "Epoch 423/500\n",
      "256/256 [==============================] - 0s 31us/step - loss: 8.6976 - acc: 0.5000 - val_loss: 6.6234 - val_acc: 0.6400\n",
      "Epoch 424/500\n",
      "256/256 [==============================] - 0s 35us/step - loss: 8.6954 - acc: 0.5000 - val_loss: 6.6213 - val_acc: 0.6400\n",
      "Epoch 425/500\n",
      "256/256 [==============================] - 0s 35us/step - loss: 8.6932 - acc: 0.5000 - val_loss: 6.6193 - val_acc: 0.6400\n",
      "Epoch 426/500\n",
      "256/256 [==============================] - 0s 39us/step - loss: 8.6910 - acc: 0.5000 - val_loss: 6.6173 - val_acc: 0.6400\n",
      "Epoch 427/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 8.6888 - acc: 0.5000 - val_loss: 6.6152 - val_acc: 0.6400\n",
      "Epoch 428/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 8.6866 - acc: 0.5000 - val_loss: 6.6132 - val_acc: 0.6400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 429/500\n",
      "256/256 [==============================] - 0s 55us/step - loss: 8.6844 - acc: 0.5000 - val_loss: 6.6112 - val_acc: 0.6400\n",
      "Epoch 430/500\n",
      "256/256 [==============================] - 0s 47us/step - loss: 8.6823 - acc: 0.5000 - val_loss: 6.6093 - val_acc: 0.6400\n",
      "Epoch 431/500\n",
      "256/256 [==============================] - 0s 42us/step - loss: 8.6801 - acc: 0.5000 - val_loss: 6.6072 - val_acc: 0.6400\n",
      "Epoch 432/500\n",
      "256/256 [==============================] - 0s 72us/step - loss: 8.6779 - acc: 0.5000 - val_loss: 6.6052 - val_acc: 0.6400\n",
      "Epoch 433/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 8.6758 - acc: 0.5000 - val_loss: 6.6031 - val_acc: 0.6400\n",
      "Epoch 434/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 8.6737 - acc: 0.5000 - val_loss: 6.6010 - val_acc: 0.6400\n",
      "Epoch 435/500\n",
      "256/256 [==============================] - 0s 54us/step - loss: 8.6715 - acc: 0.5000 - val_loss: 6.5990 - val_acc: 0.6400\n",
      "Epoch 436/500\n",
      "256/256 [==============================] - 0s 64us/step - loss: 8.6694 - acc: 0.5000 - val_loss: 6.5968 - val_acc: 0.6400\n",
      "Epoch 437/500\n",
      "256/256 [==============================] - 0s 64us/step - loss: 8.6672 - acc: 0.5000 - val_loss: 6.5947 - val_acc: 0.6400\n",
      "Epoch 438/500\n",
      "256/256 [==============================] - 0s 63us/step - loss: 8.6651 - acc: 0.5000 - val_loss: 6.5927 - val_acc: 0.6400\n",
      "Epoch 439/500\n",
      "256/256 [==============================] - 0s 69us/step - loss: 8.6630 - acc: 0.5000 - val_loss: 6.5907 - val_acc: 0.6400\n",
      "Epoch 440/500\n",
      "256/256 [==============================] - 0s 55us/step - loss: 8.6609 - acc: 0.5000 - val_loss: 6.5887 - val_acc: 0.6400\n",
      "Epoch 441/500\n",
      "256/256 [==============================] - 0s 58us/step - loss: 8.6588 - acc: 0.5000 - val_loss: 6.5866 - val_acc: 0.6400\n",
      "Epoch 442/500\n",
      "256/256 [==============================] - 0s 60us/step - loss: 8.6567 - acc: 0.5000 - val_loss: 6.5844 - val_acc: 0.6400\n",
      "Epoch 443/500\n",
      "256/256 [==============================] - 0s 34us/step - loss: 8.6546 - acc: 0.5000 - val_loss: 6.5823 - val_acc: 0.6400\n",
      "Epoch 444/500\n",
      "256/256 [==============================] - 0s 36us/step - loss: 8.6525 - acc: 0.5000 - val_loss: 6.5803 - val_acc: 0.6400\n",
      "Epoch 445/500\n",
      "256/256 [==============================] - 0s 38us/step - loss: 8.6504 - acc: 0.5000 - val_loss: 6.5783 - val_acc: 0.6400\n",
      "Epoch 446/500\n",
      "256/256 [==============================] - 0s 39us/step - loss: 8.6483 - acc: 0.5000 - val_loss: 6.5764 - val_acc: 0.6400\n",
      "Epoch 447/500\n",
      "256/256 [==============================] - 0s 42us/step - loss: 8.6463 - acc: 0.5000 - val_loss: 6.5744 - val_acc: 0.6400\n",
      "Epoch 448/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 8.6442 - acc: 0.5000 - val_loss: 6.5725 - val_acc: 0.6400\n",
      "Epoch 449/500\n",
      "256/256 [==============================] - 0s 57us/step - loss: 8.6421 - acc: 0.5000 - val_loss: 6.5706 - val_acc: 0.6400\n",
      "Epoch 450/500\n",
      "256/256 [==============================] - 0s 65us/step - loss: 8.6401 - acc: 0.5000 - val_loss: 6.5687 - val_acc: 0.6400\n",
      "Epoch 451/500\n",
      "256/256 [==============================] - 0s 44us/step - loss: 8.6380 - acc: 0.5000 - val_loss: 6.5667 - val_acc: 0.6400\n",
      "Epoch 452/500\n",
      "256/256 [==============================] - 0s 57us/step - loss: 8.6360 - acc: 0.5000 - val_loss: 6.5647 - val_acc: 0.6400\n",
      "Epoch 453/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 8.6339 - acc: 0.5000 - val_loss: 6.5627 - val_acc: 0.6400\n",
      "Epoch 454/500\n",
      "256/256 [==============================] - 0s 44us/step - loss: 8.6319 - acc: 0.5000 - val_loss: 6.5607 - val_acc: 0.6400\n",
      "Epoch 455/500\n",
      "256/256 [==============================] - 0s 58us/step - loss: 8.6299 - acc: 0.5000 - val_loss: 6.5586 - val_acc: 0.6400\n",
      "Epoch 456/500\n",
      "256/256 [==============================] - 0s 46us/step - loss: 8.6279 - acc: 0.5000 - val_loss: 6.5566 - val_acc: 0.6400\n",
      "Epoch 457/500\n",
      "256/256 [==============================] - 0s 44us/step - loss: 8.6259 - acc: 0.5000 - val_loss: 6.5546 - val_acc: 0.6400\n",
      "Epoch 458/500\n",
      "256/256 [==============================] - 0s 34us/step - loss: 8.6238 - acc: 0.5000 - val_loss: 6.5526 - val_acc: 0.6400\n",
      "Epoch 459/500\n",
      "256/256 [==============================] - 0s 42us/step - loss: 8.6218 - acc: 0.5000 - val_loss: 6.5506 - val_acc: 0.6400\n",
      "Epoch 460/500\n",
      "256/256 [==============================] - 0s 42us/step - loss: 8.6198 - acc: 0.5000 - val_loss: 6.5486 - val_acc: 0.6400\n",
      "Epoch 461/500\n",
      "256/256 [==============================] - 0s 55us/step - loss: 8.6179 - acc: 0.5000 - val_loss: 6.5467 - val_acc: 0.6400\n",
      "Epoch 462/500\n",
      "256/256 [==============================] - 0s 46us/step - loss: 8.6159 - acc: 0.5000 - val_loss: 6.5447 - val_acc: 0.6400\n",
      "Epoch 463/500\n",
      "256/256 [==============================] - 0s 54us/step - loss: 8.6139 - acc: 0.5000 - val_loss: 6.5427 - val_acc: 0.6400\n",
      "Epoch 464/500\n",
      "256/256 [==============================] - 0s 74us/step - loss: 8.6119 - acc: 0.5000 - val_loss: 6.5408 - val_acc: 0.6400\n",
      "Epoch 465/500\n",
      "256/256 [==============================] - 0s 42us/step - loss: 8.6100 - acc: 0.5000 - val_loss: 6.5389 - val_acc: 0.6400\n",
      "Epoch 466/500\n",
      "256/256 [==============================] - 0s 60us/step - loss: 8.6080 - acc: 0.5000 - val_loss: 6.5369 - val_acc: 0.6400\n",
      "Epoch 467/500\n",
      "256/256 [==============================] - 0s 51us/step - loss: 8.6060 - acc: 0.5000 - val_loss: 6.5350 - val_acc: 0.6400\n",
      "Epoch 468/500\n",
      "256/256 [==============================] - 0s 34us/step - loss: 8.6041 - acc: 0.5000 - val_loss: 6.5331 - val_acc: 0.6400\n",
      "Epoch 469/500\n",
      "256/256 [==============================] - 0s 33us/step - loss: 8.6021 - acc: 0.5000 - val_loss: 6.5312 - val_acc: 0.6400\n",
      "Epoch 470/500\n",
      "256/256 [==============================] - 0s 44us/step - loss: 8.6002 - acc: 0.5000 - val_loss: 6.5293 - val_acc: 0.6400\n",
      "Epoch 471/500\n",
      "256/256 [==============================] - 0s 30us/step - loss: 8.5983 - acc: 0.5000 - val_loss: 6.5275 - val_acc: 0.6400\n",
      "Epoch 472/500\n",
      "256/256 [==============================] - 0s 46us/step - loss: 8.5963 - acc: 0.5000 - val_loss: 6.5255 - val_acc: 0.6400\n",
      "Epoch 473/500\n",
      "256/256 [==============================] - 0s 43us/step - loss: 8.5944 - acc: 0.5000 - val_loss: 6.5236 - val_acc: 0.6400\n",
      "Epoch 474/500\n",
      "256/256 [==============================] - 0s 55us/step - loss: 8.5925 - acc: 0.5000 - val_loss: 6.5217 - val_acc: 0.6400\n",
      "Epoch 475/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 8.5906 - acc: 0.5000 - val_loss: 6.5198 - val_acc: 0.6400\n",
      "Epoch 476/500\n",
      "256/256 [==============================] - 0s 54us/step - loss: 8.5887 - acc: 0.5000 - val_loss: 6.5179 - val_acc: 0.6400\n",
      "Epoch 477/500\n",
      "256/256 [==============================] - 0s 45us/step - loss: 8.5867 - acc: 0.5000 - val_loss: 6.5159 - val_acc: 0.6400\n",
      "Epoch 478/500\n",
      "256/256 [==============================] - 0s 44us/step - loss: 8.5849 - acc: 0.5000 - val_loss: 6.5140 - val_acc: 0.6400\n",
      "Epoch 479/500\n",
      "256/256 [==============================] - 0s 36us/step - loss: 8.5830 - acc: 0.5000 - val_loss: 6.5121 - val_acc: 0.6400\n",
      "Epoch 480/500\n",
      "256/256 [==============================] - 0s 65us/step - loss: 8.5811 - acc: 0.5000 - val_loss: 6.5102 - val_acc: 0.6400\n",
      "Epoch 481/500\n",
      "256/256 [==============================] - 0s 33us/step - loss: 8.5792 - acc: 0.5000 - val_loss: 6.5082 - val_acc: 0.6400\n",
      "Epoch 482/500\n",
      "256/256 [==============================] - 0s 26us/step - loss: 8.5773 - acc: 0.5000 - val_loss: 6.5063 - val_acc: 0.6400\n",
      "Epoch 483/500\n",
      "256/256 [==============================] - 0s 36us/step - loss: 8.5755 - acc: 0.5000 - val_loss: 6.5045 - val_acc: 0.6400\n",
      "Epoch 484/500\n",
      "256/256 [==============================] - 0s 31us/step - loss: 8.5736 - acc: 0.5000 - val_loss: 6.5027 - val_acc: 0.6400\n",
      "Epoch 485/500\n",
      "256/256 [==============================] - 0s 30us/step - loss: 8.5717 - acc: 0.5000 - val_loss: 6.5009 - val_acc: 0.6400\n",
      "Epoch 486/500\n",
      "256/256 [==============================] - 0s 43us/step - loss: 8.5699 - acc: 0.5000 - val_loss: 6.4992 - val_acc: 0.6400\n",
      "Epoch 487/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 8.5681 - acc: 0.5000 - val_loss: 6.4974 - val_acc: 0.6400\n",
      "Epoch 488/500\n",
      "256/256 [==============================] - 0s 41us/step - loss: 8.5662 - acc: 0.5000 - val_loss: 6.4957 - val_acc: 0.6400\n",
      "Epoch 489/500\n",
      "256/256 [==============================] - 0s 44us/step - loss: 8.5644 - acc: 0.5000 - val_loss: 6.4938 - val_acc: 0.6400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 490/500\n",
      "256/256 [==============================] - 0s 55us/step - loss: 8.5625 - acc: 0.5000 - val_loss: 6.4920 - val_acc: 0.6400\n",
      "Epoch 491/500\n",
      "256/256 [==============================] - 0s 77us/step - loss: 8.5607 - acc: 0.5000 - val_loss: 6.4902 - val_acc: 0.6400\n",
      "Epoch 492/500\n",
      "256/256 [==============================] - 0s 36us/step - loss: 8.5589 - acc: 0.5000 - val_loss: 6.4884 - val_acc: 0.6400\n",
      "Epoch 493/500\n",
      "256/256 [==============================] - 0s 40us/step - loss: 8.5571 - acc: 0.5000 - val_loss: 6.4865 - val_acc: 0.6400\n",
      "Epoch 494/500\n",
      "256/256 [==============================] - 0s 43us/step - loss: 8.5553 - acc: 0.5000 - val_loss: 6.4847 - val_acc: 0.6400\n",
      "Epoch 495/500\n",
      "256/256 [==============================] - 0s 41us/step - loss: 8.5535 - acc: 0.5000 - val_loss: 6.4829 - val_acc: 0.6400\n",
      "Epoch 496/500\n",
      "256/256 [==============================] - 0s 48us/step - loss: 8.5517 - acc: 0.5000 - val_loss: 6.4812 - val_acc: 0.6400\n",
      "Epoch 497/500\n",
      "256/256 [==============================] - 0s 46us/step - loss: 8.5499 - acc: 0.5000 - val_loss: 6.4794 - val_acc: 0.6400\n",
      "Epoch 498/500\n",
      "256/256 [==============================] - 0s 53us/step - loss: 8.5481 - acc: 0.5000 - val_loss: 6.4776 - val_acc: 0.6400\n",
      "Epoch 499/500\n",
      "256/256 [==============================] - 0s 57us/step - loss: 8.5463 - acc: 0.5000 - val_loss: 6.4759 - val_acc: 0.6400\n",
      "Epoch 500/500\n",
      "256/256 [==============================] - 0s 68us/step - loss: 8.5445 - acc: 0.5000 - val_loss: 6.4743 - val_acc: 0.6400\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "x = train_data[col_names]\n",
    "model = prepare_classifier(dropout_rate=.25, input_shape=x.shape[1])\n",
    "\n",
    "\n",
    "for train_index, test_index in cv.split(x, y):\n",
    "\n",
    "    x_train = x.iloc[train_index]\n",
    "    y_train = y.iloc[train_index]\n",
    "    x_test = x.iloc[test_index]\n",
    "    y_test = y.iloc[test_index]\n",
    "    smt = SMOTETomek(ratio=1)\n",
    "    _x_train_smt, _y_train_smt = smt.fit_sample(x_train, y_train)\n",
    "    history = model.fit(_x_train_smt, \\\n",
    "                        _y_train_smt, \\\n",
    "                        validation_data=(x_test,y_test),\n",
    "                        epochs=500, \\\n",
    "                        batch_size = len(_x_train_smt), \\\n",
    "                        verbose=1, \\\n",
    "                        shuffle=True ,\\\n",
    "                        callbacks=[es],\n",
    "#                         class_weight = {0:.2,1:.6}\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 69us/step\n",
      "\n",
      "acc: 64.00%\n"
     ]
    }
   ],
   "source": [
    "metrics = model.evaluate(x, y)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], metrics[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_acc = history.history['acc']\n",
    "valid_acc = history.history['val_acc']\n",
    "\n",
    "# get the loss\n",
    "train_loss = history.history['loss']\n",
    "valid_loss = history.history['val_loss']\n",
    "\n",
    "# get the number of entries\n",
    "xvalues = np.arange(len(train_acc))\n",
    "\n",
    "# visualize\n",
    "f,ax = plot.subplots(1,2, figsize=(10,5))\n",
    "ax[0].plot(xvalues, train_loss)\n",
    "ax[0].plot(xvalues, valid_loss)\n",
    "ax[0].set_title(\"Loss curve\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"loss\")\n",
    "ax[0].legend(['train', 'validation'])\n",
    "\n",
    "ax[1].plot(xvalues, train_acc)\n",
    "ax[1].plot(xvalues, valid_acc)\n",
    "ax[1].set_title(\"Accuracy\")\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"accuracy\")\n",
    "ax[1].legend(['train', 'validation'])\n",
    "\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "\n",
    "1. We got highest scores of ROC/AUC 0.92 in logistic regression with l2 parameter\n",
    "\n",
    "2. From the learning curve, we also see that overfitting tendency is less\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = panda.read_csv('data/test.csv')\n",
    "\n",
    "test_data_x = test_data[col_names]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target = model.predict(test_data_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       ...,\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0, 19750], dtype=int64)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(test_target.astype(np.int64).ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_submission = panda.DataFrame({'target':test_target.astype(np.int64).ravel()})\n",
    "final_submission['id'] = test_data['id']\n",
    "final_submission[['id','target']].head() \n",
    "\n",
    "# np.bincount(test_target)\n",
    "\n",
    "final_submission[['id','target']].to_csv('sample_submission_keras.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
